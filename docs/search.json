[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":"Welcome website book Handbook Regression Modeling People Analytics Keith McNulty.\n\nNote: book published Chapman & Hall/CRC can purchased directly website 20% discount using discount code JML20, well Amazon book retailers. Please consider buying book find useful - author’s royalties donated Red Cross Ukraine Appeal. online version book free read (thanks Chapman & Hall/CRC), licensed Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. feedback, please feel free file issue GitHub. Thank !book available bootstrap format , prefer, plain gitbook format.","code":""},{"path":"index.html","id":"notes-on-data-used-in-this-book","chapter":"Welcome","heading":"Notes on data used in this book","text":"R, Python Julia users, data sets used book can downloaded individually following code chapter. Alternatively, packages containing data sets used book now available R Python. R users, install load peopleanalyticsdata R package.Python users , use pip install peopleanalyticsdata install package environment. , use package:Happy modeling!Last update: 19 August 2022","code":"\n# install peopleanalyticsdata package\ninstall.packages(\"peopleanalyticsdata\")\nlibrary(peopleanalyticsdata)\n\n# see a list of data sets\ndata(package = \"peopleanalyticsdata\")\n\n# find out more about a specific data set ('managers' example)\nhelp(managers)# import peopleanalyticsdata package\nimport peopleanalyticsdata as pad\nimport pandas as pd\n\n# see a list of data sets\npad.list_sets()\n\n# load data into a dataframe\ndf = pad.managers()\n\n# find out more about a specific data set ('managers' example)\npad.managers().info()"},{"path":"foreword-by-alexis-fink.html","id":"foreword-by-alexis-fink","chapter":"Foreword by Alexis Fink","heading":"Foreword by Alexis Fink","text":"past decade , increases compute power, emergence friendly analytic tools \nexplosion data created wonderful opportunity bring analytical rigor nearly every\nimaginable question. coincidentally, organizations increasingly looking apply data \ncapability typically greatest area expense greatest strategic differentiator—people. long, many critical decisions organization—people decisions—guided gut instinct borrowed ‘best practices’ democratization people\nanalytics opened enticing pathways fix . Suddenly, analysts originally interested \ndata problems began interested people problems, HR professionals dedicated \ncareers solving people problems needed sophisticated analysis data storytelling make\ncases refine approaches greater efficiency, effectiveness impact.data work people organizations complexities types data work\ndoesn’t. Often, employee populations relatively smaller data sets used areas,\nsometimes limiting methods can used. Various regulatory requirements may dictate \ndata can gathered used, types evidence might required various programs \npeople strategies. Human behavior organizations sufficiently complex typically, multiple\nfactors work together influencing outcome. Effects can subtle meaningful \ncombination, difficult tease apart. many disciplines, prediction important aim,\npeople analytics projects practitioners, understanding something happening \ncritical.universe analytical approaches wonderful vast, best ‘Swiss army knife’ \npeople analytics regression. volume accessible, targeted work aimed directly \nsupporting professionals people analytics work. ’ve privilege knowing respecting\nKeith McNulty many years – rare marvelous individual deeply expert \nmechanics data analytics, curious steeped opportunities improve \neffectiveness well-people work, gifted teacher storyteller. among \nprolific standard-bearers people analytics. new open-source volume keeping \nmany years contributions practice understanding people work.nearly 30 years people analytics work privilege leading people analytics teams\nseveral leading global organizations, still excited problems get solve, insights \nget spawn, tremendous impact can organizations people comprise\n. work human technical important exciting deeply gratifying. hope \nfind Handbook Regression Modeling People Analytics helps uncover new\ntruths create positive impacts work.Alexis . Fink\nDecember 2020Alexis . Fink, PhD leading figure people analytics led major people analytics teams Microsoft Intel current role Vice President People Analytics Workforce Strategy Facebook. Fellow Society Industrial Organizational Psychology frequent author, journal editor research leader field.","code":""},{"path":"introduction.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"fresh-faced undergraduate mathematics 1990s, took introductory course statistics first term. never take another. struggled subject, scored lowest grade swore never go anywhere near .wrong . Today live breathe statistics. happen?Firstly, statistics solving real-world problems, amazingly single mention relatable problem real life course took years ago, just abstract mathematics. Nowadays, know work personal learning activities mathematics meaning without motivating problem apply , ’ll see example problems book.Secondly, statistics data, working real data encouraged reengage statistics come different angle—bottom-say. Suddenly concepts put whiteboards using abstract formulas now real meaning consequence data working . , real data helps statistical theory come life, book supported numerous data sets designed reader engage .one step solidified newfound love statistics, put regression modeling practice. Faced data sets initially believed just far messy random able produce genuine insights, progressively became fascinated regression can cut messiness, compartmentalize randomness lead straight inferences often surprising clarity conclusions.Hence motivation writing book, give others—whether working people analytics otherwise—starting point practical learning regression methods, hope see immediate applications work take advantage much-underused toolkit provides strong support evidence-based practice.mathematician now practitioner analytics. reason see book neither afraid obsessed mathematics methodologies covered. general observation many students practitioners make mistake trying run multivariate models without even basic understanding underlying mathematics models, find difficult see can credible responding wide range questions critique work without understanding. said, also necessary students practitioners understand deepest levels theory order fluent running interpreting multivariate models. book tried limit mathematical exposition level allows confident fluent execution interpretation.subscribe strongly principles open source sharing knowledge. want reference material book use exercises data sets trainings classes, free need request permission. ask make reference book source.expect book improve time. found book part helpful solving problem, ’d love hear . comments improve question aspect contents book encourage leave issue Github repository. reliable way see comment. promise consider comments input, make personal judgment whether helpful aims purpose book. make changes additions based input make point acknowledge contribution.like thank following individuals reviewed contributed book point development: Liz Romero, Alex LoPilato, Kevin Jaggs, Seth Saavedra, Akshay Kotha. sincere thanks Alexis Fink drawing years people analytics experience set context book foreword. thanks people analytics community constant encouragement support sharing theory, content method, R community work giving us amazing constantly improving statistical tools work . Finally, like thank family patience understanding evenings weekends dedicated writing book, tolerating far much dinner conversation topic statistics.Keith McNulty\nDecember 2020","code":""},{"path":"inf-model.html","id":"inf-model","chapter":"1 The Importance of Regression in People Analytics","heading":"1 The Importance of Regression in People Analytics","text":"19th century, Francis Galton first used term ‘regression’ describe statistical phenomenon (see Chapter 4), little know important term today. Many powerful tools statistical inference now disposal can traced back types early analysis Galton contemporaries engaged . sheer number different regression-related methodologies variants available researchers practitioners today mind-boggling, still rich veins ongoing research focused defining refining new forms regression tackle new problems.Neither Galton imagined advent age data now live . us (like ) entered world work even recently 20 years ago remember time problems expected solved using data-driven approach, simply data. Things different now, data collected processed around us available use direct indirect measures phenomena interested .Along growth data seen recent years, also seen rapid growth availability statistical tools—open source free use—fundamentally change go analytics. Gone clunky, complex, repeated steps calculators spreadsheets. place lean statistical programming languages can implement regression analysis milliseconds single line code, allowing us easily run reproduce multivariate analysis scale.given access well-developed methodology, rich sources data readily accessible tools, somewhat surprising many analytics practitioners limited knowledge understanding regression applications. aim book encourage inexperienced analytics practitioners ‘dip toes’ wide varied world regression order deliver targeted precise insights organizations stakeholders problems interested . primary subject matter focus book analysis people-related phenomena, material easily naturally transferable disciplines. Therefore book can regarded practical introduction wide range regression methods analytics student practitioner.firm belief people analytics professionals strong understanding regression models implement interpret practice, aim book provide need help getting . chapter set scene technical learning remainder book outlining relevance regression models people analytics practice. also touch general inferential modeling theory set context later chapters, provide preview contents, structure learning objectives book.","code":""},{"path":"inf-model.html","id":"why-is-regression-modeling-so-important-in-people-analytics","chapter":"1 The Importance of Regression in People Analytics","heading":"1.1 Why is regression modeling so important in people analytics?","text":"People analytics involves study behaviors characteristics people groups relation important business, organizational institutional outcomes. can involve qualitative methods quantitative methods, data available related particular topic interest, quantitative methods almost always considered important. specific focus outcomes, analyst working people analytics frequently need model outcomes understand influences potentially predict future.Modeling outcome primary goal understanding influences can quite different matter modeling outcome primary goal predicting happen future. need understand influences outcome, need get inside model construct formula structure infer variable acts outcome, need get sense variables meaningful , need quantify ‘explainability’ outcome based variables. primary aim predict outcome, getting inside model less important don’t explain outcome, just need confident predicts accurately.model constructed understand outcome often called inferential model. Regression models well-known well-used inferential models available, providing wide range measures insights help us explain relationship input variables outcome interest, shall see later chapters book.current reality field people analytics inferential models required predictive models. two reasons . First, data sets people analytics rarely large enough facilitate satisfactory prediction accuracy, attention usually shifted inference reason alone. Second, field people analytics, decisions often real impact individuals. Therefore, even rare situations accurate predictive modeling attainable, stakeholders unlikely trust output bear consequences predictive models without sort elementary understanding predictions generated. requires analyst consider inference power well predictive accuracy selecting modeling approach. , many regression models come fore commonly able provide inferential predictive value.Finally, growing importance evidence-based practice many clinical professional fields generated need advanced modeling skills satisfy rising demand quantitative evidence decision makers. people-related fields human resources, many varieties specialized regression-based models survival models latent variable models crossed academic clinical settings business settings recent years, increasing need qualified individuals understand can implement interpret models practice.","code":""},{"path":"inf-model.html","id":"theory-modeling","chapter":"1 The Importance of Regression in People Analytics","heading":"1.2 What do we mean by ‘modeling’‍?","text":"term ‘modeling’ wide range meaning everyday life work. book focused inferential modeling, define specific form statistical learning, tries discover understand mathematical relationship set measurements certain constructs measurement outcome interest, based sample data . Modeling concept process.","code":""},{"path":"inf-model.html","id":"the-theory-of-inferential-modeling","chapter":"1 The Importance of Regression in People Analytics","heading":"1.2.1 The theory of inferential modeling","text":"start theoretical description provide real example later chapter illustrate.Imagine population \\(\\mathscr{P}\\) believe may non-random relationship certain construct set constructs \\(\\mathscr{C}\\) certain measurable outcome \\(\\mathscr{O}\\). Imagine certain sample \\(S\\) observations \\(\\mathscr{P}\\), collection data believe measure \\(\\mathscr{C}\\) acceptable level accuracy, also measure outcome \\(\\mathscr{O}\\).convention, denote set data measure \\(\\mathscr{C}\\) sample \\(S\\) \\(X = x_1, x_2, \\dots, x_p\\), \\(x_i\\) vector (column) data measuring least one constructs \\(\\mathscr{C}\\). denote set data measure \\(\\mathscr{O}\\) sample set \\(S\\) \\(y\\). upper-case \\(X\\) used expectation several columns data measuring constructs, lower-case \\(y\\) used expectation outcome single column.Inferential modeling process learning relationship (lack relationship) data \\(X\\) \\(y\\) using describe relationship (lack relationship) constructs \\(\\mathscr{C}\\) outcome \\(\\mathscr{O}\\) valid high degree statistical certainty population \\(\\mathscr{P}\\). process may include:Testing proposed mathematical relationship form function, structure iterative methodComparing relationship proposed relationshipsDescribing relationship statisticallyDetermining whether relationship (certain elements ) can generalized sample set \\(S\\) population \\(\\mathscr{P}\\)test relationship \\(X\\) \\(y\\), acknowledge data measurements imperfect observation sample \\(S\\) may contain random error control. Therefore define relationship :\\[\ny = f(X) + \\epsilon\n\\]\n\\(f\\) transformation function data \\(X\\) \\(\\epsilon\\) random, uncontrollable error.\\(f\\) can take form predetermined function formula defined \\(X\\), like linear function example. case can call model parametric model. parametric model, modeled value \\(y\\) known soon know values \\(X\\) simply applying formula. non-parametric model, predetermined formula defines modeled value \\(y\\) purely terms \\(X\\). Non-parametric models need information addition \\(X\\) order determine modeled value \\(y\\)—example value \\(y\\) observations similar \\(X\\) values.Regression models designed derive \\(f\\) using estimation based statistical likelihood expectation, founded theory distribution random variables. Regression models can parametric non-parametric, far commonly used methods (majority featured book) parametric. foundation statistical likelihood expectation, particularly suited helping answer questions generalizability—, extent can relationship observed sample \\(S\\) inferred population \\(\\mathscr{P}\\), usually driving force form inferential modeling.Note difference establishing statistical relationship \\(\\mathscr{C}\\) \\(\\mathscr{O}\\) establishing causal relationship two. can common trap inexperienced statistical analysts fall communicating conclusions modeling. Establishing relationship exists construct outcome far cry able say one causes . common truism ‘correlation equal causation’‍.bring theory life, consider walkthrough example Chapter 4 book. example, discuss establish relationship academic results students first three years education program results fourth year. case, population \\(\\mathscr{P}\\) past, present future students take similar examinations, sample \\(S\\) students completed studies past three years. \\(X = x_1, x_2, x_3\\) three scores first three years, \\(y\\) score fourth year. test \\(f\\) linear relationship, establish relationship can generalized entire population \\(\\mathscr{P}\\) substantial level statistical confidence1.Almost work book refer variables \\(X\\) input variables variable \\(y\\) outcome variable. many common terms may find sources—example \\(X\\) often known independent variables covariates \\(y\\) often known dependent response variable.","code":""},{"path":"inf-model.html","id":"the-process-of-inferential-modeling","chapter":"1 The Importance of Regression in People Analytics","heading":"1.2.2 The process of inferential modeling","text":"Inferential modeling—regression otherwise—process numerous steps. Typically main steps :Defining outcome interest \\(\\mathscr{O}\\) input constructs \\(\\mathscr{C}\\) based broader evidence-based objectiveConfirming \\(\\mathscr{O}\\) reliable measurement dataDetermining data can used measure \\(\\mathscr{C}\\)Determining sample \\(S\\) collecting, refining cleaning data.Performing exploratory data analysis (EDA) proposing set models test \\(f\\)Putting data appropriate format modelRunning modelsInterpreting outputs performing model diagnosticsSelecting optimal model modelsArticulating inferences can generalized apply \\(\\mathscr{P}\\)book primarily focused steps 7–10 process2. say steps 1–6 important. Indeed steps critical often loaded analytic traps. Defining problem, collecting reliable measures cleaning organizing data still source much pain angst analysts, topics another day.","code":""},{"path":"inf-model.html","id":"the-structure-system-and-organization-of-this-book","chapter":"1 The Importance of Regression in People Analytics","heading":"1.3 The structure, system and organization of this book","text":"purpose book put inexperienced practitioners firmly path confident appropriate use regression techniques day--day work. requires enough understanding underlying theory judgments can made results, also practical set steps help practitioners apply common regression methods variety typical modeling scenarios reliable reproducible way.chapters, time spent underlying mathematics. degree academic theorist, enough ensure reader can associate mathematical meaning outputs models. may tempting skip math, strongly recommend intend high performer field. best analysts can genuinely understand numbers telling .statistical programming language R used practical demonstration chapter. R open source particularly well geared inferential statistics, excellent choice whose work involves lot inferential analysis. later chapters, show implementations available methodologies Python Julia, also powerful open source tools sort work.chapter involves walkthrough example illustrate specific method allow reader replicate analysis . exercises end chapter designed reader can try method different data set, different problem data set, test learning understanding. , eleven different data sets used walkthrough exercise examples, data sets fictitious constructions unless otherwise indicated. Despite fiction, deliberately designed present reader something resembling data might look practice, albeit cleaner organized.chapters book arranged follows:Chapter 2 covers basics R programming language want attempt jump straight work subsequent chapters little R experience. Experienced R programmers can skip chapter.Chapter 3 covers essential statistical concepts needed understand multivariate regression models. also serves tutorial univariate bivariate statistics illustrated real data. need help developing decent understanding descriptive statistics, random distribution hypothesis testing, important chapter study.Chapter 4 covers linear regression course introduces many foundational concepts. walkthrough example involves modeling academic results prior results. exercises involve modeling income levels based various work demographic factors.Chapter 5 covers binomial logistic regression. walkthrough example involves modeling promotion likelihood based performance metrics. exercises involve modeling charitable donation likelihood based prior donation behavior demographics.Chapter 6 covers multinomial regression. walkthrough example exercise involves modeling choice three health insurance products company employees based demographic position data.Chapter 7 covers ordinal regression. walkthrough example involves modeling -game disciplinary action soccer players based prior discipline factors. exercises involve modeling manager performance based varied data.Chapter 8 covers modeling options data explicit latent hierarchy. first part covers mixed modeling uses model speed dating decisions walkthrough example. second part covers structural equation modeling uses survey political party walkthrough example. exercises involve modeling latent variables employee engagement survey.Chapter 9 covers survival analysis, Cox proportional hazard regression frailty models. chapter uses employee attrition walkthrough example exercise.Chapter 10 outlines alternative technical approaches regression modeling R, Python Julia. Models previous chapters used illustrate alternative approaches.Chapter 11 covers power analysis, focusing particular estimating required minimum sample sizes establishing meaningful inferences simple statistical tests multivariate models. Examples related experimental studies used illustrate, concurrent validity studies selection instruments. Example implementations R, Python Julia outlined.","code":""},{"path":"the-basics-of-the-r-programming-language.html","id":"the-basics-of-the-r-programming-language","chapter":"2 The Basics of the R Programming Language","heading":"2 The Basics of the R Programming Language","text":"work book implemented R statistical programming language , along Python, one two languages use day--day statistical analysis. Sample implementations Python also provided various points book, well implementations Julia keen newer --coming language. made efforts keep code simple possible, tried avoid use many external packages. part, readers see (especially earlier chapters) code blocks short simple, relying wherever possible base R functionality. doubt neater effective ways code material book using wide array R packages—illustrated Chapter 10—priority keep code simple, consistent easily reproducible.wish follow method theory without implementations book, need read chapter. However, style book use implementation illustrate theory practice, tolerance many code blocks necessary read onward.wish simply replicate models quickly possible, able avail code block copying feature, appears whenever scroll input code block. Assuming required external packages installed, code blocks transportable immediately usable. extra-inquisitive want explore constructed graphics used illustration (code usually displayed), best place go Github repository book.chapter wish learn methods book know use R. However, intended full tutorial R. many qualified individuals existing resources better serve purpose—particular recommend Wickham Grolemund (2016). recommended consult resources become comfortable basics R proceeding later chapters book. However, acknowledging many want dive sooner rather later, chapter covers absolute basics R allow uninitiated reader proceed least orientation.","code":""},{"path":"the-basics-of-the-r-programming-language.html","id":"what-is-r","chapter":"2 The Basics of the R Programming Language","heading":"2.1 What is R?","text":"R programming language originally developed statisticians, recent years capabilities environments used expanded greatly, extensive use nowadays academia public private sectors. many advantages using programming language like R. :completely free open source.faster efficient memory popular graphical user interface analytics tools.facilitates easier replication analysis person person compared many alternatives.large growing global community active users.large rapidly growing universe packages, free provide ability extremely wide range general highly specialized tasks, statistical otherwise.often heated debate tools better non-trivial statistical analysis. personally find R provides widest array resources interested inferential modeling, Python well-developed toolkit predictive modeling machine learning. Since primary focus book inferential modeling, -depth walkthroughs coded R.","code":""},{"path":"the-basics-of-the-r-programming-language.html","id":"how-to-start-using-r","chapter":"2 The Basics of the R Programming Language","heading":"2.2 How to start using R","text":"Just like programming languages, R interpreter receives input returns output. easy use without IDE. IDE Integrated Development Environment, convenient user interface allowing R programmer main tasks including writing running R code, saving files, viewing data plots, integrating code documents many things. far popular IDE R RStudio. example RStudio IDE looks like can seen Figure 2.1.\nFigure 2.1: RStudio IDE\nstart using R, follow steps:Download install latest version R https://www.r-project.org/. Ensure version suits operating system.Download latest version RStudio IDE https://rstudio.com/products/rstudio/ view video page familiarize features.Open RStudio play around.initial stages using R can challenging, mostly due need become familiar R understands, stores processes data. Extensive trial error learning necessity. Perseverance important early stages, well openness seek help others either person via online forums.","code":""},{"path":"the-basics-of-the-r-programming-language.html","id":"data-in-r","chapter":"2 The Basics of the R Programming Language","heading":"2.3 Data in R","text":"start tasks involving data R, generally want store things create can refer later. Simply calculating something store R. example, simple calculation like can performed easily:However, soon calculation complete, forgotten R result hasn’t assigned anywhere. store something R session, assign name using <- operator. can assign previous calculation object called my_sum, allows access value time.see can comment code simply adding # start line ensure line ignored interpreter.Note assignment object result value displayed. display value, name object must typed, print() command used command wrapped parentheses.","code":"\n3 + 3## [1] 6\n# store the result\nmy_sum <- 3 + 3\n\n# now I can work with it\nmy_sum + 3## [1] 9\n# show me the value of my_sum\nmy_sum## [1] 6\n# assign my_sum + 3 to new_sum and show its value\n(new_sum <- my_sum + 3)## [1] 9"},{"path":"the-basics-of-the-r-programming-language.html","id":"data-types","chapter":"2 The Basics of the R Programming Language","heading":"2.3.1 Data types","text":"data R associated type, reflect wide range data R able work . typeof() function can used see type single scalar value. Let’s look common scalar data types.Numeric data can integer form double (decimal) form.Character data text data surrounded single double quotes.Logical data takes form TRUE FALSE.","code":"\n# integers can be signified by adding an 'L' to the end\nmy_integer <- 1L  \nmy_double <- 6.38\n\ntypeof(my_integer)## [1] \"integer\"\ntypeof(my_double)## [1] \"double\"\nmy_character <- \"THIS IS TEXT\"\ntypeof(my_character)## [1] \"character\"\nmy_logical <- TRUE\ntypeof(my_logical)## [1] \"logical\""},{"path":"the-basics-of-the-r-programming-language.html","id":"homogeneous-data-structures","chapter":"2 The Basics of the R Programming Language","heading":"2.3.2 Homogeneous data structures","text":"Vectors one-dimensional structures containing data type notated using c(). type vector can also viewed using typeof() function, str() function can used display contents vector type.Categorical data—takes finite number possible values—can stored factor vector make easier perform grouping manipulation.needed, factors can given order.number elements vector can seen using length() function.Simple numeric sequence vectors can created using shorthand notation.try mix data types inside vector, usually result type coercion, one types forced different type ensure homogeneity. Often means vector become character vector.sometimes logical factor types coerced numeric.Matrices two-dimensional data structures type built vector defining number rows columns. Data read matrix columns, starting left moving right. Matrices rarely used non-numeric data types.Arrays n-dimensional data structures data type used extensively R users.","code":"\nmy_double_vector <- c(2.3, 6.8, 4.5, 65, 6)\nstr(my_double_vector)##  num [1:5] 2.3 6.8 4.5 65 6\ncategories <- factor(\n  c(\"A\", \"B\", \"C\", \"A\", \"C\")\n)\n\nstr(categories)##  Factor w/ 3 levels \"A\",\"B\",\"C\": 1 2 3 1 3\n# character vector \nranking <- c(\"Medium\", \"High\", \"Low\")\nstr(ranking)##  chr [1:3] \"Medium\" \"High\" \"Low\"\n# turn it into an ordered factor\nranking_factors <- ordered(\n  ranking, levels = c(\"Low\", \"Medium\", \"High\")\n)\n\nstr(ranking_factors)##  Ord.factor w/ 3 levels \"Low\"<\"Medium\"<..: 2 3 1\nlength(categories)## [1] 5\n(my_sequence <- 1:10)##  [1]  1  2  3  4  5  6  7  8  9 10\n# numeric sequence vector\nvec <- 1:5\nstr(vec)##  int [1:5] 1 2 3 4 5\n# create a new vector containing vec and the character \"hello\"\nnew_vec <- c(vec, \"hello\")\n\n# numeric values have been coerced into their character equivalents\nstr(new_vec)##  chr [1:6] \"1\" \"2\" \"3\" \"4\" \"5\" \"hello\"\n# attempt a mixed logical and numeric\nmix <- c(TRUE, 6)\n\n# logical has been converted to binary numeric (TRUE = 1)\nstr(mix)##  num [1:2] 1 6\n# try to add a numeric to our previous categories factor vector\nnew_categories <- c(categories, 1)\n\n# categories have been coerced to background integer representations\nstr(new_categories)##  num [1:6] 1 2 3 1 3 1\n# create a 2x2 matrix with the first four integers\n(m <- matrix(c(1, 2, 3, 4), nrow = 2, ncol = 2))##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4"},{"path":"the-basics-of-the-r-programming-language.html","id":"heterogeneous-data-structures","chapter":"2 The Basics of the R Programming Language","heading":"2.3.3 Heterogeneous data structures","text":"Lists one-dimensional data structures can take data type.List elements can data type dimension. element can given name.Named list elements can accessed using $.Dataframes used data structure R; effectively named list vectors length, vector column. , dataframe similar nature typical database table spreadsheet.","code":"\nmy_list <- list(6, TRUE, \"hello\")\nstr(my_list)## List of 3\n##  $ : num 6\n##  $ : logi TRUE\n##  $ : chr \"hello\"\nnew_list <- list(\n  scalar = 6, \n  vector = c(\"Hello\", \"Goodbye\"), \n  matrix = matrix(1:4, nrow = 2, ncol = 2)\n)\n\nstr(new_list)## List of 3\n##  $ scalar: num 6\n##  $ vector: chr [1:2] \"Hello\" \"Goodbye\"\n##  $ matrix: int [1:2, 1:2] 1 2 3 4\nnew_list$matrix##      [,1] [,2]\n## [1,]    1    3\n## [2,]    2    4\n# two vectors of different types but same length\nnames <- c(\"John\", \"Ayesha\")\nages <- c(31, 24)\n\n# create a dataframe\n(df <- data.frame(names, ages))##    names ages\n## 1   John   31\n## 2 Ayesha   24\n# get types of columns\nstr(df)## 'data.frame':    2 obs. of  2 variables:\n##  $ names: chr  \"John\" \"Ayesha\"\n##  $ ages : num  31 24\n# get dimensions of df\ndim(df)## [1] 2 2"},{"path":"the-basics-of-the-r-programming-language.html","id":"working-with-dataframes","chapter":"2 The Basics of the R Programming Language","heading":"2.4 Working with dataframes","text":"dataframe common data structure used analysts R, due similarity data tables found databases spreadsheets. work almost entirely dataframes book, let’s get know .","code":""},{"path":"the-basics-of-the-r-programming-language.html","id":"loading-and-tidying-data-in-dataframes","chapter":"2 The Basics of the R Programming Language","heading":"2.4.1 Loading and tidying data in dataframes","text":"work data R, usually need pull outside source dataframe3. R facilitates numerous ways importing data simple .csv files, Excel files, online sources databases. Let’s load data set use later—salespeople data set, contains information sales, average customer ratings performance ratings salespeople. read.csv() function can accept URL address file online.might want display entire data set knowing big . can view dimensions, big display, can use head() function display just first rows.can view specific column using $, can use square brackets view specific entry. example wanted see 6th entry sales column:Alternatively, can use [row, column] index get specific entry dataframe.can take look data types using str().can also see statistical summary column using summary(), tells us various statistics depending type column.Note missing data dataframe, indicated NAs summary. Missing data identified special NA value R. confused \"NA\", simply character string. function .na() look values vector dataframe return TRUE FALSE based whether NA . adding using sum() function, take TRUE 1 FALSE 0, effectively provides count missing data.small number NAs given dimensions data set might want remove rows data contain NAs. easiest way use complete.cases() function, identifies rows NAs, can select rows dataframe based condition. Note can overwrite objects name R.can see unique values vector column using unique() function.need change type column dataframe, can use .numeric(), .character(), .logical() .factor() functions. example, given four unique values performance column, may want convert factor.","code":"\n# url of data set \nurl <- \"http://peopleanalytics-regression-book.org/data/salespeople.csv\"\n\n# load the data set and store it as a dataframe called salespeople\nsalespeople <- read.csv(url)\ndim(salespeople)## [1] 351   4\n# hundreds of rows, so view first few\nhead(salespeople)##   promoted sales customer_rate performance\n## 1        0   594          3.94           2\n## 2        0   446          4.06           3\n## 3        1   674          3.83           4\n## 4        0   525          3.62           2\n## 5        1   657          4.40           3\n## 6        1   918          4.54           2\nsalespeople$sales[6]## [1] 918\nsalespeople[34, 4]## [1] 3\nstr(salespeople)## 'data.frame':    351 obs. of  4 variables:\n##  $ promoted     : int  0 0 1 0 1 1 0 0 0 0 ...\n##  $ sales        : int  594 446 674 525 657 918 318 364 342 387 ...\n##  $ customer_rate: num  3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ...\n##  $ performance  : int  2 3 4 2 3 2 3 1 3 3 ...\nsummary(salespeople)##     promoted          sales       customer_rate    performance \n##  Min.   :0.0000   Min.   :151.0   Min.   :1.000   Min.   :1.0  \n##  1st Qu.:0.0000   1st Qu.:389.2   1st Qu.:3.000   1st Qu.:2.0  \n##  Median :0.0000   Median :475.0   Median :3.620   Median :3.0  \n##  Mean   :0.3219   Mean   :527.0   Mean   :3.608   Mean   :2.5  \n##  3rd Qu.:1.0000   3rd Qu.:667.2   3rd Qu.:4.290   3rd Qu.:3.0  \n##  Max.   :1.0000   Max.   :945.0   Max.   :5.000   Max.   :4.0  \n##                   NA's   :1       NA's   :1       NA's   :1\nsum(is.na(salespeople))## [1] 3\nsalespeople <- salespeople[complete.cases(salespeople), ]\n\n# confirm no NAs\nsum(is.na(salespeople))## [1] 0\nunique(salespeople$performance)## [1] 2 3 4 1\nsalespeople$performance <- as.factor(salespeople$performance)\nstr(salespeople)## 'data.frame':    350 obs. of  4 variables:\n##  $ promoted     : int  0 0 1 0 1 1 0 0 0 0 ...\n##  $ sales        : int  594 446 674 525 657 918 318 364 342 387 ...\n##  $ customer_rate: num  3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ...\n##  $ performance  : Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 2 3 4 2 3 2 3 1 3 3 ..."},{"path":"the-basics-of-the-r-programming-language.html","id":"manipulating-dataframes","chapter":"2 The Basics of the R Programming Language","heading":"2.4.2 Manipulating dataframes","text":"Dataframes can subsetted contain rows satisfy specific conditions.Note use ==, used many programming languages, test precise equality. Similarly can select columns based inequalities (> ‘greater ’‍, < ‘less ’‍, >= ‘greater equal ’‍, <= ‘less equal ’‍, != ‘equal ’). example:select specific columns use select argument.Two dataframes column names can combined rows.Two dataframes different column names can combined columns.","code":"\n(sales_720 <- subset(salespeople, subset = sales == 720))##     promoted sales customer_rate performance\n## 290        1   720          3.76           3\nhigh_sales <- subset(salespeople, subset = sales >= 700)\nhead(high_sales)##    promoted sales customer_rate performance\n## 6         1   918          4.54           2\n## 12        1   716          3.16           3\n## 20        1   937          5.00           2\n## 21        1   702          3.53           4\n## 25        1   819          4.45           2\n## 26        1   736          3.94           4\nsalespeople_sales_perf <- subset(salespeople, \n                                 select = c(\"sales\", \"performance\"))\nhead(salespeople_sales_perf)##   sales performance\n## 1   594           2\n## 2   446           3\n## 3   674           4\n## 4   525           2\n## 5   657           3\n## 6   918           2\nlow_sales <- subset(salespeople, subset = sales < 400)\n\n# bind the rows of low_sales and high_sales together\nlow_and_high_sales = rbind(low_sales, high_sales)\nhead(low_and_high_sales)##    promoted sales customer_rate performance\n## 7         0   318          3.09           3\n## 8         0   364          4.89           1\n## 9         0   342          3.74           3\n## 10        0   387          3.00           3\n## 15        0   344          3.02           2\n## 16        0   372          3.87           3\n# two dataframes with two columns each\nsales_perf <- subset(salespeople, \n                     select = c(\"sales\", \"performance\"))\nprom_custrate <- subset(salespeople, \n                        select = c(\"promoted\", \"customer_rate\"))\n\n# bind the columns to create a dataframe with four columns\nfull_df <- cbind(sales_perf, prom_custrate)\nhead(full_df)##   sales performance promoted customer_rate\n## 1   594           2        0          3.94\n## 2   446           3        0          4.06\n## 3   674           4        1          3.83\n## 4   525           2        0          3.62\n## 5   657           3        1          4.40\n## 6   918           2        1          4.54"},{"path":"the-basics-of-the-r-programming-language.html","id":"functions-packages-and-libraries","chapter":"2 The Basics of the R Programming Language","heading":"2.5 Functions, packages and libraries","text":"code far used variety functions. example head(), subset(), rbind(). Functions operations take certain defined inputs return output. Functions exist perform common useful operations.","code":""},{"path":"the-basics-of-the-r-programming-language.html","id":"using-functions","chapter":"2 The Basics of the R Programming Language","heading":"2.5.1 Using functions","text":"Functions usually take one arguments. Often large number arguments function can take, many optional required specified user. example, function head(), displays first rows dataframe4, one required argument x: name dataframe. second argument optional, n: number rows display. n entered, assumed default value n = 6.running function, can either specify arguments name can enter order without names. enter arguments without naming , R expects arguments entered exactly right order.","code":"\n# see the head of salespeople, with the default of six rows\nhead(salespeople)##   promoted sales customer_rate performance\n## 1        0   594          3.94           2\n## 2        0   446          4.06           3\n## 3        1   674          3.83           4\n## 4        0   525          3.62           2\n## 5        1   657          4.40           3\n## 6        1   918          4.54           2\n# see fewer rows - arguments need to be in the right order if not named\nhead(salespeople, 3)##   promoted sales customer_rate performance\n## 1        0   594          3.94           2\n## 2        0   446          4.06           3\n## 3        1   674          3.83           4\n# or if you don't know the right order, \n# name your arguments and you can put them in any order\nhead(n = 3, x = salespeople)##   promoted sales customer_rate performance\n## 1        0   594          3.94           2\n## 2        0   446          4.06           3\n## 3        1   674          3.83           4"},{"path":"the-basics-of-the-r-programming-language.html","id":"help-with-functions","chapter":"2 The Basics of the R Programming Language","heading":"2.5.2 Help with functions","text":"functions R excellent help documentation. get help head() function, type help(head) ?head. display results Help browser window RStudio. Alternatively can open Help browser window directly RStudio search . example browser results head() Figure 2.2.\nFigure 2.2: Results search head() function RStudio Help browser\nhelp page normally shows following:Description purpose functionUsage examples, can quickly see usedArguments list can see names order argumentsDetails notes considerations useExpected value output (example head() expected return similar object first input x)Examples help orient (sometimes examples can abstract nature helpful users)","code":""},{"path":"the-basics-of-the-r-programming-language.html","id":"writing-your-own-functions","chapter":"2 The Basics of the R Programming Language","heading":"2.5.3 Writing your own functions","text":"Functions limited come packaged R. Users can write functions perform tasks helpful objectives. Experienced programmers languages subscribe principle called DRY (Don’t Repeat ). Whenever task needs done repeatedly, poor practice write code numerous times. makes sense write function task.example, simple function written generates report dataframe:can test function using built-mtcars data set R.","code":"\n# create df_report function\ndf_report <- function(df) {\n  paste(\"This dataframe contains\", nrow(df), \"rows and\", \n        ncol(df), \"columns. There are\", sum(is.na(df)), \"NA entries.\")\n}\ndf_report(mtcars)## [1] \"This dataframe contains 32 rows and 11 columns. There are 0 NA entries.\""},{"path":"the-basics-of-the-r-programming-language.html","id":"installing-packages","chapter":"2 The Basics of the R Programming Language","heading":"2.5.4 Installing packages","text":"common functions used far exist base R installation. However, beauty open source languages like R users can write functions resources release others via packages. package additional module can installed easily; makes resources available base R installation. book using functions base R popular useful packages. example, popular package used statistical modeling MASS package, based methods popular applied statistics book5.external package can used, must installed package library using install.packages(). install MASS, type install.packages(\"MASS\") console. send R main internet repository R packages (known CRAN). find right version MASS operating system download install package library. MASS needs packages order work, also install packages.want install one package, put names packages inside character vector—example:installed package, can see functions available calling help , example using help(package = MASS). One package may wish install now peopleanalyticsdata package, contains data sets used book. installing loading package, data sets used book loaded R session ready work . , can ignore read.csv() commands later book, download data internet.","code":"\nmy_packages <- c(\"MASS\", \"DescTools\", \"dplyr\")\ninstall.packages(my_packages)"},{"path":"the-basics-of-the-r-programming-language.html","id":"using-packages","chapter":"2 The Basics of the R Programming Language","heading":"2.5.5 Using packages","text":"installed package package library, use R session need load using library() function. example, load MASS installing , use library(MASS). Often nothing happen use command, rest assured package loaded can start use functions inside . Sometimes load package series messages display, usually make aware certain things need keep mind using package. Note whenever see library() command book, assumed already installed package command. , library() command fail.package loaded library, can use functions inside . example, stepAIC() function available load MASS package becomes available loaded. sense, functions ‘belong’ packages.Problems can occur load packages contain functions name functions already exist R session. Often messages see loading package alert . R faced situation function exists multiple packages loaded, R always defaults function recently loaded package. may always intended.One way completely avoid issue get habit namespacing functions. namespace, simply use package::function(), safely call stepAIC() MASS, use MASS::stepAIC(). time book function called package outside base R, use namespacing call function. help avoid confusion packages used functions.","code":""},{"path":"the-basics-of-the-r-programming-language.html","id":"the-pipe-operator","chapter":"2 The Basics of the R Programming Language","heading":"2.5.6 The pipe operator","text":"Even elementary briefing R, difficult ignore pipe operator. pipe operator makes code natural read write reduces typical computing problem many nested operations inside parentheses. pipe operator comes inside many R packages, particularly magrittr dplyr.example, imagine wanted following two operations one command:Subset salespeople sales values sales less 500Take mean valuesIn base R, one way :nested needs read inside order align instructions. pipe operator %>% takes command comes places inside function follows (default first argument). reduces complexity allows follow logic clearly.can extended perform arbitrarily many operations one piped command.pipe operator unique R widely used helps make code readable, reduces complexity, helps orient around common ‘grammar’ manipulation data. pipe operator helps structure code clearly around nouns (objects), verbs (functions) adverbs (arguments functions). One developed sets packages R follows principles tidyverse family packages, encourage explore.","code":"\nmean(subset(salespeople$sales, subset = salespeople$sales < 500))## [1] 388.6684\n# load magrittr library to get the pipe operator\nlibrary(magrittr)\n\n# use the pipe operator to lay out the steps more logically\nsubset(salespeople$sales, subset = salespeople$sales < 500) %>% \n  mean() ## [1] 388.6684\nsalespeople$sales %>% # start with all data\n  subset(subset = salespeople$sales < 500) %>% # get the subsetted data\n  mean() %>% # take the mean value\n  round() # round to the nearest integer## [1] 389"},{"path":"the-basics-of-the-r-programming-language.html","id":"errors-warnings-and-messages","chapter":"2 The Basics of the R Programming Language","heading":"2.6 Errors, warnings and messages","text":"mentioned earlier chapter, getting familiar R can frustrating beginning never programmed . can expect regularly see messages, warnings errors response commands. encourage regard friend rather enemy. tempting take latter approach starting , time hope appreciate wisdom words.Errors serious problems usually result halting code failure return requested output. usually come indication source error, can sometimes easy understand sometimes frustratingly vague abstract. example, easy--understand error :helps see used sales = 720 condition subset data, used sales == 720 precise equality.much challenging error understand :first faced error can’t understand, try get frustrated proceed knowledge usually can fixed easily quickly. Often problem much obvious think, , still 99% likelihood others made error can read online. first step take look code see can spot wrong. case, may see used square brackets [] instead parentheses () calling head() function. see wrong, next step ask colleague internet search text error message receive, consult online forums like https://stackoverflow.com. experienced become, easier interpret error messages.Warnings less serious usually alert something might overlooking indicate problem output. many cases can ignore warnings, sometimes important reminder go back edit code. example, may run model doesn’t converge, stop R returning results, also useful know didn’t converge.Messages pieces information may may useful particular point time. Sometimes receive messages load package library. Sometimes messages keep date progress process taking long time execute.","code":"subset(salespeople, subset = sales = 720)Error: unexpected '=' in \"subset(salespeople, subset = sales =\"\nhead[salespeople]Error in head[salespeople] : object of type 'closure' is not subsettable"},{"path":"the-basics-of-the-r-programming-language.html","id":"plotting-and-graphing","chapter":"2 The Basics of the R Programming Language","heading":"2.7 Plotting and graphing","text":"might expect well-developed programming language, numerous ways plot graph information R. exploratory data analysis fairly simple data don’t need worry pretty appearance formatting, built-plot capabilities base R fine. need pretty appearance, precision, color coding even 3D graphics animation, also specialized plotting graphing packages purposes. general working interactively RStudio, graphical output rendered Plots pane, can copy save image.","code":""},{"path":"the-basics-of-the-r-programming-language.html","id":"plotting-in-base-r","chapter":"2 The Basics of the R Programming Language","heading":"2.7.1 Plotting in base R","text":"simplest plot function base R plot(). performs basic X-Y plotting. example, code generate scatter plot customer_rate sales salespeople data set, results displayed Figure 2.3. Note use arguments main, xlab ylab customizing axis labels title plot.\nFigure 2.3: Simple scatterplot customer_rate sales salespeople data set\nHistograms data can generated using hist() function. command generate histogram performance displayed Figure 2.4. Note use breaks customize bars appear.\nFigure 2.4: Simple histogram performance salespeople data set\nBox whisker plots excellent ways see distribution variable, can grouped another variable see bivariate patterns. example, command show box whisker plot sales grouped performance, output shown Figure 2.5. Note use formula data notation define variable interested want grouped. study formula notation greater depth later book.\nFigure 2.5: Simple box plot sales grouped performance salespeople data set\namong common plots used data exploration purposes. examples wider range plotting graphing functions available base R, line plots, bar plots varieties may see later book.","code":"\n# scatter plot of customer_rate against sales\nplot(x = salespeople$sales, y = salespeople$customer_rate,\n     xlab = \"Sales ($m)\", ylab = \"Average customer rating\",\n     main = \"Scatterplot of Sales vs Customer Rating\")\n# convert performance ratings back to numeric data type for histogram\nsalespeople$performance <- as.numeric(salespeople$performance)\n\n# histogram of performance ratings\nhist(salespeople$performance, breaks = 0:4,\n     xlab = \"Performance Rating\", \n     main = \"Histogram of Performance Ratings\")\n# box plot of sales by performance rating\nboxplot(formula = sales ~ performance, data = salespeople,\n        xlab = \"Performance Rating\", ylab = \"Sales ($m)\",\n        main = \"Boxplot of Sales by Performance Rating\")"},{"path":"the-basics-of-the-r-programming-language.html","id":"specialist-plotting-and-graphing-packages","chapter":"2 The Basics of the R Programming Language","heading":"2.7.2 Specialist plotting and graphing packages","text":"far commonly used specialist plotting graphing package R ggplot2. ggplot2 allows flexible construction wide range charts graphs, uses specific command grammar can take getting used . However, learned, ggplot2 can extremely powerful tool. Many illustratory figures used book developed using ggplot2 code figures generally included sake brevity, can always find source code book Github6. great learning resource ggplot2 Wickham (2016).plotly package allows use plotly graphing library R. excellent package interactive graphing used 3D illustrations book. Output can rendered HTML—allowing user play explore graphs interactively—can saved static 2D images.GGally package extends ggplot2 allow easy combination charts graphs. particularly valuable quicker exploratory data analysis. One popular functions ggpairs(), produces pairplot. pairplot visualization univariate bivariate patterns data set, univariate distributions diagonal bivariate relationships correlations displayed -diagonal. Figure 2.6 example pairplot salespeople data set, explore Chapter 5.\nFigure 2.6: Pairplot salespeople data set\n","code":"\nlibrary(GGally)\n\n# convert performance and promotion to categorical\nsalespeople$promoted <- as.factor(salespeople$promoted)\nsalespeople$performance <- as.factor(salespeople$performance)\n\n# pairplot of salespeople\nGGally::ggpairs(salespeople)"},{"path":"the-basics-of-the-r-programming-language.html","id":"documenting-your-work-using-r-markdown","chapter":"2 The Basics of the R Programming Language","heading":"2.8 Documenting your work using R Markdown","text":"anyone performing sort multivariate analysis using statistical programming language, appropriate documentation reproducibility work essential success longevity. code easily obtained run others, likely limited impact lifetime. Learning create integrated documents contain text code critical providing access code narration work.R Markdown package allows create integrated documents containing formatted text executed code. , opinion, one best resources available currently purpose. entire book created using R Markdown. can start R Markdown document RStudio installing rmarkdown package opening new R Markdown document file, suffix .Rmd.R Markdown documents always start particular heading type called YAML header, contains overall information document creating. Care must taken precise formatting YAML header, sensitive spacing indentation. Usually basic YAML header created RStudio start new .Rmd file. example.output part header numerous options, commonly used html_document, generates document web page, pdf_document, generates document PDF using open source LaTeX software package. wish create PDF documents need version LaTeX installed system. One R package can easily tinytex package. function install_tinytex() package install minimal version LaTeX fine purposes.R Markdown allows build formatted document using many shorthand formatting commands. examples format headings place web links images document:Code can written executed results displayed inline using backticks. example, writinginline display 351 final document. Entire code blocks can included executed using triple-backticks. following code block:display output:{} wrapping allows specify different languages code chunk. example, wanted run Python code instead R code can use {python}. also allows set options code chunk display separated commas. example, want results code displayed, without code displayed, can use {r, echo = FALSE}.process compiling R Markdown code produce document known ‘knitting’. create knitted document, simply need click ‘Knit’ button RStudio appears R Markdown code.familiar R Markdown, strongly encourage learn alongside R challenge write practice exercises take book using R Markdown. Useful cheat sheets reference guides R Markdown formatting commands available Cheatsheets section Help menu RStudio. also recommend Xie, Dervieux, Riederer (2020) really thorough instruction reference guide.","code":"---\ntitle: \"My new document\"\nauthor: \"Keith McNulty\"\ndate: \"25/01/2021\"\noutput: html_document\n---# My top heading\n\nThis section is about this general topic.\n\n## My first sub heading \n\nTo see more information on this sub-topic visit [here](https://my.web.link).\n\n## My second sub heading\n\nHere is a nice picture about this sub-topic.\n\n![](path/to/image)`r nrow(salespeople)````{r}\n# show the first few rows of salespeople\nhead(salespeople)\n```##   promoted sales customer_rate performance\n## 1        0   594          3.94           2\n## 2        0   446          4.06           3\n## 3        1   674          3.83           4\n## 4        0   525          3.62           2\n## 5        1   657          4.40           3\n## 6        1   918          4.54           2"},{"path":"the-basics-of-the-r-programming-language.html","id":"learning-exercises","chapter":"2 The Basics of the R Programming Language","heading":"2.9 Learning exercises","text":"","code":""},{"path":"the-basics-of-the-r-programming-language.html","id":"discussion-questions","chapter":"2 The Basics of the R Programming Language","heading":"2.9.1 Discussion questions","text":"Describe following data types: numeric, character, logical, factor.vector known homogeneous data structure?Give example heterogeneous data structure R.difference NA \"NA\"?operator used return named elements list named columns dataframe?Describe functions used manipulate dataframes.package install use new package?Describe meant ‘namespacing’ might useful.pipe operator, popular R?difference error warning R?Name simple plotting functions base R.Name common specialist plotting graphing packages R.R Markdown, useful someone performing analysis using programming languages?","code":""},{"path":"the-basics-of-the-r-programming-language.html","id":"data-exercises","chapter":"2 The Basics of the R Programming Language","heading":"2.9.2 Data exercises","text":"Create character vector called my_names contains first, middle last names elements. Calculate length my_names.Create second numeric vector called corresponds my_names. entries position name order full name. Verify length my_names.Create dataframe called names, consists two vectors my_names columns. Calculate dimensions names.Create new dataframe new_names column converted character type. Verify command worked using str().Load ugtests data set via peopleanalyticsdata package download internet7. Calculate dimensions ugtests view first three rows .View statistical summary columns ugtests. Determine missing values.View subset ugtests values Yr1 greater 50.Install load package dplyr. Look help filter() function package try use repeat task previous question.Write code find mean Yr1 test scores achieved Yr3 test scores greater 100. Round mean nearest integer.Familiarize two functions filter() pull() dplyr. Use functions try calculation previous question using single unbroken piped command. sure namespace necessary.Create scatter plot using ugtests data Final scores \\(y\\) axis Yr3 scores \\(x\\) axis.Create 5-level grading logic use create new finalgrade column ugtests data set grades 1–5 increasing attainment based Final score ugtests. Generate histogram finalgrade column.Using new ugtests data extra column previous exercise, create box plot Yr3 scores grouped finalgrade.Knit answers exercises R Markdown document. Create one version displays code answers, another just displays answers.","code":""},{"path":"found-stats.html","id":"found-stats","chapter":"3 Statistics Foundations","heading":"3 Statistics Foundations","text":"properly understand multivariate models, analyst needs decent grasp foundational statistics. Many assumptions results multivariate models require understanding foundations order properly interpreted. three topics particularly important proceeding book:Descriptive statistics populations samplesDistribution random variablesHypothesis testingIf never really studied topics, strongly recommend taking course spending good time getting know . , just last chapter intended comprehensive tutorial R, neither chapter intended comprehensive tutorial introductory statistics. However, introduce key concepts critical understanding later chapters, always illustrate using real data examples.preparation chapter going download data set work later chapter, use practical examples illustration purposes. data set information sales, customer ratings performance ratings set 351 salespeople well indication whether promoted.Let’s take brief look first rows data make sure know inside .let’s understand structure data.looks like:promoted binary value, either 1 0, indicating ‘promoted’ ‘promoted’, respectively.sales customer_rate look like normal numerical values.performance looks like set performance categories—appear four based can see.","code":"\n# if needed, use online url to download salespeople data\nurl <- \"http://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople <- read.csv(url)\nhead(salespeople)##   promoted sales customer_rate performance\n## 1        0   594          3.94           2\n## 2        0   446          4.06           3\n## 3        1   674          3.83           4\n## 4        0   525          3.62           2\n## 5        1   657          4.40           3\n## 6        1   918          4.54           2\nstr(salespeople)## 'data.frame':    351 obs. of  4 variables:\n##  $ promoted     : int  0 0 1 0 1 1 0 0 0 0 ...\n##  $ sales        : int  594 446 674 525 657 918 318 364 342 387 ...\n##  $ customer_rate: num  3.94 4.06 3.83 3.62 4.4 4.54 3.09 4.89 3.74 3 ...\n##  $ performance  : int  2 3 4 2 3 2 3 1 3 3 ..."},{"path":"found-stats.html","id":"elementary-descriptive-statistics-of-populations-and-samples","chapter":"3 Statistics Foundations","heading":"3.1 Elementary descriptive statistics of populations and samples","text":"collection numerical data one variables can described using number common statistical concepts. Let \\(x = x_1, x_2, \\dots, x_n\\) sample \\(n\\) observations variable drawn population.","code":""},{"path":"found-stats.html","id":"mean-var-sd","chapter":"3 Statistics Foundations","heading":"3.1.1 Mean, variance and standard deviation","text":"mean average value observations defined adding values dividing number observations. mean \\(\\bar{x}\\) sample \\(x\\) defined :\\[\n\\bar{x} = \\frac{1}{n}\\sum_{= 1}^{n}x_i\n\\]\nmean sample \\(x\\) denoted \\(\\bar{x}\\), mean entire population usually denoted \\(\\mu\\). mean can different interpretation depending type data studied. Let’s look mean three different columns salespeople data, making sure ignore missing data.looks intuitive appears average amount sales made individuals data set.Given data can value 0 1, interpret mean likelihood expectation individual labeled 1. , average probability promotion data set. data showed perfectly random likelihood promotion, expect take value 0.5. lower 0.5, tells us majority individuals promoted.Given data can values 1, 2, 3 4, interpret expected value performance rating data set. Higher lower means inform us distribution performance ratings. low mean indicate skew towards low rating, high mean indicate skew towards high rating.common statistical summary measures include median, middle value values ranked order, mode, frequently occurring value.variance measure much data varies around mean. two different definitions variance. population variance assumes working entire population defined average squared difference mean:\\[\n\\mathrm{Var}_p(x) = \\frac{1}{n}\\sum_{= 1}^{n}(x_i - \\bar{x})^2\n\\]\nsample variance assumes working sample attempts estimate variance larger population applying Bessel’s correction account potential sampling error. sample variance :\\[\n\\mathrm{Var}_s(x) = \\frac{1}{n-1}\\sum_{= 1}^{n}(x_i - \\bar{x})^2\n\\]can see \\[\n\\mathrm{Var}_p(x) = \\frac{n - 1}{n}\\mathrm{Var}_s(x)\n\\]\ndata set gets larger, sample variance population variance become less less distinguishable, intuitively makes sense.rarely work full populations, sample variance calculated default R many statistical software packages.necessary, need apply transformation get population variance.Variance intuitive scale relative data studied, used ‘squared distance metric’‍, therefore can square-root get measure ‘deviance’ scale data. call standard deviation \\(\\sigma(x)\\), \\(\\mathrm{Var}(x) = \\sigma(x)^2\\). variance, standard deviation population sample versions, sample version calculated default. Conversion two takes form\\[\n\\sigma_p(x) = \\sqrt{\\frac{n-1}{n}}\\sigma_s(x)\n\\]Given range sales [151, 945] mean 527, see standard deviation gives intuitive sense ‘spread’ data relative inherent scale.","code":"\nmean(salespeople$sales, na.rm = TRUE)## [1] 527.0057\nmean(salespeople$promoted, na.rm = TRUE)## [1] 0.3219373\nmean(salespeople$performance, na.rm = TRUE)## [1] 2.5\n# sample variance \n(sample_variance_sales <- var(salespeople$sales, na.rm = TRUE))## [1] 34308.11\n# population variance (need length of non-NA data)\nn <- length(na.omit(salespeople$sales))\n(population_variance_sales <- ((n-1)/n) * sample_variance_sales)## [1] 34210.09\n# sample standard deviation\n(sample_sd_sales <- sd(salespeople$sales, na.rm = TRUE))## [1] 185.2245\n# verify that sample sd is sqrt(sample var)\nsample_sd_sales == sqrt(sample_variance_sales)## [1] TRUE\n# calculate population standard deviation\n(population_sd_sales <- sqrt((n-1)/n) * sample_sd_sales)## [1] 184.9597"},{"path":"found-stats.html","id":"covariance-and-correlation","chapter":"3 Statistics Foundations","heading":"3.1.2 Covariance and correlation","text":"covariance two variables measure extent one changes changes. \\(y = y_1, y_2, \\dots, y_n\\) second variable, \\(\\bar{x}\\) \\(\\bar{y}\\) means \\(x\\) \\(y\\), respectively, sample covariance \\(x\\) \\(y\\) defined \\[\n\\mathrm{cov}_s(x, y) = \\frac{1}{n - 1}\\sum_{= 1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})\n\\]\nvariance, population covariance \\[\n\\mathrm{cov}_p(x, y) = \\frac{n-1}{n}\\mathrm{cov}_s(x, y)\n\\], sample covariance default R, need transform obtain population covariance.can seen, difference covariance small sample population versions, confirm positive relationship sales customer rating. However, see issue intuitive sense scale measure.Pearson’s correlation coefficient divides covariance product standard deviations two variables:\\[\nr_{x, y} = \\frac{\\mathrm{cov}(x, y)}{\\sigma(x)\\sigma(y)}\n\\]\ncreates scale \\(-1\\) \\(1\\) \\(r_{x, y}\\), intuitive way understanding direction strength relationship \\(x\\) \\(y\\), \\(-1\\) indicating \\(x\\) increases perfectly \\(y\\) decreases, \\(1\\) indicating \\(x\\) increases perfectly \\(y\\) increases, \\(0\\) indicating relationship two., sample population version correlation coefficient, R calculates sample version default. Similar transformations can used determine population correlation coefficient large samples two measures converge.tells us moderate positive correlation sales customer rating.notice far used two variables continuous scale demonstrate covariance correlation. Pearson’s correlation can also used continuous scale dichotomous (binary) scale variable, known point-biserial correlation.Correlating ranked variables involves adjusted approach leading Spearman’s rho (\\(\\rho\\)) Kendall’s tau (\\(\\tau\\)), among others. dive mathematics , good source Bhattacharya Burman (2016). Spearman’s Kendall’s variant used whenever least one variables ranked variable, variants available R.case, indicate low moderate correlation. Spearman’s rho Kendall’s tau can also used correlate ranked dichotomous variable, known rank-biserial correlation.","code":"\n# get sample covariance for sales and customer_rate, \n# ignoring observations with missing data\n(sample_cov <- cov(salespeople$sales, salespeople$customer_rate, \n                   use = \"complete.obs\"))## [1] 55.81769\n# convert to population covariance (need number of complete obs)\ncols <- subset(salespeople, select = c(\"sales\", \"customer_rate\"))\nn <- nrow(cols[complete.cases(cols), ])\n(population_cov <- ((n-1)/n) * sample_cov)## [1] 55.65821\n# calculate sample correlation between sales and customer_rate\ncor(salespeople$sales, salespeople$customer_rate, use = \"complete.obs\")## [1] 0.337805\ncor(salespeople$sales, salespeople$promoted, use = \"complete.obs\")## [1] 0.8511283\n# spearman's rho correlation\ncor(salespeople$sales, salespeople$performance, \n    method = \"spearman\", use = \"complete.obs\")## [1] 0.2735446\n# kendall's tau correlation\ncor(salespeople$sales, salespeople$performance, \n    method = \"kendall\", use = \"complete.obs\")## [1] 0.2073609"},{"path":"found-stats.html","id":"distribution-of-random-variables","chapter":"3 Statistics Foundations","heading":"3.2 Distribution of random variables","text":"outlined Section 1.2, build model using set sample data infer general relationship larger population. major underlying assumption inference believe real-life variables dealing random nature. example, might trying model drivers voting choice millions people national election, may sample data thousand people. infer nationwide voting intentions sample, assume characteristics voting population random variables.","code":""},{"path":"found-stats.html","id":"sampling-of-random-variables","chapter":"3 Statistics Foundations","heading":"3.2.1 Sampling of random variables","text":"describe variables random, assuming take form independent identically distributed. Using salespeople data example, assuming sales one person data set influenced sales another person data set. case, seems like reasonable assumption, making many (though ) statistical methods used book. However, good recognize scenarios assumption made. example, salespeople worked together serving customers products, individual’s sales represented proportion overall sales customer, say sales data independent identically distributed. case, expect see hierarchy data need adjust techniques accordingly take consideration.central limit theorem, take samples random variable calculate summary statistic sample, statistic random variable, mean converges true population statistic sampling. Let’s test little experiment salespeople data. Figure 3.1 shows results taking 10, 100 1000 different random samples 50, 100 150 salespeople salespeople data set creating histogram resulting mean sales values. can see greater numbers samples (rows) lead normal distribution curve larger sample sizes (across columns) lead ‘spikier’ distribution smaller standard deviation.\nFigure 3.1: Histogram density mean sales salespeople data set based sample sizes 50, 100 150 (columns) 10, 100 1000 samplings (rows)\n","code":""},{"path":"found-stats.html","id":"standard-errors-the-t-distribution-and-confidence-intervals","chapter":"3 Statistics Foundations","heading":"3.2.2 Standard errors, the \\(t\\)-distribution and confidence intervals","text":"One consequence observations Figure 3.1 summary statistics calculated larger sample sizes fall distributions ‘narrower’ hence represent precise estimations population statistic. standard deviation sampled statistic called standard error statistic. special case sampled mean, formula standard error mean can derived \\[\nSE = \\frac{\\sigma}{\\sqrt{n}}\n\\]\n\\(\\sigma\\) (sample) standard deviation \\(n\\) sample size8. confirms standard error mean decreases greater sample size, confirming intuition estimation mean precise larger samples.apply logic salespeople data set, let’s take random sample 100 values customer_rate.can calculate mean sample standard error mean.normal distribution frequency (probability) distribution, can interpret standard error fundamental unit ‘sensitivity’ around sample mean. greater multiples standard errors around sample mean, can greater certainty range contains true population mean.calculate many standard errors need around sample mean 95% probability including true population mean, need use \\(t\\)-distribution. \\(t\\)-distribution essentially approximation normal distribution acknowledging sample estimate true population standard deviation calculate standard error. case dealing single sample mean, use \\(t\\)-distribution \\(n-1\\) degrees freedom. can use qt() function R find standard error multiple associated level certainty need. case, looking true population mean outside top 2.5% bottom 2.5% distribution9.see approximately 1.98 standard errors either side sample mean give us 95% confidence range contains true population mean. called 95% confidence interval10.","code":"\n# set seed for reproducibility of sampling\nset.seed(123)\n\n# generate a sample of 100 observations\ncustrate <- na.omit(salespeople$customer_rate)\nn <- 100\nsample_custrate <- sample(custrate, n)\n# mean\n(sample_mean <- mean(sample_custrate))## [1] 3.6485\n# standard error\n(se <- sd(sample_custrate)/sqrt(n))## [1] 0.08494328\n# get se multiple for 0.975\n(t <- qt(p = 0.975, df = n - 1))## [1] 1.984217\n# 95% confidence interval lower and upper bounds\nlower_bound <- sample_mean - t*se\nupper_bound <- sample_mean + t*se\n\ncat(paste0('[', lower_bound, ', ', upper_bound, ']')) ## [3.47995410046738, 3.81704589953262]"},{"path":"found-stats.html","id":"hyp-tests","chapter":"3 Statistics Foundations","heading":"3.3 Hypothesis testing","text":"Observations distribution statistics samples random variables allow us construct tests hypotheses difference similarity. hypothesis testing useful simple bivariate analysis practice settings, particularly critical later chapters determining whether models useful . go technical examples hypothesis testing, let’s overview logic intuition hypothesis testing works.purpose hypothesis testing establish high degree statistical certainty regarding claim difference population based properties sample. Consistent high burden proof, start hypothesis difference, called null hypothesis. reject null hypothesis statistical properties sample data render unlikely, case confirm alternative hypothesis statistical difference exist population.hypothesis tests can return p-value, maximum probability finding sample results (results extreme unusual sample results) null hypothesis true population. analyst must decide level p-value needed reject null hypothesis. threshold referred significance level \\(\\alpha\\) (alpha). common standard set \\(\\alpha\\) 0.05. , reject null hypothesis p-value find sample results less 0.05. reject null hypothesis \\(\\alpha = 0.05\\), means results observe sample extreme unusual occur chance 1 20 times null hypothesis true. alpha 0.05 standard certainty used research practice, fields study smaller alphas norm, particularly erroneous conclusions might serious consequences.Three common types hypothesis tests are11:Testing difference means two groupsTesting non-zero correlation two variablesTesting difference frequency distributions different categoriesWe go example . case, see three-step process. First, calculate test statistic. Second, determine expected distribution test statistic. Finally, determine calculated statistic falls distribution order assess likelihood sample occurring null hypothesis true. examples, go logic calculation steps needed hypothesis testing, demonstrate simple functions perform steps R. Readers don’t absolutely need know details contained section, strong understanding underlying methods encouraged.","code":""},{"path":"found-stats.html","id":"means-sig","chapter":"3 Statistics Foundations","heading":"3.3.1 Testing for a difference in means (Welch’s \\(t\\)-test)","text":"Imagine asked , general, sales low-performing salespeople different sales high-performing salespeople. question refers salespeople, data sample salespeople data set. Let’s take two subsets data performance rating 1 performance rating 4, calculate difference mean sales.can see higher performance rating sample generate higher mean sales lower performance rating. just samples, asked give conclusion populations drawn .Let’s take null hypothesis difference true mean sales two performance groups samples drawn . combine two samples calculate distribution around difference means. reject null hypothesis \\(\\alpha = 0.05\\), need determine 95% confidence interval distribution contain zero.calculate standard error combined sample using formula12:\\[\n\\sqrt{\\frac{\\sigma_{\\mathrm{perf1}}^2}{n_{\\mathrm{perf1}}} + \\frac{\\sigma_{\\mathrm{perf4}}^2}{n_{\\mathrm{perf4}}}}\n\\]\n\\(\\sigma_{\\mathrm{perf1}}\\) \\(\\sigma_{\\mathrm{perf4}}\\) standard deviations two samples \\(n_{\\mathrm{perf1}}\\) \\(n_{\\mathrm{perf4}}\\) two sample sizes.use special formula called Welch-Satterthwaite approximation calculate degrees freedom two samples, case calculates 100.9813. allows us construct 95% confidence interval difference means, can test whether contains zero.Since returned FALSE, conclude mean difference zero outside 95% confidence interval sample mean difference, 95% certainty difference population means zero. reject null hypothesis mean sales performance levels .Looking graphically, assuming \\(t\\)-distribution mean difference, determining zero sits distribution, Figure 3.2.\nFigure 3.2: t-distribution mean sales difference perf1 perf4, 95% confidence intervals (red dashed lines) zero difference (blue dot-dash line)\nred dashed lines diagram represent 95% confidence interval around mean difference two samples. ‘tails’ curve outside two lines represent maximum 0.025 probability true population mean. can see position blue dot-dashed line can correspond maximum probability population mean difference zero. p-value hypothesis test14.p-value can derived calculating standard error multiple associated zero \\(t\\)-distribution (called \\(t\\)-statistic \\(t\\)-value), applying conversion function pt() obtain upper tail probability multiplying 2 get probability associated tails distribution.Nowadays, never necessary manual calculations hypothesis tests standard part statistical software. R, t.test() function performs hypothesis test difference means two samples confirms manually calculated p-value 95% confidence interval.p-value less alpha 0.05, reject null hypothesis favor alternative hypothesis. standard \\(\\alpha = 0.05\\) associated term statistically significant. Therefore say two performance groups statistically significant difference mean sales.practice, numerous alphas interest analysts, reflecting different levels certainty. 0.05 common standard many disciplines, stringent alphas 0.01 0.001 often used situations high degree certainty desirable (example, medical fields). Similarly, less stringent alpha standard 0.1 can interest particularly sample sizes small analyst satisfied ‘indications’ data. many statistical software packages, including see book, tests meet \\(\\alpha = 0.1\\) standard usually marked period(.), meet \\(\\alpha = 0.05\\) asterisk(*), \\(\\alpha = 0.01\\) double asterisk(**) \\(\\alpha = 0.001\\) triple asterisk(***).Many leading statisticians argued p-values test sample size anything else cautioned much focus p-values making statistical conclusions data. particular, situations data methodology deliberately manipulated achieve certain alpha standards—process known ‘p-hacking’—increasing concern recently. See Chapter 11 better understanding significance level sample size contribute determining statistical power hypothesis testing.","code":"\n# take two performance group samples \nperf1 <- subset(salespeople, subset = performance == 1)\nperf4 <- subset(salespeople, subset = performance == 4)\n\n# calculate the difference in mean sales\n(diff <- mean(perf4$sales) - mean(perf1$sales))## [1] 154.9742\n# calculate standard error of the two sets\nse <- sqrt(sd(perf1$sales)^2/length(perf1$sales) \n           + sd(perf4$sales)^2/length(perf4$sales))\n\n# calculate the required t-statistic\nt <- qt(p = 0.975, df = 100.98)\n\n# calculate 95% confidence interval\n(lower_bound <- diff - t*se)## [1] 88.56763\n(upper_bound <- diff + t*se)## [1] 221.3809\n# test if zero is inside this interval\n(0 <= upper_bound) & (0 >= lower_bound)## [1] FALSE\n# get t-statistic\nt_actual <- diff/se \n\n# convert t-statistic to p-value\n2*pt(t_actual, df = 100.98, lower = FALSE)## [1] 1.093212e-05\nt.test(perf4$sales, perf1$sales)## \n##  Welch Two Sample t-test\n## \n## data:  perf4$sales and perf1$sales\n## t = 4.6295, df = 100.98, p-value = 1.093e-05\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##   88.5676 221.3809\n## sample estimates:\n## mean of x mean of y \n##  619.8909  464.9167"},{"path":"found-stats.html","id":"t-test-cor","chapter":"3 Statistics Foundations","heading":"3.3.2 Testing for a non-zero correlation between two variables (\\(t\\)-test for correlation)","text":"Imagine given sample data two variables asked variables correlated overall population. can take null hypothesis variables correlated, determine t-statistic associated zero correlation convert p-value. t-statistic associated correlation \\(r\\) two samples length \\(n\\) often notated \\(t^*\\) defined \\[\nt^* = \\frac{r\\sqrt{n-2}}{\\sqrt{1-r^2}}\n\\]\n\\(t^*\\) can converted associated p-value using \\(t\\)-distribution similar way previous section, time \\(n - 2\\) degrees freedom \\(t\\)-distribution. example, let’s calculate \\(t^*\\) correlation sales customer rating sample convert p-value., useful function R cut need manual calculations. cor.test() function R performs hypothesis test null hypothesis two variables zero correlation.confirms manual calculations, see null hypothesis rejected can conclude significant correlation sales customer rating.","code":"\n# remove NAs from salespeople\nsalespeople <- salespeople[complete.cases(salespeople), ]\n\n# calculate t_star\nr <- cor(salespeople$sales, salespeople$customer_rate)\nn <- nrow(salespeople)\nt_star <- (r*sqrt(n - 2))/sqrt(1 - r^2)\n\n# convert to p-value on t-distribution with n - 2 degrees of freedom\n2*pt(t_star, df = n - 2, lower = FALSE)## [1] 8.647952e-11\ncor.test(salespeople$sales, salespeople$customer_rate)## \n##  Pearson's product-moment correlation\n## \n## data:  salespeople$sales and salespeople$customer_rate\n## t = 6.6952, df = 348, p-value = 8.648e-11\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.2415282 0.4274964\n## sample estimates:\n##      cor \n## 0.337805"},{"path":"found-stats.html","id":"testing-for-a-difference-in-frequency-distribution-between-different-categories-in-a-data-set-chi-square-test","chapter":"3 Statistics Foundations","heading":"3.3.3 Testing for a difference in frequency distribution between different categories in a data set (Chi-square test)","text":"Imagine asked performance category person salespeople data set relationship promotion likelihood. test null hypothesis difference distribution promoted versus promoted across four performance categories.First can produce contingency table, matrix containing counts many people promoted promoted category.can see summing row total sample can expect 113 people promoted 237 miss promotion. can use ratio compute expected proportion performance category assumption distribution exactly across four categories.Now can compare observed versus expected values using difference metric:\\[\n\\frac{(\\mathrm{observed} - \\mathrm{expected})^2}{\\mathrm{expected}}\n\\]\nadd get total, known \\(\\chi^2\\) statistic.\\(\\chi^2\\) statistic expected distribution can used determine p-value associated statistic. \\(t\\)-distribution, \\(\\chi^2\\)-distribution depends degrees freedom. calculated subtracting one number rows number columns contingency table multiplying together. case 2 rows 4 columns, calculates 3 degrees freedom. Armed \\(\\chi^2\\) statistic degrees freedom, can now calculate p-value hypothesis test using pchisq() function.chisq.test() function R performs steps involved chi-square test independence contingency table returns \\(\\chi^2\\) statistic associated p-value null hypothesis, case confirming manual calculations., can reject null hypothesis confirm alternative hypothesis difference distribution promoted/promoted individuals four performance categories.","code":"\n# create contingency table of promoted vs performance\n(contingency <- table(salespeople$promoted, salespeople$performance))##    \n##      1  2  3  4\n##   0 50 85 77 25\n##   1 10 25 48 30\n# calculate expected promoted and not promoted\n(expected_promoted <- (sum(contingency[2, ])/sum(contingency)) * \n   colSums(contingency))##        1        2        3        4 \n## 19.37143 35.51429 40.35714 17.75714\n(expected_notpromoted <- (sum(contingency[1, ])/sum(contingency)) * \n    colSums(contingency))##        1        2        3        4 \n## 40.62857 74.48571 84.64286 37.24286\n# calculate the difference metrics for promoted and not promoted\npromoted <- sum((expected_promoted - contingency[2, ])^2/\n                  expected_promoted)\n\nnotpromoted <- sum((expected_notpromoted - contingency[1, ])^2/\n                     expected_notpromoted)\n\n# calculate chi-squared statistic\n(chi_sq_stat <- notpromoted + promoted)## [1] 25.89541\n# calculate p-value from chi_squared stat\npchisq(chi_sq_stat, df = 3, lower.tail=FALSE)## [1] 1.003063e-05\nchisq.test(contingency)## \n##  Pearson's Chi-squared test\n## \n## data:  contingency\n## X-squared = 25.895, df = 3, p-value = 1.003e-05"},{"path":"found-stats.html","id":"foundational-statistics-in-python","chapter":"3 Statistics Foundations","heading":"3.4 Foundational statistics in Python","text":"Elementary descriptive statistics can performed Python using various packages. Descriptive statistics numpy arrays usually available methods.Population statistics can obtained setting ddof parameter zero.numpy covariance function produces covariance matrix.Specific covariances variable pairs can pulled matrix.Similarly Pearson correlation:Specific types correlation coefficients can accessed via stats module scipy package.Common hypothesis testing tools available scipy.stats. example perform Welch’s \\(t\\)-test difference means samples unequal variance.seen , hypothesis tests non-zero correlation coefficients performed automatically part scipy.stats correlation calculations.Finally, chi-square test difference frequency distribution can performed contingency table follows. first value output \\(\\chi^2\\) statistic, second value p-value.","code":"import pandas as pd\nimport numpy as np\n\n# get data\nurl = \"http://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople = pd.read_csv(url)\n\n# mean sales\nmean_sales = salespeople.sales.mean()\nprint(mean_sales)## 527.0057142857142# sample variance\nvar_sales = salespeople.sales.var()\nprint(var_sales)## 34308.11458043389# sample standard deviation\nsd_sales = salespeople.sales.std()\nprint(sd_sales)## 185.2244977869663# population standard deviation\npopsd_sales = salespeople.sales.std(ddof = 0)\nprint(popsd_sales)## 184.9597020864771# generate a sample covariance matrix between two variables\nsales_rate = salespeople[['sales', 'customer_rate']]\nsales_rate = sales_rate[~np.isnan(sales_rate)]\ncov = sales_rate.cov()\nprint(cov)##                       sales  customer_rate\n## sales          34308.114580      55.817691\n## customer_rate     55.817691       0.795820# pull out specific covariances\nprint(cov['sales']['customer_rate'])## 55.817691199345006# sample pearson correlation matrix\ncor = sales_rate.corr()\nprint(cor)##                   sales  customer_rate\n## sales          1.000000       0.337805\n## customer_rate  0.337805       1.000000from scipy import stats\n\n# spearman's correlation\nstats.spearmanr(salespeople.sales, salespeople.performance, \nnan_policy='omit')## SpearmanrResult(correlation=0.27354459847452534, pvalue=2.0065434379079837e-07)# kendall's tau\nstats.kendalltau(salespeople.sales, salespeople.performance, \nnan_policy='omit')## KendalltauResult(correlation=0.20736088105812, pvalue=2.7353258226376615e-07)# get sales for top and bottom performers\nperf1 = salespeople[salespeople.performance == 1].sales\nperf4 = salespeople[salespeople.performance == 4].sales\n\n# welch's t-test with unequal variance\nttest = stats.ttest_ind(perf4, perf1, equal_var=False)\nprint(ttest)## Ttest_indResult(statistic=4.629477606844271, pvalue=1.0932443461577038e-05)# calculate correlation and p-value \nsales = salespeople.sales[~np.isnan(salespeople.sales)]\n\ncust_rate = salespeople.customer_rate[\n  ~np.isnan(salespeople.customer_rate)\n]\n\ncor = stats.pearsonr(sales, cust_rate)\nprint(cor)## (0.33780504485867796, 8.647952212091035e-11)# create contingency table for promoted versus performance\ncontingency = pd.crosstab(salespeople.promoted, salespeople.performance)\n\n# perform chi-square test\nchi2_test = stats.chi2_contingency(contingency)\nprint(chi2_test)## (25.895405268094862, 1.0030629464566802e-05, 3, array([[40.62857143, 74.48571429, 84.64285714, 37.24285714],\n##        [19.37142857, 35.51428571, 40.35714286, 17.75714286]]))"},{"path":"found-stats.html","id":"foundational-statistics-in-julia","chapter":"3 Statistics Foundations","heading":"3.5 Foundational statistics in Julia","text":"Statistics package Julia provides wide range functions univariate bivariate analysis. Note Julia particular missing type missing data, specific string form missing data may need defined importing data CSVs data sources. little front work required Julia ensure missing data structures passed statistics functions.Population statistics can obtained setting corrected argument false inside standard statistics functions.Note useful time-saving double functions Julia common statistics.Covariance correlations straightforward, specific functions Spearman’s rho Kendall’s tau:HypothesisTests package provides common hypothesis testing functionality nicely formatted output. perform two-tailed t-test samples unequal variance:Currently, Julia packaged hypothesis tests correlations, need done manually using methods illustrated Section 3.3.215. chi-square test conducted contingency matrix follows:Specific elements hypothesis test results can extracted follows:","code":"using DataFrames, CSV, StatsBase, Statistics, Missings;\n\n# get salespeople data\nurl = \"http://peopleanalytics-regression-book.org/data/salespeople.csv\";\nsalespeople = CSV.read(download(url), DataFrame, missingstrings=[\"NA\"]);\n\n# remove missing value rows from dataset\nsalespeople = salespeople[completecases(salespeople), :];\n# ensure no missing data structures\nsalespeople = mapcols(col -> disallowmissing(col), salespeople);\n\n# mean of sales\nmean(salespeople.sales)## 527.0057142857142# sample variance\nvar(salespeople.sales)## 34308.1145804339# sample standard deviation\nstd(salespeople.sales)## 185.22449778696634# population standard deviation\nstd(salespeople.sales, corrected = false)## 184.95970208647714# sample mean and variance\nmean_and_var(salespeople.sales)## (527.0057142857142, 34308.1145804339)# sample mean and standard deviation\nmean_and_std(salespeople.sales)## (527.0057142857142, 185.22449778696634)# sample covariance\ncov(salespeople.sales, salespeople.customer_rate)## 55.81769119934503# sample Pearson correlation\ncor(salespeople.sales, salespeople.customer_rate)## 0.337805044858678# sample Spearman correlation\ncorspearman(salespeople.sales, salespeople.performance)## 0.2735445984745253# sample Kendall correlation\ncorkendall(salespeople.sales, salespeople.performance)## 0.20736088105812using HypothesisTests\n\n# get sales for top and bottom performers\nperf1 = salespeople[salespeople.performance .== 1, :].sales;\nperf4 = salespeople[salespeople.performance .== 4, :].sales;\n\n# perform two-tailed t-test with unequal variance\nUnequalVarianceTTest(perf4, perf1)## Two sample t-test (unequal variance)\n## ------------------------------------\n## Population details:\n##     parameter of interest:   Mean difference\n##     value under h_0:         0\n##     point estimate:          154.974\n##     95% confidence interval: (88.5676, 221.381)\n## \n## Test summary:\n##     outcome with 95% confidence: reject h_0\n##     two-sided p-value:           <1e-04\n## \n## Details:\n##     number of observations:   [55,60]\n##     t-statistic:              4.629477606844271\n##     degrees of freedom:       100.97689117620553\n##     empirical standard error: 33.47553559717553# chi-square test\ncontingency = counts(salespeople.promoted, salespeople.performance);\nChisqTest(contingency)## Pearson's Chi-square Test\n## -------------------------\n## Population details:\n##     parameter of interest:   Multinomial Probabilities\n##     value under h_0:         [0.116082, 0.0553469, 0.212816, 0.101469, 0.241837, 0.115306, 0.106408, 0.0507347]\n##     point estimate:          [0.142857, 0.0285714, 0.242857, 0.0714286, 0.22, 0.137143, 0.0714286, 0.0857143]\n##     95% confidence interval: [(0.0943, 0.1958), (0.0, 0.0815), (0.1943, 0.2958), (0.0229, 0.1243), (0.1714, 0.2729), (0.0886, 0.19), (0.0229, 0.1243), (0.0371, 0.1386)]\n## \n## Test summary:\n##     outcome with 95% confidence: reject h_0\n##     one-sided p-value:           <1e-04\n## \n## Details:\n##     Sample size:        350\n##     statistic:          25.895405268094883\n##     degrees of freedom: 3\n##     residuals:          [1.47025, -2.12924, 1.21827, -1.76432, -0.830731, 1.20308, -2.00614, 2.90534]\n##     std. residuals:     [2.84263, -2.84263, 2.58921, -2.58921, -1.82347, 1.82347, -3.84573, 3.84573]# get confidence interval of t-test\nconfint(UnequalVarianceTTest(perf4, perf1))## (88.56760038276616, 221.38088446571862)# get p-value of chi-square test\npvalue(ChisqTest(contingency))## 1.0030629464566678e-5"},{"path":"found-stats.html","id":"learning-exercises-1","chapter":"3 Statistics Foundations","heading":"3.6 Learning exercises","text":"","code":""},{"path":"found-stats.html","id":"discussion-questions-1","chapter":"3 Statistics Foundations","heading":"3.6.1 Discussion questions","text":"relevant discussion exercises, let \\(x = x_1, x_2, \\dots, x_n\\) \\(y = y_1, y_2, \\dots, y_m\\) samples two random variables length \\(n\\) \\(m\\) respectively.values \\(x\\) can take form 0 1, mean 0.25, many values equal 0?\\(m = n\\) \\(x + y\\) formed element-wise sum \\(x\\) \\(y\\), show mean \\(x + y\\) equal sum mean \\(x\\) mean \\(y\\).scalar multiplier \\(\\), show \\(\\mathrm{Var}(ax) = ^2\\mathrm{Var}(x)\\).Explain standard deviation \\(x\\) intuitive measure deviation \\(x\\) variance.Describe two types correlation use \\(x\\) ordered ranking.Describe role sample size sampling frequency distribution sampling means random variable.Describe standard error statistic can used determine confidence interval true population statistic.conduct t-test null hypothesis \\(x\\) \\(y\\) drawn populations mean, describe p-value 0.01 means.Extension: sum variance law states , independent random variables \\(x\\) \\(y\\), \\(\\mathrm{Var}(x \\pm y) = \\mathrm{Var}(x) + \\mathrm{Var}(y)\\). Use together identity Exercise 3 derive formula standard error mean \\(x = x_1, x_2, \\dots, x_n\\):\\[\nSE = \\frac{\\sigma(x)}{\\sqrt{n}}\n\\]Extension: similar way Exercise 9, show standard error difference means \\(x\\) \\(y\\) \\[\n\\sqrt{\\frac{\\sigma(x)^2}{n} + \\frac{\\sigma(y)^2}{m}}\n\\]","code":""},{"path":"found-stats.html","id":"data-exercises-1","chapter":"3 Statistics Foundations","heading":"3.6.2 Data exercises","text":"exercises, load charity_donation data set via peopleanalyticsdata package, download internet16. data set contains information sample individuals made donations nature charity.Calculate mean total_donations data set.Calculate sample variance total_donation convert population variance.Calculate sample standard deviation total_donations verify square root sample variance.Calculate sample correlation total_donations time_donating. using appropriate hypothesis test, determine two variables independent overall population.Calculate mean standard error mean first 20 entries total_donations.Calculate mean standard error mean first 50 entries total_donations. Verify standard error less Exercise 5.using appropriate hypothesis test, determine mean age made recent donation different .using appropriate hypothesis test, determine difference whether recent donation made according people reside.Extension: using appropriate hypothesis test, determine age recently donated least 10 years older recently donated population.Extension: using appropriate hypothesis test, determine average donation amount least 10 dollars higher recently donated versus . Retest 20 dollars higher.","code":""},{"path":"linear-reg-ols.html","id":"linear-reg-ols","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4 Linear Regression for Continuous Outcomes","text":"chapter, introduce explore linear regression, one first learning methods developed statisticians one easiest interpret. Despite simplicity—indeed simplicity—can powerful tool many situations. Linear regression often first methodology trialed given problem, give immediate benchmark judge efficacy , complex, modeling techniques. Given ease interpretation, many analysts select linear regression model complex approaches even approaches produce slightly better fit. chapter also introduce many critical concepts apply modeling approaches proceed book. Therefore inexperienced modelers considered foundational chapter skipped.","code":""},{"path":"linear-reg-ols.html","id":"when-ols","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.1 When to use it","text":"","code":""},{"path":"linear-reg-ols.html","id":"origins-ols","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.1.1 Origins and intuition of linear regression","text":"Linear regression, also known Ordinary Least Squares linear regression OLS regression short, developed independently mathematicians Gauss Legendre around first decade 19th century, remains today controversy take credit discovery. However, time discovery actually known ‘regression’‍. term became popular following work Francis Galton—British intellectual jack---trades cousin Charles Darwin. late 1800s, Galton researched relationship heights population almost 1000 children average height parents (mid-parent height). surprised discover perfect relationship height child average height parents, general children’s heights likely range closer mean total population. described statistical phenomenon ‘regression towards mediocrity’ (‘regression’ comes Latin term approximately meaning ‘go back’).Figure 4.1 scatter plot Galton’s data black solid line showing perfect relationship look like, black dot-dashed line indicating mean child height red dashed line showing actual relationship determined Galton17. can regard red dashed line ‘going back’ perfect relationship (symbolized black line). might give intuition help understand later sections chapter. arbitrary data set, red dashed line can lie anywhere black dot-dashed line (relationship) black solid line (perfect relationship). Linear regression finding red dashed line data using explain degree input data (\\(x\\) axis) explains outcome data (\\(y\\) axis).\nFigure 4.1: Galton’s study height children introduced term ‘regression’\n","code":""},{"path":"linear-reg-ols.html","id":"use-cases-ols","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.1.2 Use cases for linear regression","text":"Linear regression particularly suited problem outcome interest sort continuous scale (example, quantity, money, height, weight). outcomes type, can first port call trying complex modeling approaches. simple easy explain, analysts often accept somewhat poorer fit using linear regression order avoid interpret complex model.illustratory examples questions tackled linear regression approach:Given data set demographic data, job data current salary data, extent can current salary explained rest data?Given data set demographic data, job data current salary data, extent can current salary explained rest data?Given annual test scores set students four-year period, relationship final test score earlier test scores?Given annual test scores set students four-year period, relationship final test score earlier test scores?Given set GPA data, SAT data data percentile score aptitude test set job applicants, extent can GPA SAT data explain aptitude test score?Given set GPA data, SAT data data percentile score aptitude test set job applicants, extent can GPA SAT data explain aptitude test score?","code":""},{"path":"linear-reg-ols.html","id":"walkthrough-ols","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.1.3 Walkthrough example","text":"working analyst biology department large academic institution offers four-year undergraduate degree program. academic leaders department interested understanding student performance final-year examination degree program relates performance prior three years.help , provided data 975 individuals graduating past three years, asked create model explain individual’s final examination score based examination scores first three years program. Year 1 examination scores awarded scale 0–100, Years 2 3 scale 0–200, Final year awarded scale 0–300.load ugtests data set session take brief look .data looks expected, test scores four years read numeric data types, course rows. need quick statistical structural overview data.can see results seem different scales different years informed, judging means, students seem found Year 2 exams challenging. can also assured missing data, displayed NA counts summary existed.can also plot four years test scores pairwise see initial relationships interest, displayed Figure 4.2.\nFigure 4.2: Pairplot ugtests data set\ndiagonal, can see distributions data column. observe relatively normal-looking distributions year. can see scatter plots pairwise correlation statistics diagonal. example, see particularly strong correlation Yr3 Final test scores, moderate correlation Yr2 Final relative independence elsewhere.","code":"\n# if needed, download ugtests data\nurl <- \"http://peopleanalytics-regression-book.org/data/ugtests.csv\"\nugtests <- read.csv(url)\n# look at the first few rows of data\nhead(ugtests)##   Yr1 Yr2 Yr3 Final\n## 1  27  50  52    93\n## 2  70 104 126   207\n## 3  27  36 148   175\n## 4  26  75 115   125\n## 5  46  77  75   114\n## 6  86 122 119   159\n# view structure\nstr(ugtests)## 'data.frame':    975 obs. of  4 variables:\n##  $ Yr1  : int  27 70 27 26 46 86 40 60 49 80 ...\n##  $ Yr2  : int  50 104 36 75 77 122 100 92 98 127 ...\n##  $ Yr3  : int  52 126 148 115 75 119 125 78 119 67 ...\n##  $ Final: int  93 207 175 125 114 159 153 84 147 80 ...\n# view statistical summary\nsummary(ugtests)##       Yr1             Yr2             Yr3            Final    \n##  Min.   : 3.00   Min.   :  6.0   Min.   :  8.0   Min.   :  8  \n##  1st Qu.:42.00   1st Qu.: 73.0   1st Qu.: 81.0   1st Qu.:118  \n##  Median :53.00   Median : 94.0   Median :105.0   Median :147  \n##  Mean   :52.15   Mean   : 92.4   Mean   :105.1   Mean   :149  \n##  3rd Qu.:62.00   3rd Qu.:112.0   3rd Qu.:130.0   3rd Qu.:175  \n##  Max.   :99.00   Max.   :188.0   Max.   :198.0   Max.   :295\nlibrary(GGally)\n\n# display a pairplot of all four columns of data\nGGally::ggpairs(ugtests)"},{"path":"linear-reg-ols.html","id":"simple-ols","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.2 Simple linear regression","text":"order visualize approach improve intuition, start simple linear regression, case single input variable outcome variable.","code":""},{"path":"linear-reg-ols.html","id":"linear-single","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.2.1 Linear relationship between a single input and an outcome","text":"Let input variable \\(x\\) outcome variable \\(y\\). Chapter 1, introduced concept parametric modeling assumption outcome variable \\(y\\) expected related input variable \\(x\\) means mathematically definable function. words, assume:\\[\ny = f(x) + \\epsilon\n\\]\\(f\\) mathematical transformation \\(x\\) \\(\\epsilon\\) random, uncontrollable error. Put another way, assume \\(f(x)\\) represents mean expected value \\(y\\) many observations \\(x\\), single observation \\(y\\) vary \\(f(x)\\) error \\(\\epsilon\\). Since assuming \\(f(x)\\) represents mean \\(y\\), errors \\(\\epsilon\\) therefore distributed around mean zero. assume distribution errors \\(\\epsilon\\) normal distribution18.Recalling equation straight line, assume expected relationship linear, take form:\\[y = mx + c\\]\n\\(m\\) represents slope gradient line, \\(c\\) represents point line intercepts \\(y\\) axis. using straight line model relationship data, call \\(c\\) \\(m\\) coefficients model.Now let’s assume sample 10 observations estimate linear relationship. Let’s take first 10 values Yr3 Final ugtests data set:can simple plot observations Figure 4.3. Intuitively, can imagine line passing points ‘fits’ general pattern. example, taking \\(m = 1.2\\) \\(c = 5\\), resulting line \\(y = 1.2x + 5\\) fit points given, displayed Figure 4.4.\nFigure 4.3: Basic scatter plot 10 observations\n\nFigure 4.4: Fitting \\(y=1.2x + 5\\) 10 observations\nlooks like approximation relationship, know best approximation?","code":"\n(d <- head(ugtests[ , c(\"Yr3\", \"Final\")], 10))##    Yr3 Final\n## 1   52    93\n## 2  126   207\n## 3  148   175\n## 4  115   125\n## 5   75   114\n## 6  119   159\n## 7  125   153\n## 8   78    84\n## 9  119   147\n## 10  67    80"},{"path":"linear-reg-ols.html","id":"minimising-error-ols","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.2.2 Minimising the error","text":"Remember assumed relationship \\(y = mx + c\\) represents expected mean value \\(y\\) many observations \\(x\\). specific single observation \\(x\\), expect \\(y\\) precisely expected line, individual observation error term \\(\\epsilon\\) either expected line. can determine error term \\(\\epsilon\\) fitted model calculating difference real value \\(y\\) one predicted model. example, \\(x = 52\\), modeled value y 67.4, real value 93, producing error \\(\\epsilon\\) 25.6. errors \\(\\epsilon\\) known residuals model. residuals 10 points data set illustrated solid red line segments Figure 4.5. looks like least one residuals pretty large.\nFigure 4.5: Residuals \\(y=1.2x + 5\\) 10 observations\nerror model—want minimize—defined number ways:average residualsThe average absolute values residuals (negative values converted positive values)average squares residuals (note squares positive)number reasons (least fact time method developed one easiest derive), common approach number 3, call regression model Ordinary Least Squares regression. algebra calculus can help us determine equation line generates least-squared residual error. theory behind , consult Montgomery, Peck, Vining (2012), let’s look works practice.","code":""},{"path":"linear-reg-ols.html","id":"best-fit-simple-ols","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.2.3 Determining the best fit","text":"can run fairly simple function R calculate best fit linear model data. run function, model details saved session investigation use.First need express model looking calculate formula. simple case, want regress outcome \\(y =\\) Final input \\(x =\\) Yr3, therefore use simple formula notation Final ~ Yr3. Now can use lm() function calculate linear model based data set formula.model object created list number different pieces information, can see looking names objects list.can already see terms familiar . example, can look coefficients.tells us best fit model—one minimizes average squares residuals—\\(y = 1.14x + 16.63\\). words, Final test score can expected take value 16.63 even zero score Yr3 input, every additional point scored Yr3 increase Final score 1.14.","code":"\n# calculate model\nmodel <- lm(formula = Final ~ Yr3, data = d)\n# view the names of the objects in the model\nnames(model) ##  [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"          \"fitted.values\" \"assign\"        \"qr\"           \n##  [8] \"df.residual\"   \"xlevels\"       \"call\"          \"terms\"         \"model\"\nmodel$coefficients## (Intercept)         Yr3 \n##   16.630452    1.143257"},{"path":"linear-reg-ols.html","id":"measuring-the-fit-of-the-model","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.2.4 Measuring the fit of the model","text":"calculated model minimizes average squared residual error sample data , don’t really sense ‘good’ model . tell well model uses input data explain outcome? important question answer want propose model good job explaining outcome, also may need compare model alternatives, require sort benchmark metric.One natural way benchmark good job model explaining outcome compare situation input model . situation, outcome values, can considered random variable mean variance. case 10 observations, 10 values Final mean 133.7. can consider horizontal line representing mean \\(y\\) ‘random model’‍, can calculate residuals around mean. can seen Figure 4.6.\nFigure 4.6: Residuals 10 observations around mean value\nRecall Section 3.1.1 definition population variance \\(y\\), notated \\(\\mathrm{Var}(y)\\). Note defined average squares residuals around mean \\(y\\). Therefore \\(\\mathrm{Var}(y)\\) represents average squared residual error random model. calculates case 1574.21. Let’s overlay fitted model onto random model Figure 4.7.\nFigure 4.7: Comparison residuals fitted model (red) random variable (blue)\nobservations (though ) seem reduced ‘distance’ random model fitting new model. average square residuals fitted model, obtain average squared residual error fitted model, calculates 398.35.Therefore, fit model, error 1574.21, fit , error 398.35. reduced error model 1175.86 , expressed proportion, 0.75. words, can say model explains 0.75 (75%) variance outcome.metric known \\(R^2\\) model primary metric used measuring fit linear regression model19.","code":""},{"path":"linear-reg-ols.html","id":"multiple-linear-regression","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.3 Multiple linear regression","text":"reality, regression problems rarely involve one single input variable, rather multiple variables. methodology multiple linear regression similar nature simple linear regression, obviously difficult visualize increased dimensionality.case, inputs set \\(p\\) variables \\(x_1, x_2, \\dots, x_p\\). Extending linear equation Section 4.2.1, seek develop equation form:\\[y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_px_p\\]\naverage squared residual error minimized.","code":""},{"path":"linear-reg-ols.html","id":"running-a-multiple-linear-regression-model-and-interpreting-its-coefficients","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.3.1 Running a multiple linear regression model and interpreting its coefficients","text":"multiple linear regression model run similar way simple linear regression model, formula notation determining outcome input variables wish model. Let’s now perform multiple linear regression entire ugtests data set regress Final test score prior test scores using formula Final ~ Yr3 + Yr2 + Yr1 determine coefficients .Referring formula Section 4.3, let’s understand coefficient \\(\\beta_0, \\beta_1, \\dots, \\beta_p\\) means. \\(\\beta_0\\), intercept model, represents value \\(y\\) assuming inputs zero. can imagine output can expected base value even without inputs—student completely flunked first three years can still redeem extent Final year.Now looking coefficients, let’s consider happens first input \\(x_1\\) increased single unit, assuming nothing else changed. expect value y increase \\(\\beta_1\\). Similarly input \\(x_k\\), unit increase result increase \\(y\\) \\(\\beta_k\\), assuming changes inputs.case ugtests data set, can say following:intercept model 14.146. value student expected score final exam even scored zero previous exams.intercept model 14.146. value student expected score final exam even scored zero previous exams.Yr3 coefficient 0.866. Assuming change inputs, increase Final exam score expected extra point Year 3 score.Yr3 coefficient 0.866. Assuming change inputs, increase Final exam score expected extra point Year 3 score.Yr2 coefficient 0.431. Assuming change inputs, increase Final exam score expected extra point Year 2 score.Yr2 coefficient 0.431. Assuming change inputs, increase Final exam score expected extra point Year 2 score.Yr1 coefficient 0.076. Assuming change inputs, increase Final exam score expected extra point Year 1 score.Yr1 coefficient 0.076. Assuming change inputs, increase Final exam score expected extra point Year 1 score.","code":"\nmodel <- lm(data = ugtests, formula = Final ~ Yr3 + Yr2 + Yr1)\nmodel$coefficients## (Intercept)         Yr3         Yr2         Yr1 \n## 14.14598945  0.86568123  0.43128539  0.07602621"},{"path":"linear-reg-ols.html","id":"coefficient-confidence","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.3.2 Coefficient confidence","text":"Intuitively, coefficients appear precise comfort. , attempting estimate relationship based limited set data. particular, looking Yr1 coefficient, seems close zero, implying possibility Year 1 examination score impact final examination score. Like statistical estimation, coefficients calculated model margin error. Typically, situation, seek know 95% confidence interval set standard certainty around values interpreting.\nsummary() function useful way gather critical information model, including important statistics coefficients:95% confidence interval corresponds approximately two standard errors estimated value. given coefficient, confidence interval includes zero, reject hypothesis variable relationship outcome. Another indicator Pr(>|t|) column coefficient summary, represents p-value null hypothesis input variable relationship outcome. value less certain threshold (usually 0.05), can conclude variable statistically significant relationship outcome. see precise confidence intervals model coefficients, can use confint() function.case, can conclude examinations Years 2 3 significant relationship Final examination score, conclude Year 1. Effectively, means can drop Yr1 model substantial loss fit. general, simpler models easier manage interpret, let’s remove non-significant variable now.Given new model three dimensions, luxury visualizing . Interactive Figure 4.8 shows data fitted plane model.\nFigure 4.8: 3D visualization fitted newmodel ugtests data\n","code":"\nmodel_summary <- summary(model)\nmodel_summary$coefficients##                Estimate Std. Error   t value      Pr(>|t|)\n## (Intercept) 14.14598945 5.48005618  2.581358  9.986880e-03\n## Yr3          0.86568123 0.02913754 29.710169 1.703293e-138\n## Yr2          0.43128539 0.03250783 13.267124  4.860109e-37\n## Yr1          0.07602621 0.06538163  1.162807  2.451936e-01\nconfint(model)##                   2.5 %     97.5 %\n## (Intercept)  3.39187185 24.9001071\n## Yr3          0.80850142  0.9228610\n## Yr2          0.36749170  0.4950791\n## Yr1         -0.05227936  0.2043318\nnewmodel <- lm(data = ugtests, formula = Final ~ Yr3 + Yr2)"},{"path":"linear-reg-ols.html","id":"lin-good-fit","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.3.3 Model ‘goodness-of-fit’","text":"point can explore overall summary model. saw previous section, model summary contains numerous objects interest, including statistics coefficients model. can see inside summary looking names contents, can dive explore specific objects interest.can see model explains half variance Final examination score. Alternatively, can view entire summary receive formatted report model.provides us important metrics model. particular, last line gives us report overall model confidence ‘goodness--fit’—hypothesis test null hypothesis model fit data better random model. high F-statistic indicates strong likelihood model fits data better random model. intuitively, perhaps, also p-value F-statistic. case extremely small, can reject null hypothesis conclude model significant explanatory power random model.careful confuse model goodness--fit \\(R^2\\). Depending sample, entirely possible model low \\(R^2\\) high certainty goodness--fit vice versa.","code":"\n# get summary of model\nnewmodel_summary <- summary(newmodel)\n\n# see summary contents\nnames(newmodel_summary)##  [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\"  \"aliased\"       \"sigma\"         \"df\"           \n##  [8] \"r.squared\"     \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\"\n# view r-squared\nnewmodel_summary$r.squared## [1] 0.5296734\n# see full model summary\nnewmodel_summary## \n## Call:\n## lm(formula = Final ~ Yr3 + Yr2, data = ugtests)\n## \n## Residuals:\n##    Min     1Q Median     3Q    Max \n## -91.12 -20.36  -0.22  18.94  98.29 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(>|t|)    \n## (Intercept) 18.08709    4.30701   4.199 2.92e-05 ***\n## Yr3          0.86496    0.02914  29.687  < 2e-16 ***\n## Yr2          0.43236    0.03250  13.303  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 30.44 on 972 degrees of freedom\n## Multiple R-squared:  0.5297, Adjusted R-squared:  0.5287 \n## F-statistic: 547.3 on 2 and 972 DF,  p-value: < 2.2e-16"},{"path":"linear-reg-ols.html","id":"making-predictions-from-your-model","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.3.4 Making predictions from your model","text":"book focuses inferential rather predictive analytics, briefly touch mechanics generating predictions models. might imagine, model fitted, prediction relatively straightforward process. feed Yr2 Yr3 examination scores fitted model, applies coefficients calculate predicted outcome. Let’s look three fictitious students create dataframe scores input model.Now can feed values model get predictions Final examination result three new students.modeling expected value Final examination result, prediction represents expected mean many students scores previous years examinations. know earlier work chapter confidence interval around coefficients model, means range values expected Final score according confidence intervals. can determined specifying require confidence interval predictions.example, another way interpreting confidence interval say , many students scored precisely 67 144 Year 2 Year 3 examinations respectively, expect mean Final score students somewhere 168 175.may also recall Section 4.2.1 prediction individual observation subject error term \\(\\epsilon\\). Therefore, generate reliable prediction range individual observation, calculate ‘prediction interval’‍.discussed Chapter 1, process developing model predict outcome can quite different developing model explain outcome. start, unlikely use entire sample fit predictive model, want reserve portion data test fit new data. Since focus book inferential modeling, much topic scope.","code":"\n(new_students <- data.frame(\n  Yr2 = c(67, 23, 88), \n  Yr3 = c(144, 100, 166)\n))##   Yr2 Yr3\n## 1  67 144\n## 2  23 100\n## 3  88 166\n# use newmodel to predict for new_students\npredict(newmodel, new_students)##        1        2        3 \n## 171.6093 114.5273 199.7179\n# get a confidence interval \npredict(newmodel, new_students, interval = \"confidence\")##        fit      lwr      upr\n## 1 171.6093 168.2125 175.0061\n## 2 114.5273 109.7081 119.3464\n## 3 199.7179 195.7255 203.7104\n# get a prediction interval \npredict(newmodel, new_students, interval = \"prediction\")##        fit       lwr      upr\n## 1 171.6093 111.77795 231.4406\n## 2 114.5273  54.59835 174.4562\n## 3 199.7179 139.84982 259.5860"},{"path":"linear-reg-ols.html","id":"managing-inputs-in-linear-regression","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.4 Managing inputs in linear regression","text":"walkthrough example chapter, useful illustrating key concepts, straightforward data set run model . missing data, data inputs numeric data type (exercises end chapter present varied data set analysis). Commonly, analyst list possible input variables can consider model, rarely run model using variables. section cover common elements decision making design input variables regression models.","code":""},{"path":"linear-reg-ols.html","id":"relevance-of-input-variables","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.4.1 Relevance of input variables","text":"first step managing input variables make judgment relevance outcome modeled. Analysts blindly run model set variables considering relevance. two common reasons rejecting inclusion input variable:reasonable possibility direct indirect causal relationship input outcome. example, provided height individual taking Final examination walkthrough example, difficult see reasonably relate outcome modeling.reasonable possibility direct indirect causal relationship input outcome. example, provided height individual taking Final examination walkthrough example, difficult see reasonably relate outcome modeling.possibility model used predict based new data future, may variables explicitly wish used prediction. example, walkthrough model contained student gender data, want include model predicted future student scores want gender taken consideration determining student performance.possibility model used predict based new data future, may variables explicitly wish used prediction. example, walkthrough model contained student gender data, want include model predicted future student scores want gender taken consideration determining student performance.","code":""},{"path":"linear-reg-ols.html","id":"sparseness-missingness-of-data","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.4.2 Sparseness (‘missingness’) of data","text":"Missing data common problem modeling. observation missing data variable included model, observation ignored, error thrown. forces model trained smaller set data, can compromise powers inference. Running summary functions data (summary() R) reveal variables contain missing data exist.three main options missing data handled:data given variable relatively complete small number observations missing, ’s usually best simplest remove observations missing data set. Note many modeling functions (though ) take care automatically.data given variable relatively complete small number observations missing, ’s usually best simplest remove observations missing data set. Note many modeling functions (though ) take care automatically.data becomes sparse, removing observations becomes less option. sparseness massive (example, half data missing), choice remove variable model. may unsatisfactory given variable (thought important explanatory role), fact remains data mostly missing good measure construct first place.data becomes sparse, removing observations becomes less option. sparseness massive (example, half data missing), choice remove variable model. may unsatisfactory given variable (thought important explanatory role), fact remains data mostly missing good measure construct first place.Moderate sparse data considered imputation. Imputation methods involve using overall statistical properties entire data set specific variables ‘suggest’ missing value might , ranging simple mean median values complex imputation methods. Imputation methods commonly used predictive settings, cover imputation methods depth .Moderate sparse data considered imputation. Imputation methods involve using overall statistical properties entire data set specific variables ‘suggest’ missing value might , ranging simple mean median values complex imputation methods. Imputation methods commonly used predictive settings, cover imputation methods depth .","code":""},{"path":"linear-reg-ols.html","id":"transforming-categorical-inputs-to-dummy-variables","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.4.3 Transforming categorical inputs to dummy variables","text":"Many models categorical inputs rather numerical inputs. Categorical inputs usually take forms :Binary values—example, Yes/, True/FalseUnordered categories—example Car, Train, BicycleOrdered categories—example Low, Medium, HighCategorical variables behave like numerical variables. sense quantity categorical variable. know Car relates Train quantitatively, know different. Even ordered category, although know ‘Medium’ higher ‘Low’‍, know much higher indeed whether difference ‘High’ ‘Medium’‍.general, model input variables take numeric form. reliable way convert categorical values dummy variables. packages functions built-ability convert categorical data dummy variables, , important know . Consider following data set:make data categorical, converted several columns possible value make, binary labeling used identify whether value present specific observation. Many packages functions available conveniently , example:worth moment consider interpret coefficients dummy variables linear regression model. Note observations one dummy variable values (cars must make). Therefore model assume ‘reference value’ categorical variable—often first value alphabetical numerical order. case, Audi reference dummy variable. model calculates effect outcome variable ‘switch’ Audi one dummies20. try use data vehicle_data_dummies data set explain retail price vehicle, interpret coefficients like :Comparing two cars make, expect extra dollar spent manufacturing change retail price …Comparing Ford Audi manufacturing cost, expect difference retail price …Comparing Toyota Audi manufacturing cost, expect difference retail price …highlights importance appropriate interpretation coefficients, particular proper understanding units. common see much larger coefficients dummy variables regression models represent binary ‘’ ‘nothing’ variable model. coefficient manufacturing cost much smaller unit case dollar manufacturing spend, scale many thousands potential dollars spend. Care taken ‘rank’ coefficients value. Higher coefficients imply greater importance21.","code":"\n(vehicle_data <- data.frame(\n  make = factor(c(\"Ford\", \"Toyota\", \"Audi\")), \n  manufacturing_cost = c(15000, 19000, 28000)\n))##     make manufacturing_cost\n## 1   Ford              15000\n## 2 Toyota              19000\n## 3   Audi              28000\nlibrary(makedummies)\n(dummy_vehicle <- makedummies::makedummies(vehicle_data, \n                                           basal_level = TRUE))##   make_Audi make_Ford make_Toyota manufacturing_cost\n## 1         0         1           0              15000\n## 2         0         0           1              19000\n## 3         1         0           0              28000"},{"path":"linear-reg-ols.html","id":"testing-your-model-assumptions","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.5 Testing your model assumptions","text":"modeling techniques underlying assumptions data model can generate inaccurate results assumptions hold true. Conscientious analysts verify assumptions satisfied finalizing modeling efforts. section outline common checks model assumptions running linear regression models.","code":""},{"path":"linear-reg-ols.html","id":"assumption-of-linearity-and-additivity","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.5.1 Assumption of linearity and additivity","text":"Linear regression assumes relationship trying model linear additive nature. Therefore can expect problems using approach model pattern linear additive.can check whether linearity assumption reasonable couple ways. can plot true versus predicted (fitted) values see look correlated. can see plot student examination model Figure 4.9.\nFigure 4.9: Plot true versus fitted/predicted student scores\nAlternatively, can plot residuals model predicted values look pattern random distribution (, major discernible pattern) Figure 4.10.\nFigure 4.10: Plot residuals fitted/predicted scores\ncan also plot residuals input variable extra check independent randomness, looking reasonably random distribution cases. find residuals following clear pattern random nature, indication linear model good choice data.","code":"\npredicted_values <- newmodel$fitted.values\ntrue_values <- ugtests$Final\n\n# plot true values against predicted values\nplot(predicted_values, true_values)\nresiduals <- newmodel$residuals\n\n# plot residuals against predicted values\nplot(predicted_values, residuals)"},{"path":"linear-reg-ols.html","id":"lin-reg-const-var","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.5.2 Assumption of constant error variance","text":"assumed linear model errors residuals homoscedastic—means variance constant across values input variables. errors model heteroscedastic—, increase decrease according value model inputs—can lead poor estimations inaccurate inferences.simple plot residuals predicted values (Figure 4.10) can give quick indication homoscedacity, thorough residuals plotted input variable, verified range residuals remains broadly stable. student examination model, can first plot residuals values Yr2 Figure 4.11.\nFigure 4.11: Plot residuals Yr2 values\nsee pretty consistent range values residuals 4.11. Similarly can plot residuals values Yr3, Figure 4.12.\nFigure 4.12: Plot residuals Yr3 values\nFigure 4.12 also shows consistent range values residuals, reassures us homoscedacity.","code":"\nYr2 <- ugtests$Yr2\n\n# plot residuals against Yr2 values\nplot(Yr2, residuals)\nYr3 <- ugtests$Yr3\n\n# plot residuals against Yr3 values\nplot(Yr3, residuals)"},{"path":"linear-reg-ols.html","id":"norm-dist-assum","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.5.3 Assumption of normally distributed errors","text":"Recall Section 4.2.1 one assumptions run linear regression model residuals model normally distributed around mean zero. Therefore useful check data see well fits assumptions. data shows residuals substantially non-normal distribution, may tell something model well can rely accurate inferences predictions.residuals violate assumption, means ingoing mathematical premise model fully describe data. Whether big problem depends existing properties sample model diagnostics. models built smaller samples, hypothesis test criteria narrowly met, residual non-normality present significant challenge reliability inferences predictions. models hypothesis test criteria comfortably met, residual non-normality less likely problem (Lumley et al. (2002)). case, good practice examine distribution residuals can refine improve model.quickest way determine residuals sample consistent normal distribution run quantile-quantile plot (Q-Q plot) residuals. plot observed quantiles sample theoretical quantiles normal distribution. closer plot looks like perfect correlation, certain can normality assumption holds. example student examination model Figure 4.13.\nFigure 4.13: Quantile-quantile plot residuals\n","code":"\n# normal distribution qqplot of residuals\nqqnorm(newmodel$residuals)"},{"path":"linear-reg-ols.html","id":"collinearity","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.5.4 Avoiding high collinearity and multicollinearity between input variables","text":"multiple linear regression, various input variables used can considered ‘dimensions’ problem model. theory, ideally expect dimensions independent uncorrelated. Practically speaking, however, ’s challenging large data sets ensure every input variable completely uncorrelated another. example, even limited ugtests data set saw Figure 4.2 Yr2 Yr3 examination scores correlated degree.correlation input variables can expected tolerated linear regression models, high levels correlation can result significant inflation coefficients inaccurate estimates p-values coefficients.Collinearity means two input variables highly correlated. definition ‘high correlation’ matter judgment, rule thumb correlations greater 0.5 might considered high greater 0.7 might considered extreme. Creating simple correlation matrix pairplot (Figure 4.2) can immediately surface high extreme collinearity.Multicollinearity means linear relationship two input variables. may always present form high correlations pairs input variables, may seen identifying ‘clusters’ moderately correlated variables, calculating Variance Inflation Factor (VIF) input variable—VIFs greater 5 indicate high multicollinearity. Easy--use tests also exist statistical software identifying multicollinearity (example mctest package R). test multicollinearity student examination model.Note collinearity multicollinearity affect coefficients variables impacted, affect variables overall statistics fit model. Therefore, model developed primarily make predictions little interest using model explain phenomenon, may need address issue . However, inferential modeling accuracy coefficients important, testing multicollinearity essential. general, best way deal collinear variables remove one model (usually one least significance explaining outcome).","code":"\nlibrary(mctest)\n\n# diagnose possible overall presence of multicollinearity\nmctest::omcdiag(newmodel)## \n## Call:\n## mctest::omcdiag(mod = newmodel)\n## \n## \n## Overall Multicollinearity Diagnostics\n## \n##                        MC Results detection\n## Determinant |X'X|:         0.9981         0\n## Farrar Chi-Square:         1.8365         0\n## Red Indicator:             0.0434         0\n## Sum of Lambda Inverse:     2.0038         0\n## Theil's Method:           -0.5259         0\n## Condition Number:          9.1952         0\n## \n## 1 --> COLLINEARITY is detected by the test \n## 0 --> COLLINEARITY is not detected by the test\n# if necessary, diagnose specific multicollinear variables using VIF \nmctest::imcdiag(newmodel, method = \"VIF\")## \n## Call:\n## mctest::imcdiag(mod = newmodel, method = \"VIF\")\n## \n## \n##  VIF Multicollinearity Diagnostics\n## \n##        VIF detection\n## Yr3 1.0019         0\n## Yr2 1.0019         0\n## \n## NOTE:  VIF Method Failed to detect multicollinearity\n## \n## \n## 0 --> COLLINEARITY is not detected by the test\n## \n## ==================================="},{"path":"linear-reg-ols.html","id":"extending-multiple-linear-regression","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.6 Extending multiple linear regression","text":"wrap chapter introducing simple extensions linear regression, particular aim trying improve overall fit model relaxing linear additive assumptions. rare practitioners extend linear regression models greatly due negative impact can interpretation, simple extensions experimenting interaction terms quadratics uncommon. appetite explore topic fully, recommend Rao et al. (2008).","code":""},{"path":"linear-reg-ols.html","id":"interactions-between-input-variables","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.6.1 Interactions between input variables","text":"Recall model student examination scores took year’s score independent input variable, therefore making assumption score obtained year acts independently additively predicting Final score. However, possible several input variables act together relation outcome. One way modeling include interaction terms model, new input variables formed products original input variables.student examination data ugtests, consider extending model include individual year examinations, also include impact combined changes across multiple years. example, combine impact Yr2 Yr3 examinations multiplying together model.see introducing interaction term improved fit model 0.53 0.59, interaction term significant, conclude addition significant effect Yr2 Yr3 scores, additional significant effect interaction Yr2*Yr3. Let’s take moment understand interpret , since note coefficients now negative.model now includes two input variables interaction, can written \\[\n\\begin{aligned}\n\\mathrm{Final} &= \\beta_0 + \\beta_1\\mathrm{Yr3} + \\beta_2\\mathrm{Yr2} + \\beta_3\\mathrm{Yr3}\\mathrm{Yr2} \\\\\n&= \\beta_0 + (\\beta_1 + \\beta_3\\mathrm{Yr2})\\mathrm{Yr3} + \\beta_2\\mathrm{Yr2} \\\\\n&= \\beta_0 + \\gamma\\mathrm{Yr3} + \\beta_2\\mathrm{Yr2}\n\\end{aligned}\n\\]\n\\(\\gamma = \\beta_1 + \\beta_3\\mathrm{Yr2}\\). Therefore model coefficients constant change values input variables. can conclude effect extra point examination Year 3 different depending student performed Year 2. Visualizing , can see Interactive Figure 4.14 non-constant term introduces curvature fitted surface aligns little closely observations data set.\nFigure 4.14: 3D visualization fitted interaction_model ugtests data\nexamining shape curved plane, can observe model considers trajectories Year 2 Year 3 examination scores. individuals improved one year next perform better model declined. demonstrate, let’s look predicted scores interaction_model someone declined someone improved Year 2 Year 3.including interaction effect, model interprets declining examination scores negatively improving examination scores. kinds additional inferential insights may great interest. However, consider impact interpretability modeling many combinations interactions. always, trade-intepretability accuracy22.running models interaction terms, can expect see hierarchy coefficients according level interaction. example, single terms usually generate higher coefficients interactions two terms, generate higher coefficients interactions three terms, . Given , whenever interaction terms considered significant model, single terms contained interaction automatically regarded significant.","code":"\ninteraction_model <- lm(data = ugtests, \n                        formula = Final ~ Yr2 + Yr3 + Yr2*Yr3)\nsummary(interaction_model)## \n## Call:\n## lm(formula = Final ~ Yr2 + Yr3 + Yr2 * Yr3, data = ugtests)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -78.084 -18.284  -0.546  18.395  79.824 \n## \n## Coefficients:\n##               Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)  1.320e+02  1.021e+01  12.928  < 2e-16 ***\n## Yr2         -7.947e-01  1.056e-01  -7.528 1.18e-13 ***\n## Yr3         -2.267e-01  9.397e-02  -2.412   0.0161 *  \n## Yr2:Yr3      1.171e-02  9.651e-04  12.134  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 28.38 on 971 degrees of freedom\n## Multiple R-squared:  0.5916, Adjusted R-squared:  0.5903 \n## F-statistic: 468.9 on 3 and 971 DF,  p-value: < 2.2e-16\n# data frame with a declining and an improving observation\nobs <- data.frame(\n  Yr2 = c(150, 75),\n  Yr3 = c(75, 150)\n)\n\npredict(interaction_model, obs)##        1        2 \n## 127.5010 170.1047"},{"path":"linear-reg-ols.html","id":"quadratic-and-higher-order-polynomial-terms","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.6.2 Quadratic and higher-order polynomial terms","text":"many situations real underlying relationship outcome inputs may non-linear. example, underlying relationship thought quadratic given input variable \\(x\\), formula take form \\(y = \\beta_0 + \\beta_1x + \\beta_2x^2\\). can easily trial polynomial terms using linear model technology.example, recall removed Yr1 data model significant modeled linearly. test quadratic model Yr1 helps improve fit23:case find modeling Yr1 quadratic makes difference fit model.","code":"\n# add a quadratic term in Yr1\nquadratic_yr1_model <- lm(data = ugtests, \n                          formula = Final ~ Yr3 + Yr2 + Yr1 + I(Yr1^2))\n\n# test R-squared\nsummary(quadratic_yr1_model)$r.squared## [1] 0.5304198"},{"path":"linear-reg-ols.html","id":"learning-exercises-2","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.7 Learning exercises","text":"","code":""},{"path":"linear-reg-ols.html","id":"discussion-questions-2","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.7.1 Discussion questions","text":"approximate meaning term ‘regression’‍? term particularly suited methodology described chapter?approximate meaning term ‘regression’‍? term particularly suited methodology described chapter?basic condition must outcome variable satisfy linear regression potential modeling approach? Describe ideas problems might modeled using linear regression.basic condition must outcome variable satisfy linear regression potential modeling approach? Describe ideas problems might modeled using linear regression.difference simple linear regression multiple linear regression?difference simple linear regression multiple linear regression?residual, relate term ‘Ordinary Least Squares’‍?residual, relate term ‘Ordinary Least Squares’‍?coefficients linear regression model interpreted? Explain higher coefficients necessarily imply greater importance.coefficients linear regression model interpreted? Explain higher coefficients necessarily imply greater importance.\\(R^2\\) linear regression model interpreted? minimum maximum possible values \\(R^2\\), mean?\\(R^2\\) linear regression model interpreted? minimum maximum possible values \\(R^2\\), mean?key considerations preparing input data linear regression model?key considerations preparing input data linear regression model?Describe understanding term ‘dummy variable’‍. dummy variable coefficients often larger coefficients linear regression models?Describe understanding term ‘dummy variable’‍. dummy variable coefficients often larger coefficients linear regression models?Describe term ‘collinearity’ important consideration regression models.Describe term ‘collinearity’ important consideration regression models.Describe ways linear regression models can extended non-linear models.Describe ways linear regression models can extended non-linear models.","code":""},{"path":"linear-reg-ols.html","id":"data-exercises-2","chapter":"4 Linear Regression for Continuous Outcomes","heading":"4.7.2 Data exercises","text":"Load sociological_data data set via peopleanalyticsdata package download internet24. data represents sample information obtained individuals participated global research study contains following fields:annual_income_ppp: annual income individual PPP adjusted US dollarsaverage_wk_hrs: average number hours per week worked individualeducation_months: total number months spent individual formal primary, secondary tertiary educationregion: region world individual livesjob_type: Whether individual works skilled unskilled professiongender: gender individualfamily_size: size individual’s family dependentswork_distance: distance individual’s residence workplace kilometerslanguages: number languages spoken fluently individualConduct exploratory data analysis data set. Including:Identify extent missing data issue.Determine data types appropriate analysis.Using correlation matrix, pairplot alternative method, identify whether collinearity present data.Identify discuss anything else interesting see data.Prepare build linear regression model explain variation annual_income_ppp using data data set.fields believe included model? , ?consider imputing missing data fields issue? , might simple ways impute missing data?variables categorical? Convert variables dummy variables using convenient function using approach.Run interpret model. convenience, avoid long formula strings, can use formula notation annual_income_ppp ~ . means ‘regress annual_income everything else’‍. can also remove fields way, example annual_income_ppp ~ . - family_size.Determine variables significant predictors annual income effect outcome.Determine overall fit model.simple analysis residuals model determine model safe interpret.Experiment improving model fit possible interaction terms non-linear extensions.Comment results. anything results surprise ? , might possible explanations .Explain comfortable using model like predictive setting—example help employers determine right pay employees.","code":""},{"path":"bin-log-reg.html","id":"bin-log-reg","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5 Binomial Logistic Regression for Binary Outcomes","text":"previous chapter looked explain outcomes continuous scale, quantity, money, height weight. number typical outcomes type people analytics domain, common form outcomes typically modeled. Much common situations outcome interest takes form limited set classes. Binary (two class) problems common. Hiring, promotion attrition often modeled binary outcomes: example ‘Promoted’ ‘promoted’. Multi-class outcomes like performance ratings ordinal scale, survey responses Likert scale often converted binary outcomes dividing ratings two groups, example ‘High’ ‘High’.situation outcome binary, effectively working likelihoods. generally linear nature, longer comfort inputs directly linearly related outcome. Therefore direct linear regression methods Ordinary Least Squares regression well suited outcomes type. Instead, linear relationships can inferred transformations outcome variable, gives us path building interpretable models. Hence, binomial logistic regression said class generalized linear models GLMs. Understanding logistic regression using reliably practice straightforward, invaluable skill people analytics domain. mathematics chapter little involved worth time investment order build competent understanding interpret types models.","code":""},{"path":"bin-log-reg.html","id":"when-to-use-it","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.1 When to use it","text":"","code":""},{"path":"bin-log-reg.html","id":"logistic-origins","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.1.1 Origins and intuition of binomial logistic regression","text":"logistic function first introduced Belgian mathematician Pierre François Verhulst mid-1800s tool modeling population growth humans, animals certain species plants fruits. time, generally accepted population growth continue exponentially forever, environmental resource limits place maximum limit size population. formula Verhulst’s function :\\[\ny = \\frac{L}{1 + e^{-k(x - x_0)}}\n\\]\n\\(e\\) exponential constant, \\(x_0\\) value \\(x\\) midpoint, \\(L\\) maximum value \\(y\\) (known ‘carrying capacity’) \\(k\\) maximum gradient curve.logistic function, shown Figure 5.1, felt accurately capture theorized stages population growth, slower growth initial stage, moving exponential growth intermediate stage slower growth population approaches carrying capacity.\nFigure 5.1: Verhulst’s logistic function modeled exponential nature natural limit population growth\nearly 20th century, starting applications economics chemistry, logistic function adopted wide array fields useful tool modeling phenomena. statistics, observed logistic function similar S-shape (sigmoid) cumulative normal distribution probability, depicted Figure 5.225, \\(x\\) scale represents standard deviations around mean. learn, logistic function gives rise mathematical model coefficients easily interpreted terms likelihood outcome. Unsurprisingly, therefore, logistic model soon became common approach modeling probabilistic phenomena.\nFigure 5.2: logistic function (blue dashed line) similar cumulative normal distribution (red solid line) easier interpret\n","code":""},{"path":"bin-log-reg.html","id":"use-cases-for-binomial-logistic-regression","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.1.2 Use cases for binomial logistic regression","text":"Binomial logistic regression can used outcome interest binary dichotomous nature. , takes one two values. example, one zero, true false, yes . classes commonly described ‘positive’ ‘negative’ classes. underlying assumption cumulative probability outcome takes shape similar cumulative normal distribution.example questions approached using binomial logistic regression:Given set data sales managers organization, including performance targets, team size, tenure organization factors, influence factors likelihood individual receiving high performance rating?Given set demographic, income location data, influence likelihood individual voting election?Given set statistics -game activity soccer players, relationship statistic likelihood player scoring goal?","code":""},{"path":"bin-log-reg.html","id":"walkthrough-logit","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.1.3 Walkthrough example","text":"analyst large company consisting regional sales teams across country. Twice every year, company promotes salespeople. Promotion discretion head regional sales team, taking consideration financial performance, customer satisfaction ratings, recent performance ratings personal judgment.asked management company conduct analysis determine factors financial performance, customer ratings performance ratings influence likelihood given salesperson promoted. provided data set containing data last three years salespeople considered promotion. salespeople data set contains following fields:promoted: binary value indicating 1 individual promoted 0 notsales: sales (thousands dollars) attributed individual period promotioncustomer_rate: average satisfaction rating survey individual’s customers promotion periodperformance: recent performance rating prior promotion, 1 (lowest) 4 (highest)Let’s take quick look data.data looks expected. Let’s get summary data.First see small number missing values, remove observations. see third individuals promoted, sales ranged $151k $945k, expected average satisfaction ratings range 1 5, finally see four performance ratings, although performance categories numeric ordered factor, promoted numeric categorical. Let’s convert , let’s pairplot get quick view possible underlying relationships, Figure 5.3.\nFigure 5.3: Pairplot salespeople data set\ncan see pairplot clearly higher sales promoted versus . also see moderate relationship customer rating sales, intuitive (customer doesn’t think much , sales wouldn’t likely high).can see relationships outcome may exist , ’s clear tease quantify relative . Let’s explore binomial logistic regression can help us .","code":"\n# if needed, download salespeople data\nurl <- \"http://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople <- read.csv(url)\n# look at the first few rows of data\nhead(salespeople)##   promoted sales customer_rate performance\n## 1        0   594          3.94           2\n## 2        0   446          4.06           3\n## 3        1   674          3.83           4\n## 4        0   525          3.62           2\n## 5        1   657          4.40           3\n## 6        1   918          4.54           2\nsummary(salespeople)##     promoted          sales       customer_rate    performance \n##  Min.   :0.0000   Min.   :151.0   Min.   :1.000   Min.   :1.0  \n##  1st Qu.:0.0000   1st Qu.:389.2   1st Qu.:3.000   1st Qu.:2.0  \n##  Median :0.0000   Median :475.0   Median :3.620   Median :3.0  \n##  Mean   :0.3219   Mean   :527.0   Mean   :3.608   Mean   :2.5  \n##  3rd Qu.:1.0000   3rd Qu.:667.2   3rd Qu.:4.290   3rd Qu.:3.0  \n##  Max.   :1.0000   Max.   :945.0   Max.   :5.000   Max.   :4.0  \n##                   NA's   :1       NA's   :1       NA's   :1\nlibrary(GGally)\n\n# remove NAs\nsalespeople <- salespeople[complete.cases(salespeople), ]\n\n# convert performance to ordered factor and promoted to categorical\nsalespeople$performance <- ordered(salespeople$performance, \n                                   levels = 1:4)\nsalespeople$promoted <- as.factor(salespeople$promoted)\n\n# generate pairplot\nGGally::ggpairs(salespeople)"},{"path":"bin-log-reg.html","id":"mod-prob","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.2 Modeling probabilistic outcomes using a logistic function","text":"Imagine outcome event \\(y\\) either occurs occur. probability \\(y\\) occurring, \\(P(y = 1)\\), obviously takes value 0 1. Now imagine input variable \\(x\\) positive effect probability event occurring. naturally expect \\(P(y = 1)\\) increase \\(x\\) increases.salespeople data set, let’s plot promotion outcome sales input. can seen Figure 5.4.\nFigure 5.4: Plot promotion sales salespeople data set\n’s clear promotion likely higher sales levels. move along \\(x\\) axis left right gradually include individuals higher sales, know probability promotion gradually increasing overall. try model probability using logistic function, learned Section 5.1.1. example, let’s plot logistic function\n\\[\nP(y = 1) = \\frac{1}{1 + e^{-k(x - x_{0})}}\n\\]data, set \\(x_0\\) mean sales \\(k\\) maximum gradient value. Figure 5.5 can see logistic functions different values \\(k\\). seem reflect pattern observing extent, determine best-fitting logistic function?\nFigure 5.5: Overlaying logistic functions various gradients onto previous plot\n","code":""},{"path":"bin-log-reg.html","id":"deriving-the-concept-of-log-odds","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.2.1 Deriving the concept of log odds","text":"Let’s look carefully index exponential constant \\(e\\) denominator logistic function. Note , \\(x_{0}\\) constant, :\\[\n-k(x - x_{0}) = -(-kx_{0} + kx) = -(\\beta_{0} + \\beta_1x)\n\\]\n\\(\\beta_0 = -kx_0\\) \\(\\beta_{1} = k\\). Therefore,\\[\nP(y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}\n\\]equation makes intuitive sense. value \\(x\\) increases, value \\(e^{-(\\beta_0 + \\beta_1x)}\\) gets smaller smaller towards zero, thus \\(P(y = 1)\\) approaches theoretical maximum value 1. value \\(x\\) decreases towards zero, see value \\(P(y = 1)\\) approaches minimum value \\(\\frac{1}{1 + e^{-\\beta_0}}\\). Referring back salespeople example, can thus see \\(\\beta_0\\) helps determine baseline probability promotion assuming sales . \\(\\beta_0\\) extremely negative value, baseline probability approach theoretical minimum zero.Let’s formalize role \\(\\beta_0\\) \\(\\beta_1\\) likelihood positive outcome. know binary event \\(y\\), \\(P(y = 0)\\) equal \\(1 - P(y = 1)\\), \\[\n\\begin{aligned}\nP(y = 0) &= 1 - \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}} \\\\\n&= \\frac{1 + e^{-(\\beta_0 + \\beta_1x)} - 1}{1 + e^{-(\\beta_0 + \\beta_1x)}} \\\\\n&= \\frac{e^{-(\\beta_0 + \\beta_1x)}}{1 + e^{-(\\beta_0 + \\beta_1x)}}\n\\end{aligned}\n\\]Putting together, find \\[\n\\begin{aligned}\n\\frac{P(y = 1)}{P(y = 0)} &= \\frac{\\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x)}}}{\\frac{e^{-(\\beta_0 + \\beta_1x)}}{1 + e^{-(\\beta_0 + \\beta_1x)}}} \\\\\n&= \\frac{1}{e^{-(\\beta_0 + \\beta_1x)}} \\\\\n&= e^{\\beta_0 + \\beta_1x}\n\\end{aligned}\n\\]alternatively, apply natural logarithm sides\\[\n\\ln\\left(\\frac{P(y = 1)}{P(y = 0)}\\right) = \\beta_0 + \\beta_1x\n\\]right-hand side look familiar previous chapter linear regression, meaning something can model linearly. left-hand side?\\(P(y = 1)\\) probability event occur, \\(P(y = 0)\\) probability event occur. may familiar sports like horse racing gambling situations ratio two represents odds event. example, given horse odds 1:4, means 20% probability win 80% probability not26.Therefore can conclude natural logarithm odds \\(y\\)—usually termed log odds \\(y\\)—linear \\(x\\), therefore can model log odds \\(y\\) using similar linear regression methods studied Chapter 427.","code":""},{"path":"bin-log-reg.html","id":"modeling-the-log-odds-and-interpreting-the-coefficients","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.2.2 Modeling the log odds and interpreting the coefficients","text":"Let’s take simple case regressing promoted outcome sales. use standard binomial GLM function standard formula notation learned previous chapter.can interpret coefficients follows:(Intercept) coefficient value log odds zero input value \\(x\\)—log odds promotion made sales.(Intercept) coefficient value log odds zero input value \\(x\\)—log odds promotion made sales.sales coefficient represents increase log odds promotion associated unit increase sales.sales coefficient represents increase log odds promotion associated unit increase sales.can convert coefficients log odds odds applying exponent function, return identity previously\\[\n\\frac{P(y = 1)}{P(y = 0)} = e^{\\beta_0 + \\beta_1x} = e^{\\beta_0}(e^{\\beta_1})^x\n\\], can interpret \\(e^{\\beta_0}\\) represents base odds promotion assuming sales, every additional unit sales, base odds multiplied \\(e^{\\beta_1}\\). Given multiplicative effect \\(e^{\\beta_1}\\) odds, known odds ratio.can see base odds promotion zero sales close zero, makes sense. Note odds can precisely zero situation impossible positive class (, nobody gets promoted). can also see unit (, every $1000) sales multiplies base odds approximately 1.04—words, increases odds promotion 4%.","code":"\n# run a binomial model \nsales_model <- glm(formula = promoted ~ sales, \n                   data = salespeople, family = \"binomial\")\n\n# view the coefficients\nsales_model$coefficients##  (Intercept)        sales \n## -21.77642020   0.03675848\n# convert log odds to base odds and odds ratio\nexp(sales_model$coefficients)##  (Intercept)        sales \n## 3.488357e-10 1.037442e+00"},{"path":"bin-log-reg.html","id":"odds-versus-probability","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.2.3 Odds versus probability","text":"worth spending little time understanding concept odds relates probability. extremely common two terms used synonymously, can lead serious misunderstandings interpreting logistic regression model.certain event probability 0.1, means odds 1:9, 0.111. probability 0.5, odds 1, probability 0.9, odds 9, probability 0.99, odds 99. approach probability 1, odds become exponentially large, illustrated Figure 5.6:\nFigure 5.6: Odds plotted probability\nconsequence given increase odds can different effect probability depending original probability first place. probability already quite low, example 0.1, 4% increase odds translates odds 0.116, translates new probability 0.103586, representing increase probability 3.59%, close increase odds. probability already high, say 0.9, 4% increase odds translates odds 9.36, translates new probability 0.903475 representing increase probability 0.39%, different increase odds. Figure 5.7 shows impact 4% increase odds according original probability event.\nFigure 5.7: Effect 4% increase odds plotted original probability\ncan see closer base probability zero, similar effect increase odds probability. However, higher probability event, less impact increase odds . case, ’s useful remember formulas converting odds probability vice versa. \\(O\\) represents odds \\(P\\) represents probability :\\[\n\\begin{aligned}\nO &= \\frac{P}{1 - P} \\\\\nP &= \\frac{O}{1 + O}\n\\end{aligned}\n\\]","code":""},{"path":"bin-log-reg.html","id":"running-a-multivariate-binomial-logistic-regression-model","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.3 Running a multivariate binomial logistic regression model","text":"derivations previous section extend multivariate data. Let \\(y\\) dichotomous outcome, let \\(x_1, x_2, \\dots, x_p\\) input variables. \\[\n\\ln\\left(\\frac{P(y = 1)}{P(y = 0)}\\right) = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_px_p\n\\]\ncoefficients \\(\\beta_0, \\beta_1,\\dots, \\beta_p\\). :\\(\\beta_0\\) represents log odds outcome inputs zeroEach \\(\\beta_i\\) represents increase log odds outcome associated unit change \\(x_i\\), assuming change inputs.Applying exponent , \\[\n\\begin{aligned}\n\\frac{P(y = 1)}{P(y = 0)} &= e^{\\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_px_p} \\\\\n&= e^{\\beta_0}(e^{\\beta_1})^{x_1}(e^{\\beta_2})^{x_2}\\dots(e^{\\beta_p})^{x_p}\n\\end{aligned}\n\\]Therefore can conclude :\\(e^{\\beta_0}\\) represents odds outcome inputs zero.\\(e^{\\beta_i}\\) represents odds ratio associated unit increase \\(x_i\\) assuming change inputs (, unit increase \\(x_i\\) multiplies odds outcome \\(e^{\\beta_i}\\)).Let’s put practice.","code":""},{"path":"bin-log-reg.html","id":"running-and-interpreting-a-multivariate-binomial-logistic-regression-model","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.3.1 Running and interpreting a multivariate binomial logistic regression model","text":"Let’s use binomial logistic regression model understand three inputs salespeople data set influence likelihood promotion.First, learned previously, good practice convert categorical performance variable dummy variable28.Now can run model (using formula promoted ~ . mean regressing promoted everything else) view coefficients.Note three performance dummies displayed. everyone one four performance categories, model using performance_1 reference case. can interpret performance coefficient effect move performance category performance_1.can already see last column coefficient summary—coefficient p-values—sales customer_rate meet significance threshold less 0.05. Interestingly, appears Estimate column customer_rate negative effect log odds promotion. convenience, can add extra column coefficient summary create exponents estimated coefficients can see odds ratios. can also remove columns less useful us wish.Now can interpret model follows:else equal, sales significant positive effect likelihood promotion, additional thousand dollars sales increasing odds promotion 4%else equal, customer ratings significant negative effect likelihood promotion, one full rating higher associated 67% lower odds promotionAll else equal, performance ratings significant effect likelihood promotionThe second conclusion may appear counter-intuitive, remember pairplot Section 5.1.3 already moderate correlation sales customer ratings, model controlling relationship. Recall odds ratios act assuming variables . Therefore, two individuals sales performance ratings, one lower customer rating likely promoted. Similarly, two individuals level sales customer rating, performance rating significant bearing likelihood promotion.Many analysts feel uncomfortable stating conclusions much precision, therefore exponent confidence intervals can calculated provide range odds ratios.Therefore can say —else equal—every additional unit sales increases odds promotion 3.0% 5.7%, every additional point customer rating decreases odds promotion 22% 89%.Similar regression models, unit scale needs taken consideration interpretation. first sight, decrease 89% odds seems lot important increase 5.7% odds. However, increase 5.7% one unit ($1000) many thousands sales units, 10 100 additional units can substantial compound effect odds promotion. decrease 89% full customer rating point scale 4 full points.","code":"\nlibrary(makedummies)\n\n# convert performance to dummy\nsalespeople_dummies <- makedummies::makedummies(salespeople)\n\n# check it worked\nhead(salespeople_dummies)##   promoted sales customer_rate performance_2 performance_3 performance_4\n## 1        0   594          3.94             1             0             0\n## 2        0   446          4.06             0             1             0\n## 3        1   674          3.83             0             0             1\n## 4        0   525          3.62             1             0             0\n## 5        1   657          4.40             0             1             0\n## 6        1   918          4.54             1             0             0\n# run binomial glm\nfull_model <- glm(formula = \"promoted ~ .\",\n                  family = \"binomial\",\n                  data = salespeople_dummies)\n\n# get coefficient summary \n(coefs <- summary(full_model)$coefficients)##                   Estimate  Std. Error    z value     Pr(>|z|)\n## (Intercept)   -19.85893195 3.444078811 -5.7661085 8.112287e-09\n## sales           0.04012425 0.006576429  6.1012212 1.052611e-09\n## customer_rate  -1.11213130 0.482681585 -2.3040682 2.121881e-02\n## performance_2   0.26299953 1.021980179  0.2573431 7.969139e-01\n## performance_3   0.68495453 0.982166998  0.6973911 4.855581e-01\n## performance_4   0.73449340 1.071963758  0.6851849 4.932272e-01\n# create coefficient table with estimates, p-values and odds ratios\n(full_coefs <- cbind(coefs[ ,c(\"Estimate\", \"Pr(>|z|)\")], \n                     odds_ratio = exp(full_model$coefficients))) ##                   Estimate     Pr(>|z|)   odds_ratio\n## (Intercept)   -19.85893195 8.112287e-09 2.373425e-09\n## sales           0.04012425 1.052611e-09 1.040940e+00\n## customer_rate  -1.11213130 2.121881e-02 3.288573e-01\n## performance_2   0.26299953 7.969139e-01 1.300826e+00\n## performance_3   0.68495453 4.855581e-01 1.983682e+00\n## performance_4   0.73449340 4.932272e-01 2.084426e+00\nexp(confint(full_model))##                      2.5 %       97.5 %\n## (Intercept)   7.879943e-13 7.385387e-07\n## sales         1.029762e+00 1.057214e+00\n## customer_rate 1.141645e-01 7.793018e-01\n## performance_2 1.800447e-01 1.061602e+01\n## performance_3 3.060299e-01 1.547188e+01\n## performance_4 2.614852e-01 1.870827e+01"},{"path":"bin-log-reg.html","id":"logistic-gof","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.3.2 Understanding the fit and goodness-of-fit of a binomial logistic regression model","text":"Understanding fit binomial logistic regression model straightforward sometimes controversial. discuss , let’s simplify model based learning performance data significant effect outcome.previous chapter, luxury three-dimensional model, can visualize Interactive Figure 5.8, revealing 3D sigmoid curve ‘twists’ reflect relative influence sales customer_rate outcome.\nFigure 5.8: 3D visualization fitted simpler_model salespeople data\nNow let’s look summary simpler_model.Note , unlike saw linear regression Section 4.3.3, summary provide statistic overall model fit goodness--fit. main reason clear unified point view statistics community single appropriate measure model fit case logistic regression. Nevertheless, number options available analysts estimating fit goodness--fit models.Pseudo-\\(R^2\\) measures attempts estimate amount variance outcome explained fitted model, analogous \\(R^2\\) linear regression. numerous variants pseudo-\\(R^2\\) common listed :McFadden’s \\(R^2\\) works comparing likelihood function fitted model random model using estimate explained variance outcome.Cox Snell’s \\(R^2\\) works applying ‘sum squares’ analogy likelihood functions align closely precise methodology calculating \\(R^2\\) linear regression. However, usually means maximum value less 1 certain circumstances substantially less 1, can problematic unintuitive \\(R^2\\).Nagelkerke’s \\(R^2\\) resolves issue upper bound Cox Snell dividing Cox Snell’s \\(R^2\\) upper bound. restores intuitive scale maximum 1, considered somewhat arbitrary limited theoretical foundation.Tjur’s \\(R^2\\) recent simpler concept. defined simply absolute difference predicted probabilities positive observations negative observations.Standard modeling functions generally offer calculation pseudo-\\(R^2\\) standard, numerous methods available calculation. example:see Cox Snell variant notably lower estimates, consistent known issues upper bound. However, estimates reasonably aligned suggest strong fit.Goodness--fit tests logistic regression models compare predictions observed outcome test null hypothesis similar. means , unlike linear regression, low p-value indicates poor fit. One commonly used method Hosmer-Lemeshow test, divides observations number groups (usually 10) according fitted probabilities, calculates proportion group positive compares expected proportions based model prediction using Chi-squared test. However, method limitations. particularly problematic situations low sample size can return highly varied results based number groups used. therefore recommended use range goodness--fit tests, rely entirely one specific approach.R, LogisticDx package offers range diagnostic tools logistic regression models, recommended exploration. example using gof() function assessing goodness--fit.gof object list provides range variants goodness--fit statistics.confirms almost tests, including Hosmer-Lemeshow test, first list, suggest fit model.Various measures predictive accuracy can also used assess binomial logistic regression model predictive context, precision, recall ROC-curve analysis. particularly suited implementations logistic regression models predictive classifiers Machine Learning context, topic outside scope book. However, recommended source deeper treatment goodness--fit tests logistic regression models Hosmer, Lemeshow, Sturdivant (2013).","code":"\n# simplify model\nsimpler_model <- glm(formula = promoted ~ sales + customer_rate,\n                     family = \"binomial\",\n                     data = salespeople)\nsummary(simpler_model)## \n## Call:\n## glm(formula = promoted ~ sales + customer_rate, family = \"binomial\", \n##     data = salespeople)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -2.02984  -0.09256  -0.02070   0.00874   3.06380  \n## \n## Coefficients:\n##                 Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)   -19.517689   3.346762  -5.832 5.48e-09 ***\n## sales           0.040389   0.006525   6.190 6.03e-10 ***\n## customer_rate  -1.122064   0.466958  -2.403   0.0163 *  \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 440.303  on 349  degrees of freedom\n## Residual deviance:  65.131  on 347  degrees of freedom\n## AIC: 71.131\n## \n## Number of Fisher Scoring iterations: 8\nlibrary(DescTools)\nDescTools::PseudoR2(\n  simpler_model, \n  which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"Tjur\")\n)##   McFadden   CoxSnell Nagelkerke       Tjur \n##  0.8520759  0.6576490  0.9187858  0.8784834\nlibrary(LogisticDx)\n\n# get range of goodness-of-fit diagnostics\nsimpler_model_diagnostics <- LogisticDx::gof(simpler_model, \n                                             plotROC = FALSE)\n\n# returns a list\nnames(simpler_model_diagnostics)## [1] \"ct\"    \"chiSq\" \"ctHL\"  \"gof\"   \"R2\"    \"auc\"\n# in our case we are interested in goodness-of-fit statistics\nsimpler_model_diagnostics$gof##          test  stat         val df        pVal\n## 1:         HL chiSq  3.44576058  8 0.903358158\n## 2:        mHL     F  2.74709957  8 0.005971045\n## 3:       OsRo     Z -0.02415249 NA 0.980730971\n## 4: SstPgeq0.5     Z  0.88656856 NA 0.375311227\n## 5:   SstPl0.5     Z  0.96819352 NA 0.332947728\n## 6:    SstBoth chiSq  1.72340251  2 0.422442787\n## 7: SllPgeq0.5 chiSq  1.85473814  1 0.173233325\n## 8:   SllPl0.5 chiSq  0.68570870  1 0.407627859\n## 9:    SllBoth chiSq  1.86640617  2 0.393291943"},{"path":"bin-log-reg.html","id":"model-parsimony","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.3.3 Model parsimony","text":"saw linear regression logistic regression approach, decided drop variables model determined significant effect outcome. principle Occam’s Razor states —else equal—simplest explanation best. sense, model contains information contribute primary inference objective complex needs . model increases communication burden explaining results others, notable analytic benefit return.Parsimony describes concept careful resources information. model described parsimonious can achieve (close ) fit smaller number inputs. Akaike Information Criterion AIC measure model parsimony computed log-likelihood models like logistic regression models, lower AIC indicating parsimonious model. AIC often calculated standard summary reports logistic regression models can also calculated independently. Let’s compare different iterations model chapter using AIC.can see model limited two significant inputs—sales customer rating—determined parsimonious model according AIC. Note AIC used interpret model quality confidence—possible lowest AIC might still poor fit.Model parsimony becomes substantial concern large number input variables. general rule, input variables model greater chance model difficult interpret clearly, greater risk measurement problems, multicollinearity. Analysts eager please customers, clients, professors bosses can easily tempted think new potential inputs model, often derived mathematically measures already inputs model. long model complex, extreme cases inputs observations. primary way manage model complexity exercise caution selecting model inputs. large numbers inputs unavoidable, coefficient regularization methods LASSO regression can help model parsimony.","code":"\n# sales only model\nAIC(sales_model)## [1] 76.49508\n# sales and customer rating model\nAIC(simpler_model)## [1] 71.13145\n# model with all inputs\nAIC(full_model)## [1] 76.37433"},{"path":"bin-log-reg.html","id":"other-considerations-in-binomial-logistic-regression","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.4 Other considerations in binomial logistic regression","text":"predict new data, just use predict() function previous chapter. function recognizes type model used—case generalized linear model—adjusts prediction approach accordingly. particular, want return probability new observations promoted, need use type = \"response\" argument.Many principles covered previous chapter linear regression equally important logistic regression. example, input variables managed similar way. Collinearity multicollinearity concern. Interaction input variables can modeled. part, analysts aware fundamental mathematical transformations take place logistic regression model consider issues (another reason ensure mathematics covered earlier chapter well understood). example, coefficients linear regression direct additive impact \\(y\\), logistic regression direct additive impact log odds \\(y\\), alternatively exponents direct multiplicative impact odds \\(y\\). Therefore coefficient overestimation can occur collinearity managed can result inferences substantially overstate importance effect input variables.binary nature outcome variable, residuals logistic regression model limited direct application problem studied. practical contexts residuals logistic regression models rarely examined, can useful identifying outliers particularly influential observations assessing goodness--fit. residuals examined, need transformed order analyzed appropriately. example, Pearson residual standardized form residual logistic regression can expected normal distribution large-enough samples. can see Figure 5.9 case simpler_model, small number substantial underestimates model. good source learning diagnostics logistic regression models Menard (2010).\nFigure 5.9: Distribution Pearson residuals simpler_model\n","code":"\n# define new observations\n(new_data <- data.frame(sales = c(420, 510, 710), \n                        customer_rate = c(3.4, 2.3, 4.2)))##   sales customer_rate\n## 1   420           3.4\n## 2   510           2.3\n## 3   710           4.2\n# predict probability of promotion\npredict(simpler_model, new_data, type = \"response\")##          1          2          3 \n## 0.00171007 0.18238565 0.98840506\nd <- density(residuals(simpler_model, \"pearson\"))\nplot(d, main= \"\")"},{"path":"bin-log-reg.html","id":"learning-exercises-3","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.5 Learning exercises","text":"","code":""},{"path":"bin-log-reg.html","id":"discussion-questions-3","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.5.1 Discussion questions","text":"Draw shape logistic function. Describe three population growth phases originally intended model.Draw shape logistic function. Describe three population growth phases originally intended model.Explain logistic function useful statisticians modeling.Explain logistic function useful statisticians modeling.formula logistic function Section 5.1.1, might common value \\(L\\) probabilistic applications? ?formula logistic function Section 5.1.1, might common value \\(L\\) probabilistic applications? ?types problems suitable logistic regression modeling?types problems suitable logistic regression modeling?Can think modeling scenarios work studies use logistic regression approach?Can think modeling scenarios work studies use logistic regression approach?Explain concept odds. odds differ probability? odds change probability increases?Explain concept odds. odds differ probability? odds change probability increases?Complete following:Complete following:event 1% probability occurring, 10% increase odds results almost __% increase probability.event 99% probability occurring, 10% increase odds results almost __% increase probability.Describe coefficients logistic regression model affect fitted outcome. \\(\\beta\\) coefficient estimate, odds ratio associated \\(\\beta\\) calculated mean?Describe coefficients logistic regression model affect fitted outcome. \\(\\beta\\) coefficient estimate, odds ratio associated \\(\\beta\\) calculated mean?options determining fit binomial logistic regression model?options determining fit binomial logistic regression model?Describe concept model parsimony. measure commonly used determine parsimonious logistic regression model?Describe concept model parsimony. measure commonly used determine parsimonious logistic regression model?","code":""},{"path":"bin-log-reg.html","id":"data-exercises-3","chapter":"5 Binomial Logistic Regression for Binary Outcomes","heading":"5.5.2 Data exercises","text":"nature preservation charity asked analyze data help understand features members public donated given month. Load charity_donation data set via peopleanalyticsdata package download internet29. contains following data:n_donations: total number times individual donated previous month studied.total_donations: total amount money donated individual previous month studiedtime_donating: number months first donation month studiedrecent_donation: Whether individual donated month studiedlast_donation: number months recent previous donation month studiedgender: gender individualreside: Whether person resides Urban Rural Domestic location Overseasage: age individualView data obtain statistical summaries. Ensure data types appropriate missing data. Determine outcome input variables.Using pairplot plotting correlating selected fields, try hypothesize variables may significant explaining recently donated.Run binomial logistic regression model using input fields. Determine input variables significant effect outcome direction effect.Calculate odds ratios significant variables explain impact outcome.Check collinearity multicollinearity model using methods previous chapters.Experiment model parsimony reducing input variables significant impact outcome. Decide parsimonious model.Calculate variety Pseudo-\\(R^2\\) variants model. explain someone statistics expertise?Report conclusions modeling exercise charity writing simple explanation assumes knowledge statistics.Extension: Using variety methods choice, test hypothesis model fits data. conclusive tests?","code":""},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"multinomial-logistic-regression-for-nominal-category-outcomes","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6 Multinomial Logistic Regression for Nominal Category Outcomes","text":"previous chapter looked model binary dichotomous outcome using logistic function. chapter look extend case outcome number categories order . outcome nominal categorical form, sense direction. ‘better’ ‘worse’‍, ‘higher’ ‘lower’‍, ‘different’‍.","code":""},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"when-to-use-it-1","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.1 When to use it","text":"","code":""},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"intuition-for-multinomial-logistic-regression","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.1.1 Intuition for multinomial logistic regression","text":"binary dichotomous outcome like studied previous chapter already fact nominal outcome two categories, principle already basic technology study problem. said, way approach problem can differ according types inferences wish make.wish make inferences choice specific category—drives whether observation Category versus others, Category B versus others—option running separate binomial logistic regression models ‘one versus rest’ basis. case can refine model differently category, eliminating variables significant determining membership category. potentially lead models defined differently different target outcome categories. Notably, common comparison category models. sometimes called stratified approach.However, many studies need ‘reference’ category better understand relative odds category membership. example, clinical settings relative risk factors different clinical outcomes can understood relative reference (usually ‘healthy’ ‘recovered’ patients)30. organizational settings, one can imagine odds different types mid-tenure career path changes well understood relative reference career path (probably common one). approach still founded binomial models, reference points models different; need make decisions refining model differently, interpret coefficients different way.chapter briefly look stratified approach (effectively repetition work done previous chapter) focusing intently construct models make inferences using multinomial approach.","code":""},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"use-cases-for-multinomial-logistic-regression","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.1.2 Use cases for multinomial logistic regression","text":"Multinomial logistic regression appropriate situation limited number outcome categories (two) modeled outcome categories order. underlying assumption independence irrelevant alternatives (IIA). Otherwise stated, assumption means alternative outcome , included, disproportionately influence membership one categories31. cases assumption violated, one choose take stratified approach, attempt hierarchical nested multinomial model alternatives, beyond scope book.Examples typical situations might modeled multinomial logistic regression include:Modeling voting choice elections multiple candidatesModeling choice career options studentsModeling choice benefit options employees","code":""},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"walkthrough-example","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.1.3 Walkthrough example","text":"analyst large technology company. company recently introduced new health insurance provider employees. beginning year employees choose one three different health plan products provider best suit needs. asked determine factors influenced choice product.health_insurance data set consists following fields:product: choice product individual—, B Cage: age individual made choicegender: gender individual stated made choicehousehold: number people living individual household time choiceposition_level: Position level company time made choice, 1 lowest 5 highestabsent: number days individual absent work year prior choiceFirst load data take look briefly.looks like two columns converted factor—product gender—let’s run pairplot quick overview patterns, can seen Figure 6.1.\nFigure 6.1: Pairplot health_insurance data set\ndata appears somewhat chaotic . However, things note. Firstly, notice relatively even spread choice products. also notice age seems playing role product choice. also mild--moderate correlations data—particular age position_level, absent position_level. However, problem clearly complex can determine bivariate perspective.","code":"\n# if needed, download health_insurance data\nurl <- \"http://peopleanalytics-regression-book.org/data/health_insurance.csv\"\nhealth_insurance <- read.csv(url)\n# view first few rows\nhead(health_insurance)##   product age household position_level gender absent\n## 1       C  57         2              2   Male     10\n## 2       A  21         7              2   Male      7\n## 3       C  66         7              2   Male      1\n## 4       A  36         4              2 Female      6\n## 5       A  23         0              2   Male     11\n## 6       A  31         5              1   Male     14\n# view structure\nstr(health_insurance)## 'data.frame':    1453 obs. of  6 variables:\n##  $ product       : chr  \"C\" \"A\" \"C\" \"A\" ...\n##  $ age           : int  57 21 66 36 23 31 37 37 55 66 ...\n##  $ household     : int  2 7 7 4 0 5 3 0 3 2 ...\n##  $ position_level: int  2 2 2 2 2 1 3 3 3 4 ...\n##  $ gender        : chr  \"Male\" \"Male\" \"Male\" \"Female\" ...\n##  $ absent        : int  10 7 1 6 11 14 12 25 3 18 ...\nlibrary(GGally)\n\n# convert product and gender to factors\nhealth_insurance$product <- as.factor(health_insurance$product)\nhealth_insurance$gender <- as.factor(health_insurance$gender)\n\nGGally::ggpairs(health_insurance)"},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"stratified","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.2 Running stratified binomial models","text":"One approach problem look product choice treat independent binomial logistic regression model, modeling choice alternative choices. model may help us describe dynamics choice specific product, careful making conclusions overall choice three products. Running stratified models efficient wider range choices outcome, since three possible choices , reasonable take route.","code":""},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"modeling-the-choice-of-product-a-versus-other-products","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.2.1 Modeling the choice of Product A versus other products","text":"Let’s first create refine binomial model choice Product .see variables except absent seem play significant role choice Product . else equal, older makes choice Product less likely. Males likely choose Product , larger households higher position levels also make choice Product likely. Based , can consider simplifying model remove absent. can also calculate odds ratios perform model diagnostics wish, similar approached problem previous chapter.results need interpreted carefully. example, odds ratios Product choice based simplified model follows:example, reminder previous chapter, interpret odds ratio age follows: else equal, every additional year age associated approximately 21% decrease odds choosing Product products.","code":"\nlibrary(makedummies)\n\n# create dummies for product choice outcome\ndummy_product <- makedummies::makedummies(health_insurance, \n                                          col = \"product\",\n                                          basal_level = TRUE)\n\n# combine to original set\nhealth_insurance <- cbind(health_insurance, dummy_product)\n\n# run a binomial model for the Product A dummy against \n# all input variables (let glm() handle dummy input variables)\nA_model <- glm(\n  formula = product_A ~ age + gender + household + \n    position_level + absent, \n  data = health_insurance, \n  family = \"binomial\"\n)\n\n\n# summary\nsummary(A_model)## \n## Call:\n## glm(formula = product_A ~ age + gender + household + position_level + \n##     absent, family = \"binomial\", data = health_insurance)\n## \n## Deviance Residuals: \n##      Min        1Q    Median        3Q       Max  \n## -2.19640  -0.43691  -0.07051   0.46304   2.37416  \n## \n## Coefficients:\n##                   Estimate Std. Error z value Pr(>|z|)    \n## (Intercept)       5.873634   0.453041  12.965  < 2e-16 ***\n## age              -0.239814   0.013945 -17.197  < 2e-16 ***\n## genderMale        0.845978   0.168237   5.028 4.94e-07 ***\n## genderNon-binary  0.222521   1.246591   0.179    0.858    \n## household         0.240205   0.037358   6.430 1.28e-10 ***\n## position_level    0.321497   0.071770   4.480 7.48e-06 ***\n## absent           -0.003751   0.010753  -0.349    0.727    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 1864.15  on 1452  degrees of freedom\n## Residual deviance:  940.92  on 1446  degrees of freedom\n## AIC: 954.92\n## \n## Number of Fisher Scoring iterations: 6\n# simpler model\nA_simple <- glm(\n  formula = product_A ~ age + household + gender + position_level, \n  data = health_insurance,\n  family = \"binomial\"\n)\n\n# view odds ratio as a data frame\nas.data.frame(exp(A_simple$coefficients))##                  exp(A_simple$coefficients)\n## (Intercept)                     343.4406669\n## age                               0.7868098\n## household                         1.2711317\n## genderMale                        2.3282637\n## genderNon-binary                  1.2794288\n## position_level                    1.3692971"},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"modeling-other-choices","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.2.2 Modeling other choices","text":"similar way can produce two models, representing choice Products B C. models produce similar significant variables, except position_level appear significant choice Product C. simplify three models slightly differently defined model choice Product C versus models two product choices. However, can conclude general input variable seems non-significant across choices product absent.","code":""},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"running-a-multinomial-regression-model","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.3 Running a multinomial regression model","text":"alternative running separate binary stratified models run multinomial logistic regression model. multinomial logistic model base defined reference category, run generalized linear model log-odds membership categories versus reference category. Due extensive use epidemiology medicine, often known relative risk one category compared reference category. Mathematically speaking, \\(X\\) vector input variables, \\(y\\) takes value \\(\\), \\(B\\) \\(C\\), \\(\\) reference, multinomial logistic regression model calculate:\\[\n\\mathrm{ln}\\left(\\frac{P(y = B)}{P(y=)}\\right) = \\alpha{X}\n\\]\n\\[\n\\mathrm{ln}\\left(\\frac{P(y = C)}{P(y=)}\\right) = \\beta{X}\n\\]\ndifferent vectors coefficients \\(\\alpha\\) \\(\\beta\\).","code":""},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"def-ref","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.3.1 Defining a reference level and running the model","text":"nnet package R contains multinom() function running multinomial logistic regression model using neural network technology32. can run model need make sure reference level defined.reference outcome defined, multinom() function nnet package run series binomial models comparing reference categories.First calculate multinomial logistic regression model.Now look summary results.Notice output summary(multi_model) much less detailed standard binomial models, effectively just delivers coefficients standard errors two models reference. determine whether specific input variables significant need calculate p-values coefficients manually calculating z-statistics converting (covered hypothesis testing methodology Section 3.3.1).","code":"\n# define reference by ensuring it is the first level of the factor\nhealth_insurance$product <- relevel(health_insurance$product, ref = \"A\")\n\n# check that A is now our reference\nlevels(health_insurance$product)## [1] \"A\" \"B\" \"C\"\nlibrary(nnet)\n\nmulti_model <- multinom(\n  formula = product ~ age + gender + household + \n    position_level + absent, \n  data = health_insurance\n)\nsummary(multi_model)## Call:\n## multinom(formula = product ~ age + gender + household + position_level + \n##     absent, data = health_insurance)\n## \n## Coefficients:\n##   (Intercept)       age  genderMale genderNon-binary  household position_level      absent\n## B    -4.60100 0.2436645 -2.38259765        0.2523409 -0.9677237     -0.4153040 0.011676034\n## C   -10.22617 0.2698141  0.09670752       -1.2715643  0.2043568     -0.2135843 0.003263631\n## \n## Std. Errors:\n##   (Intercept)        age genderMale genderNon-binary  household position_level     absent\n## B   0.5105532 0.01543139  0.2324262         1.226141 0.06943089     0.08916739 0.01298141\n## C   0.6197408 0.01567034  0.1954353         2.036273 0.04960655     0.08226087 0.01241814\n## \n## Residual Deviance: 1489.365 \n## AIC: 1517.365\n# calculate z-statistics of coefficients\nz_stats <- summary(multi_model)$coefficients/\n  summary(multi_model)$standard.errors\n\n# convert to p-values\np_values <- (1 - pnorm(abs(z_stats)))*2\n\n\n# display p-values in transposed data frame\ndata.frame(t(p_values))##                             B            C\n## (Intercept)      0.000000e+00 0.000000e+00\n## age              0.000000e+00 0.000000e+00\n## genderMale       0.000000e+00 6.207192e-01\n## genderNon-binary 8.369465e-01 5.323278e-01\n## household        0.000000e+00 3.796088e-05\n## position_level   3.199529e-06 9.419906e-03\n## absent           3.684170e-01 7.926958e-01"},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"interpreting-the-model","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.3.2 Interpreting the model","text":"confirms variables except absent play role choice products relative reference Product . can also calculate odds ratios .examples odds ratios can interpreted multinomial context (used combination p-values ):else equal, every additional year age increases relative odds selecting Product B versus Product approximately 28%, increases relative odds selecting Product C versus Product approximately 31%else equal, Male reduces relative odds selecting Product B relative Product 91%.else equal, additional household member deceases odds selecting Product B relative Product 62%, increases odds selecting Product C relative Product 23%.","code":"\n# display odds ratios in transposed data frame\nodds_ratios <- exp(summary(multi_model)$coefficients)\ndata.frame(t(odds_ratios))##                           B            C\n## (Intercept)      0.01004179 3.621021e-05\n## age              1.27591615 1.309721e+00\n## genderMale       0.09231048 1.101538e+00\n## genderNon-binary 1.28703467 2.803927e-01\n## household        0.37994694 1.226736e+00\n## position_level   0.66013957 8.076841e-01\n## absent           1.01174446 1.003269e+00"},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"changing-ref","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.3.3 Changing the reference","text":"may case someone like hear odds ratios stated reference individual choosing Product B. example, odds ratios Product C relative reference Product B? One way change reference run model . Another option note :\\[\n\\frac{P(y = C)}{P(y=B)} = \\frac{\\frac{P(y = C)}{P(y = )}}{\\frac{P(y=B)}{P(y = )}}\n= \\frac{e^{\\beta{X}}}{e^{\\alpha{X}}}\n= e^{(\\beta - \\alpha)X}\n\\]\nTherefore\\[\n\\mathrm{ln}\\left(\\frac{P(y = C)}{P(y=B)}\\right) = (\\beta - \\alpha)X\n\\]\nmeans can obtain coefficients C reference B simply calculating difference coefficients C B common reference . Let’s .number categories outcome variable limited, can efficient way obtain model coefficients various reference points without rerun models. However, determine standard errors p-values coefficients model need recalculated new reference.","code":"\n# calculate difference between coefficients and view as column\ncoefs_c_to_b <- summary(multi_model)$coefficients[2, ] - \n   summary(multi_model)$coefficients[1, ]\n\ndata.frame(coefs_c_to_b)##                  coefs_c_to_b\n## (Intercept)      -5.625169520\n## age               0.026149597\n## genderMale        2.479305168\n## genderNon-binary -1.523905192\n## household         1.172080452\n## position_level    0.201719688\n## absent           -0.008412403"},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"model-simplification-fit-and-goodness-of-fit-for-multinomial-logistic-regression-models","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.4 Model simplification, fit and goodness-of-fit for multinomial logistic regression models","text":"Simplifying multinomial regression model needs done care. binomial model, one set coefficients p-values can strong guide variables can removed safely. However, multinomial models several sets coefficients consider.","code":""},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"elim","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.4.1 Gradual safe elimination of variables","text":"Hosmer, Lemeshow, Sturdivant (2013), gradual process elimination variables recommended ensure significant variables confound different logistic models accidentally dropped final model. recommended approach follows:Start variable least significant p-values sets coefficients—case absent obvious first candidate.Run multinomial model without variable.Test none previous coefficients change 20–25%.change, safely remove variable proceed next non-significant variable.change, retain variable proceed next non-significant variable.Stop non-significant variables tested.case, can compare coefficients model without absent included verify changes coefficients substantial.can see genderNon-binary changed substantially, note extremely small sample size effect model33. therefore appears safe remove absent. Furthermore, Akaike Information Criterion equally valid multinomial models evaluating model parsimony. can calculate AIC model without absent 1517.36 1514.25, respectively, confirming model without absent marginally parsimonious.","code":"\n# remove absent\nsimpler_multi_model <- multinom(\n  formula = product ~ age + gender + household + position_level,\n  data = health_insurance, \n  model = TRUE\n)\n# view coefficients with absent\ndata.frame(t(summary(multi_model)$coefficients))##                            B             C\n## (Intercept)      -4.60099991 -10.226169428\n## age               0.24366447   0.269814063\n## genderMale       -2.38259765   0.096707521\n## genderNon-binary  0.25234087  -1.271564323\n## household        -0.96772368   0.204356774\n## position_level   -0.41530400  -0.213584308\n## absent            0.01167603   0.003263631\n# view coefficients without absent\ndata.frame(t(summary(simpler_multi_model)$coefficients))##                           B            C\n## (Intercept)      -4.5008999 -10.19269011\n## age               0.2433855   0.26976294\n## genderMale       -2.3771342   0.09801281\n## genderNon-binary  0.1712091  -1.29636779\n## household        -0.9641956   0.20510806\n## position_level   -0.3912014  -0.20908835"},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"model-fit-and-goodness-of-fit","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.4.2 Model fit and goodness-of-fit","text":"binomial case, variety Pseudo-\\(R^2\\) methods available assess fit multinomial logistic regression model, although previous variants (particularly Tjur) defined models two outcome categories.Due fact multinomial models one set coefficients, assessing goodness--fit challenging, still area intense research. approachable method assess model confidence Hosmer-Lemeshow test mentioned previous chapter, extended Fagerland, Hosmer, Bofin (2008) multinomial models. implementation available generalhoslem package R. However, version Hosmer-Lemeshow test problematic models small number input variables (fewer ten), therefore experiment . exploration topic, Chapter 8 Hosmer, Lemeshow, Sturdivant (2013) recommended, thorough treatment entire topic categorical analytics, Agresti (2007) excellent companion.","code":"\nDescTools::PseudoR2(simpler_multi_model, \n                    which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\"))##   McFadden   CoxSnell Nagelkerke \n##  0.5329175  0.6896945  0.7760413"},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"learning-exercises-4","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.5 Learning exercises","text":"","code":""},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"discussion-questions-4","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.5.1 Discussion questions","text":"Describe difference stratified versus multinomial approach modeling outcome two nominal categories.Describe interpret odds ratio input variable given category stratified modeling approach.Describe meant ‘reference’ multinomial logistic regression model least three nominal outcome categories.Describe interpret odds ratio input variable given category multinomial modeling approach.Given multinomial logistic regression model outcome categories , B, C D reference category , describe two ways determine coefficients multinomial logistic regression model reference category C.Describe process safely simplifying multinomial logistic regression model removing input variables.","code":""},{"path":"multinomial-logistic-regression-for-nominal-category-outcomes.html","id":"data-exercises-4","chapter":"6 Multinomial Logistic Regression for Nominal Category Outcomes","heading":"6.5.2 Data exercises","text":"Use health_insurance data set chapter answer questions.Complete full stratified approach modeling three product choices started Section 6.2. Calculate coefficients, odds ratios p-values case.Carefully write interpretation odds ratios previous question.Run multinomial logistic regression model product outcome using Product B reference. Calculate coefficients, ratios p-values case.Verify coefficients Product C reference Product B matches calculated Section 6.3.3.Carefully write interpretation odds ratios calculated previous question.Use process described Section 6.4.1 simplify multinomial model Question 3.","code":""},{"path":"ord-reg.html","id":"ord-reg","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","text":"Often outcomes categorical nature, also order . sometimes known ordinal outcomes. common examples include ratings form, job performance ratings survey responses Likert scales. appropriate modeling approach outcome types ordinal logistic regression. Surprisingly, approach frequently understood adopted analysts. Often treat outcome continuous variable perform simple linear regression, can lead wildly inaccurate inferences. Given prevalence ordinal outcomes people analytics, serve analysts well know run ordinal logistic regression models, interpret confirm validity.fact, numerous known ways approach inferential modeling ordinal outcomes, build theory linear, binomial multinomial regression covered previous chapters. chapter, focus commonly adopted approach: proportional odds logistic regression. Proportional odds models (sometimes known constrained cumulative logistic models) attractive approaches ease interpretation used blindly without important checking underlying assumptions.","code":""},{"path":"ord-reg.html","id":"when-to-use-it-2","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.1 When to use it","text":"","code":""},{"path":"ord-reg.html","id":"ord-intuit","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.1.1 Intuition for proportional odds logistic regression","text":"Ordinal outcomes can considered suitable approach somewhere ‘’ linear regression multinomial regression. common linear regression, can consider outcome increase decrease dependent inputs. However, unlike linear regression increase decrease ‘stepwise’ rather continuous, know difference steps across scale. medical settings, difference moving healthy early-stage disease may equivalent moving early-stage disease intermediate- advanced-stage. Equally, may much bigger psychological step individual say dissatisfied work say satisfied work. sense, analyzing categorical outcomes similar multinomial approach.formalize intuition, can imagine latent version outcome variable takes continuous form, categories formed specific cutoff points continuous variable. example, outcome variable \\(y\\) represents survey responses ordinal Likert scale 1 5, can imagine actually dealing continuous variable \\(y'\\) along four increasing ‘cutoff points’ \\(y'\\) \\(\\tau_1\\), \\(\\tau_2\\), \\(\\tau_3\\) \\(\\tau_4\\). define ordinal category follows: \\(y = 1\\) corresponds \\(y' \\le \\tau_1\\), \\(y \\le 2\\) \\(y' \\le \\tau_2\\), \\(y \\le 3\\) \\(y' \\le \\tau_3\\) \\(y \\le 4\\) \\(y' \\le \\tau_4\\). , cutoff \\(\\tau_k\\), assume probability \\(P(y > \\tau_k)\\) takes form logistic function. Therefore, proportional odds model, ‘divide’ probability space level outcome variable consider binomial logistic regression model. example, rating 3, generate binomial logistic regression model \\(P(y > \\tau_3)\\), illustrated Figure 7.1.\nFigure 7.1: Proportional odds model illustration 5-point Likert survey scale outcome greater 3 single input variable. cutoff point latent continuous outcome variable gives rise binomial logistic function.\napproach leads highly interpretable model provides single set coefficients agnostic outcome category. example, can say unit increase input variable \\(x\\) increases odds \\(y\\) higher category certain ratio.","code":""},{"path":"ord-reg.html","id":"use-cases-for-proportional-odds-logistic-regression","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.1.2 Use cases for proportional odds logistic regression","text":"Proportional odds logistic regression can used two outcome categories order. important underlying assumption input variable disproportionate effect specific level outcome variable. known proportional odds assumption. Referring Figure 7.1, assumption means ‘slope’ logistic function category cutoffs34. assumption violated, reduce coefficients model single set across outcome categories, modeling approach fails. Therefore, testing proportional odds assumption important validation step anyone running type model.Examples problems can utilize proportional odds logistic regression approach include:Understanding factors associated higher ratings employee survey Likert scaleUnderstanding factors associated higher job performance ratings ordinal performance scaleUnderstanding factors associated voting preference ranked preference voting system (example, proportional representation systems)","code":""},{"path":"ord-reg.html","id":"ord-walkthrough","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.1.3 Walkthrough example","text":"analyst sports broadcaster feature player discipline professional soccer games. prepare feature, asked verify whether certain metrics significant influencing extent player disciplined referee unfair dangerous play game. provided data 2000 different players different games, data contains fields:discipline: record maximum discipline taken referee player game. ‘None’ means discipline taken, ‘Yellow’ means player issued yellow card (warned), ‘Red’ means player issued red card ordered field play.n_yellow_25 total number yellow cards issued player previous 25 games played prior game.n_red_25 total number red cards issued player previous 25 games played prior game.position playing position player game: ‘D’ defense (including goalkeeper), ‘M’ midfield ‘S’ striker/attacker.level skill level competition game took place, 1 higher 2 lower.country country game took place—England Germany.result result game team player—‘W’ win, ‘L’ lose, ‘D’ draw/tie.Let’s download soccer data set take quick look .Let’s also take look structure data.see numerous fields need converted factors can model . Firstly, outcome interest discipline needs ordered factor, can choose increase seriousness disciplinary action.also know position, country, result level categorical, convert factors. fact choose convert result level ordered factors wish, necessary input variables, results usually little bit easier read nominal factors.Now data position run model. may wish conduct exploratory data analysis stage similar previous chapters, chapter onward skip focus modeling methodology.","code":"\n# if needed, download data\nurl <- \"http://peopleanalytics-regression-book.org/data/soccer.csv\"\nsoccer <- read.csv(url)\nhead(soccer)##   discipline n_yellow_25 n_red_25 position result country level\n## 1       None           4        1        S      D England     1\n## 2       None           2        2        D      W England     2\n## 3       None           2        1        M      D England     1\n## 4       None           2        1        M      L Germany     1\n## 5       None           2        0        S      W Germany     1\n## 6       None           3        2        M      W England     1\nstr(soccer)## 'data.frame':    2291 obs. of  7 variables:\n##  $ discipline : chr  \"None\" \"None\" \"None\" \"None\" ...\n##  $ n_yellow_25: int  4 2 2 2 2 3 4 3 4 3 ...\n##  $ n_red_25   : int  1 2 1 1 0 2 2 0 3 3 ...\n##  $ position   : chr  \"S\" \"D\" \"M\" \"M\" ...\n##  $ result     : chr  \"D\" \"W\" \"D\" \"L\" ...\n##  $ country    : chr  \"England\" \"England\" \"England\" \"Germany\" ...\n##  $ level      : int  1 2 1 1 1 1 2 1 1 1 ...\n# convert discipline to ordered factor\nsoccer$discipline <- ordered(soccer$discipline, \n                             levels = c(\"None\", \"Yellow\", \"Red\"))\n\n# check conversion\nstr(soccer)## 'data.frame':    2291 obs. of  7 variables:\n##  $ discipline : Ord.factor w/ 3 levels \"None\"<\"Yellow\"<..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ n_yellow_25: int  4 2 2 2 2 3 4 3 4 3 ...\n##  $ n_red_25   : int  1 2 1 1 0 2 2 0 3 3 ...\n##  $ position   : chr  \"S\" \"D\" \"M\" \"M\" ...\n##  $ result     : chr  \"D\" \"W\" \"D\" \"L\" ...\n##  $ country    : chr  \"England\" \"England\" \"England\" \"Germany\" ...\n##  $ level      : int  1 2 1 1 1 1 2 1 1 1 ...\n# apply as.factor to four columns\ncats <- c(\"position\", \"country\", \"result\", \"level\")\nsoccer[ ,cats] <- lapply(soccer[ ,cats], as.factor)\n\n# check again\nstr(soccer)## 'data.frame':    2291 obs. of  7 variables:\n##  $ discipline : Ord.factor w/ 3 levels \"None\"<\"Yellow\"<..: 1 1 1 1 1 1 1 1 1 1 ...\n##  $ n_yellow_25: int  4 2 2 2 2 3 4 3 4 3 ...\n##  $ n_red_25   : int  1 2 1 1 0 2 2 0 3 3 ...\n##  $ position   : Factor w/ 3 levels \"D\",\"M\",\"S\": 3 1 2 2 3 2 2 2 2 1 ...\n##  $ result     : Factor w/ 3 levels \"D\",\"L\",\"W\": 1 3 1 2 3 3 3 3 1 2 ...\n##  $ country    : Factor w/ 2 levels \"England\",\"Germany\": 1 1 1 2 2 1 2 1 2 1 ...\n##  $ level      : Factor w/ 2 levels \"1\",\"2\": 1 2 1 1 1 1 2 1 1 1 ..."},{"path":"ord-reg.html","id":"modeling-ordinal-outcomes-under-the-assumption-of-proportional-odds","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.2 Modeling ordinal outcomes under the assumption of proportional odds","text":"simplicity, noting easily generalizable, let’s assume ordinal outcome variable \\(y\\) three levels similar walkthrough example, one input variable \\(x\\). Let’s call outcome levels 1, 2 3. follow intuition Section 7.1.1, can model linear continuous variable \\(y' = \\alpha_1x + \\alpha_0 + E\\), \\(E\\) error mean zero, two increasing cutoff values \\(\\tau_1\\) \\(\\tau_2\\). define \\(y\\) terms \\(y'\\) follows: \\(y = 1\\) \\(y' \\le \\tau_1\\), \\(y = 2\\) \\(\\tau_1 < y' \\le \\tau_2\\) \\(y = 3\\) \\(y' > \\tau_2\\).","code":""},{"path":"ord-reg.html","id":"mod-lin-reg","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.2.1 Using a latent continuous outcome variable to derive a proportional odds model","text":"Recall Section 4.5.3 linear regression approach assumes residuals \\(E\\) around line \\(y' = \\alpha_1x + \\alpha_0\\) normal distribution. Let’s modify assumption slightly instead assume residuals take logistic distribution based variance \\(y'\\). Therefore, \\(y' = \\alpha_1x + \\alpha_0 + \\sigma\\epsilon\\), \\(\\sigma\\) proportional variance \\(y'\\) \\(\\epsilon\\) follows shape logistic function. \\[\nP(\\epsilon \\leq z) = \\frac{1}{1 + e^{-z}}\n\\]Let’s look probability ordinal outcome variable \\(y\\) lowest category.\\[\n\\begin{aligned}\nP(y = 1) &= P(y' \\le \\tau_1) \\\\\n&= P(\\alpha_1x + \\alpha_0 + \\sigma\\epsilon \\leq \\tau_1) \\\\\n&= P(\\epsilon \\leq \\frac{\\tau_1 - \\alpha_1x - \\alpha_0}{\\sigma}) \\\\\n&= P(\\epsilon \\le \\gamma_1 - \\beta{x}) \\\\\n&= \\frac{1}{1 + e^{-(\\gamma_1 - \\beta{x})}}\n\\end{aligned}\n\\]\\(\\gamma_1 = \\frac{\\tau_1 - \\alpha_0}{\\sigma}\\) \\(\\beta = \\frac{\\alpha_1}{\\sigma}\\).Since values \\(y\\) 1, 2 3, similar derivations Section 5.2, conclude \\(P(y > 1) = 1 - P(y = 1)\\), calculates \\[\nP(y > 1) = \\frac{e^{-(\\gamma_1 - \\beta{x})}}{1 + e^{-(\\gamma_1 - \\beta{x})}}\n\\]\nTherefore\n\\[\n\\begin{aligned}\n\\frac{P(y = 1)}{P(y > 1)} = \\frac{\\frac{1}{1 + e^{-(\\gamma_1 - \\beta{x})}}}{\\frac{e^{-(\\gamma_1 - \\beta{x})}}{1 + e^{-(\\gamma_1 - \\beta{x})}}}\n= e^{\\gamma_1 - \\beta{x}}\n\\end{aligned}\n\\]applying natural logarithm, conclude log odds \\(y\\) bottom category \\[\n\\mathrm{ln}\\left(\\frac{P(y = 1)}{P(y > 1)}\\right) = \\gamma_1 - \\beta{x}\n\\]\nsimilar way can derive log odds ordinal outcome bottom two categories \\[\n\\mathrm{ln}\\left(\\frac{P(y \\leq 2)}{P(y = 3)}\\right) = \\gamma_2 - \\beta{x}\n\\]\n\\(\\gamma_2 = \\frac{\\tau_2 - \\alpha_0}{\\sigma}\\). One can easily see generalizes arbitrary number ordinal categories, can state log odds category \\(k\\) lower \\[\n\\mathrm{ln}\\left(\\frac{P(y \\leq k)}{P(y > k)}\\right) = \\gamma_k - \\beta{x}\n\\]\nAlternatively, can state log odds category higher \\(k\\) simply inverting expression:\\[\n\\mathrm{ln}\\left(\\frac{P(y > k)}{P(y \\leq k)}\\right) = -(\\gamma_k - \\beta{x}) = \\beta{x} - \\gamma_k\n\\]taking exponents see impact unit change \\(x\\) odds \\(y\\) higher ordinal category \\(\\beta\\), irrespective category looking . Therefore single coefficient explain effect \\(x\\) \\(y\\) throughout ordinal scale. Note still different intercept coefficients \\(\\gamma_1\\) \\(\\gamma_2\\) level ordinal scale.","code":""},{"path":"ord-reg.html","id":"running-a-proportional-odds-logistic-regression-model","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.2.2 Running a proportional odds logistic regression model","text":"MASS package provides function polr() running proportional odds logistic regression model data set similar way previous models. key (obvious) requirement outcome ordered factor. Since conversions Section 7.1.3 ready run model. start running input variables let polr() function handle dummy variables automatically.can see summary returns single set coefficients input variables expect, standard errors t-statistics. also see separate intercepts various levels outcomes, also expect. interpreting model, generally don’t great deal interest intercepts, focus coefficients. First like obtain p-values, can add p-value column using conversion methods t-statistic learned Section 3.3.135.Next can convert coefficients odds ratios.can display critical statistics combining dataframe.Taking consideration p-values, can interpret coefficients follows, case assuming coefficients held still:additional yellow card received prior 25 games associated approximately 38% higher odds greater disciplinary action referee.additional red card received prior 25 games associated approximately 47% higher odds greater disciplinary action referee.Strikers approximately 50% lower odds greater disciplinary action referees compared Defenders.player team lost game approximately 62% higher odds greater disciplinary action versus player team drew game.player team won game approximately 52% lower odds greater disciplinary action versus player team drew game.can, per previous chapters, remove level country variables model simplify wish. examination coefficients AIC simpler model reveal substantial difference, therefore proceed model.","code":"\n# run proportional odds model\nlibrary(MASS)\nmodel <- polr(\n  formula = discipline ~ n_yellow_25 + n_red_25 + position + \n    country + level + result, \n  data = soccer\n)\n\n# get summary\nsummary(model)## Call:\n## polr(formula = discipline ~ n_yellow_25 + n_red_25 + position + \n##     country + level + result, data = soccer)\n## \n## Coefficients:\n##                   Value Std. Error t value\n## n_yellow_25     0.32236    0.03308  9.7456\n## n_red_25        0.38324    0.04051  9.4616\n## positionM       0.19685    0.11649  1.6899\n## positionS      -0.68534    0.15011 -4.5655\n## countryGermany  0.13297    0.09360  1.4206\n## level2          0.09097    0.09355  0.9724\n## resultL         0.48303    0.11195  4.3147\n## resultW        -0.73947    0.12129 -6.0966\n## \n## Intercepts:\n##             Value   Std. Error t value\n## None|Yellow  2.5085  0.1918    13.0770\n## Yellow|Red   3.9257  0.2057    19.0834\n## \n## Residual Deviance: 3444.534 \n## AIC: 3464.534\n# get coefficients (it's in matrix form)\ncoefficients <- summary(model)$coefficients\n\n# calculate p-values\np_value <- (1 - pnorm(abs(coefficients[ ,\"t value\"]), 0, 1))*2\n\n# bind back to coefficients\n(coefficients <- cbind(coefficients, p_value))##                      Value Std. Error    t value      p_value\n## n_yellow_25     0.32236030 0.03307768  9.7455529 0.000000e+00\n## n_red_25        0.38324333 0.04050515  9.4615947 0.000000e+00\n## positionM       0.19684666 0.11648690  1.6898610 9.105456e-02\n## positionS      -0.68533697 0.15011194 -4.5655060 4.982908e-06\n## countryGermany  0.13297173 0.09359946  1.4206464 1.554196e-01\n## level2          0.09096627 0.09354717  0.9724108 3.308462e-01\n## resultL         0.48303227 0.11195131  4.3146639 1.598459e-05\n## resultW        -0.73947295 0.12129301 -6.0965834 1.083595e-09\n## None|Yellow     2.50850778 0.19182628 13.0769766 0.000000e+00\n## Yellow|Red      3.92572124 0.20571423 19.0833721 0.000000e+00\n# calculate odds ratios\nodds_ratio <- exp(coefficients[ ,\"Value\"])\n# combine with coefficient and p_value\n(coefficients <- cbind(\n  coefficients[ ,c(\"Value\", \"p_value\")],\n  odds_ratio\n))##                      Value      p_value odds_ratio\n## n_yellow_25     0.32236030 0.000000e+00  1.3803820\n## n_red_25        0.38324333 0.000000e+00  1.4670350\n## positionM       0.19684666 9.105456e-02  1.2175573\n## positionS      -0.68533697 4.982908e-06  0.5039204\n## countryGermany  0.13297173 1.554196e-01  1.1422177\n## level2          0.09096627 3.308462e-01  1.0952321\n## resultL         0.48303227 1.598459e-05  1.6209822\n## resultW        -0.73947295 1.083595e-09  0.4773654\n## None|Yellow     2.50850778 0.000000e+00 12.2865822\n## Yellow|Red      3.92572124 0.000000e+00 50.6896241"},{"path":"ord-reg.html","id":"calculating-the-likelihood-of-an-observation-being-in-a-specific-ordinal-category","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.2.3 Calculating the likelihood of an observation being in a specific ordinal category","text":"Recall Section 7.2.1 proportional odds model generates multiple stratified binomial models, following form:\\[\nP(y \\leq k) = P(y' \\leq \\tau_k)\n\\]\nNote ordinal variable \\(y\\), \\(y \\leq k\\) \\(y > k-1\\), \\(y = k\\). Therefore \\(P(y = k) = P(y \\leq k) - P(y \\leq k - 1)\\). means can calculate specific probability observation level ordinal variable fitted model simply calculating difference fitted values pair adjacent stratified binomial models. walkthrough example, means can calculate specific probability action referee, yellow card awarded, red card awarded. can viewed using fitted() function.can seen output ordinal logistic regression models can used predictive analytics classifying new observations ordinal category highest fitted probability. also allows us graphically understand output proportional odds model. Interactive Figure 7.2 shows output simpler proportional odds model fitted n_yellow_25 n_red_25 input variables, fitted probabilities level discipline referee plotted different colored surfaces. can see situations discipline likely outcome red card least likely outcome. upper ends scales see likelihood discipline overcoming likelihood discipline, strong likelihood red cards extremely poor recent disciplinary record.\nFigure 7.2: 3D visualization simple proportional odds model discipline fitted n_yellow_25 n_red_25 soccer data set. Blue represents probability discipline referee. Yellow red represent probability yellow card red card, respectively.\n","code":"\nhead(fitted(model))##        None     Yellow        Red\n## 1 0.8207093 0.12900184 0.05028889\n## 2 0.8514232 0.10799553 0.04058128\n## 3 0.7830785 0.15400189 0.06291964\n## 4 0.6609864 0.22844107 0.11057249\n## 5 0.9591298 0.03064719 0.01022301\n## 6 0.7887766 0.15027145 0.06095200"},{"path":"ord-reg.html","id":"model-diagnostics","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.2.4 Model diagnostics","text":"Similar binomial multinomial models, pseudo-\\(R^2\\) methods available assessing model fit, AIC can used assess model parsimony. Note DescTools::PseudoR2() also offers AIC.numerous tests goodness--fit can apply ordinal logistic regression models, area subject considerable recent research. generalhoslem package R contains routes four possible tests, two particularly recommended ordinal models. work similar way Hosmer-Lemeshow test discussed Section 5.3.2, dividing sample groups comparing observed versus fitted outcomes using chi-square test. Since null hypothesis good model fit, low p-values indicate potential problems model. run tests reference. information, see Fagerland Hosmer (2017), really intensive treatment ordinal data modeling Agresti (2010) recommended.","code":"\n# diagnostics of simpler model\nDescTools::PseudoR2(\n  model, \n  which = c(\"McFadden\", \"CoxSnell\", \"Nagelkerke\", \"AIC\")\n)##     McFadden     CoxSnell   Nagelkerke          AIC \n##    0.1009411    0.1553264    0.1912445 3464.5339371\n# lipsitz test \ngeneralhoslem::lipsitz.test(model)## \n##  Lipsitz goodness of fit test for ordinal response models\n## \n## data:  formula:  discipline ~ n_yellow_25 + n_red_25 + position + country + level + formula:      result\n## LR statistic = 10.429, df = 9, p-value = 0.3169\n# pulkstenis-robinson test \n# (requires the vector of categorical input variables as an argument)\ngeneralhoslem::pulkrob.chisq(model, catvars = cats)## \n##  Pulkstenis-Robinson chi-squared test\n## \n## data:  formula:  discipline ~ n_yellow_25 + n_red_25 + position + country + level + formula:      result\n## X-squared = 129.29, df = 137, p-value = 0.668"},{"path":"ord-reg.html","id":"testing-the-proportional-odds-assumption","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.3 Testing the proportional odds assumption","text":"discussed earlier, suitability proportional odds logistic regression model depends assumption input variable similar effect different levels ordinal outcome variable. important check assumption violated proceeding declare results proportional odds model valid. two common approaches validating proportional odds assumption, go .","code":""},{"path":"ord-reg.html","id":"sighting-the-coefficients-of-stratified-binomial-models","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.3.1 Sighting the coefficients of stratified binomial models","text":"learned , proportional odds regression models effectively act series stratified binomial models assumption ‘slope’ logistic function stratified model . can verify actually running stratified binomial models data checking similar coefficients input variables. Let’s use walkthrough example illustrate.Let’s create two columns binary values correspond two higher levels ordinal variable.Now let’s create two binomial logistic regression models two higher levels outcome variable.can now display coefficients models examine difference .Ignoring intercept, concern , differences appear relatively small. Large differences coefficients indicate proportional odds assumption likely violated alternative approaches problem considered.","code":"\n# create binary variable for \"Yellow\" or \"Red\" versus \"None\"\nsoccer$yellow_plus <- ifelse(soccer$discipline == \"None\", 0, 1)\n\n# create binary variable for \"Red\" versus \"Yellow\" or \"None\"\nsoccer$red <- ifelse(soccer$discipline == \"Red\", 1, 0)\n# model for at least a yellow card\nyellowplus_model <- glm(\n  yellow_plus ~ n_yellow_25 + n_red_25 + position + \n    result + country + level, \n  data = soccer, \n  family = \"binomial\"\n)\n\n# model for a red card\nred_model <- glm(\n  red ~ n_yellow_25 + n_red_25 + position + \n    result + country + level,\n  data = soccer, \n  family = \"binomial\"\n)\n(coefficient_comparison <- data.frame(\n  yellowplus = summary(yellowplus_model)$coefficients[ , \"Estimate\"],\n  red = summary(red_model)$coefficients[ ,\"Estimate\"],\n  diff = summary(red_model)$coefficients[ ,\"Estimate\"] - \n    summary(yellowplus_model)$coefficients[ , \"Estimate\"]\n))##                 yellowplus         red        diff\n## (Intercept)    -2.63646519 -3.89865929 -1.26219410\n## n_yellow_25     0.34585921  0.32468746 -0.02117176\n## n_red_25        0.41454059  0.34213238 -0.07240822\n## positionM       0.26108978  0.06387813 -0.19721165\n## positionS      -0.72118538 -0.44228286  0.27890252\n## resultL         0.46162324  0.64295195  0.18132871\n## resultW        -0.77821530 -0.58536482  0.19285048\n## countryGermany  0.13136665  0.10796418 -0.02340247\n## level2          0.08056718  0.12421593  0.04364875"},{"path":"ord-reg.html","id":"wald","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.3.2 The Brant-Wald test","text":"previous method, judgment required decide whether coefficients stratified binomial models ‘different enough’ decide violation proportional odds assumption. requiring formal support, option Brant-Wald test. test, generalized ordinal logistic regression model approximated compared calculated proportional odds model. generalized ordinal logistic regression model simply relaxing proportional odds model allow different coefficients level ordinal outcome variable.Wald test conducted comparison proportional odds generalized models. Wald test hypothesis test significance difference model coefficients, producing chi-square statistic. low p-value Brant-Wald test indicator coefficient satisfy proportional odds assumption. brant package R provides implementation Brant-Wald test, case supports judgment proportional odds assumption holds.p-value less 0.05 test—particularly Omnibus plus least one variables—interpreted failure proportional odds assumption.","code":"\nlibrary(brant)\nbrant::brant(model)## -------------------------------------------- \n## Test for X2  df  probability \n## -------------------------------------------- \n## Omnibus      14.16   8   0.08\n## n_yellow_25  0.24    1   0.62\n## n_red_25 1.83    1   0.18\n## positionM    1.7 1   0.19\n## positionS    2.33    1   0.13\n## countryGermany   0.04    1   0.85\n## level2       0.13    1   0.72\n## resultL      1.53    1   0.22\n## resultW      1.3 1   0.25\n## -------------------------------------------- \n## \n## H0: Parallel Regression Assumption holds"},{"path":"ord-reg.html","id":"alternatives-to-proportional-odds-models","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.3.3 Alternatives to proportional odds models","text":"proportional odds model far utilized approach modeling ordinal outcomes (least neglect testing underlying assumptions). learned, always appropriate model choice ordinal outcomes. test proportional odds fails, need consider strategy remodeling data. one two variables fail test proportional odds, simple option remove variables. Whether comfortable depend much impact overall model fit.event option remove variables unattractive, alternative models ordinal outcomes considered. common alternatives (cover depth , explored Agresti (2010)) :Baseline logistic model. model multinomial regression model covered previous chapter, using lowest ordinal value reference.Adjacent-category logistic model. model compares level ordinal variable next highest level, constrained version baseline logistic model. brglm2 package R offers function bracl() calculating adjacent category logistic model.Continuation-ratio logistic model. model compares level ordinal variable lower levels. can modeled using binary logistic regression techniques, new variables need constructed data set allow . R package rms function cr.setup() utility preparing outcome variable continuation ratio model.","code":""},{"path":"ord-reg.html","id":"learning-exercises-5","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.4 Learning exercises","text":"","code":""},{"path":"ord-reg.html","id":"discussion-questions-5","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.4.1 Discussion questions","text":"Describe meant ordinal variable.Describe ordinal variable can represented using latent continuous variable.Describe series binomial logistic regression models components proportional odds regression model. can say coefficients?\\(y\\) ordinal outcome variable least three levels, \\(x\\) input variable coefficient \\(\\beta\\) proportional odds logistic regression model, describe interpret odds ratio \\(e^{\\beta}\\).Describe approaches assessing fit goodness--fit ordinal logistic regression model.Describe use stratified binomial logistic regression models validate key assumption proportional odds model.Describe statistical significance test can support reject hypothesis proportional odds assumption holds.Describe possible options situations proportional odds assumption violated.","code":""},{"path":"ord-reg.html","id":"data-exercises-5","chapter":"7 Proportional Odds Logistic Regression for Ordered Category Outcomes","heading":"7.4.2 Data exercises","text":"Load managers data set via peopleanalyticsdata package download internet36. set information 571 managers sales organization consists following fields:employee_id managerperformance_group manager recent performance review: Bottom performer, Middle performer, Top performeryrs_employed: total length time employed yearsmanager_hire: whether individual hired directly manager (Y) promoted manager (N)test_score: score test given managersgroup_size: number employees group responsible forconcern_flag: whether individual subject complaint member groupmobile_flag: whether individual works mobile (Y) office (N)customers: number customer accounts manager responsible forhigh_hours_flag: whether manager entered unusually high hours timesheet past yeartransfers: number transfer requests coming manager’s group managerreduced_schedule: whether manager works part time (Y) full time (N)city: current office manager.Construct model determine data provided may help explain performance_group manager following steps:Convert outcome variable ordered factor increasing performance.Convert input variables categorical factors appropriate.Perform exploratory data analysis wish .Run proportional odds logistic regression model relevant input variables.Construct p-values coefficients consider simplify model remove variables impact outcome.Calculate odds ratios simplified model write interpretation .Estimate fit simplified model using variety metrics perform tests determine model good fit data.Construct new outcome variables use stratified binomial approach determine proportional odds assumption holds simplified model. input variables may concerned assumption violated? consider case?Use Brant-Wald test support reject hypothesis proportional odds assumption holds simplified model.Write full report model intended audience people limited knowledge statistics.","code":""},{"path":"modeling-explicit-and-latent-hierarchy-in-data.html","id":"modeling-explicit-and-latent-hierarchy-in-data","chapter":"8 Modeling Explicit and Latent Hierarchy in Data","heading":"8 Modeling Explicit and Latent Hierarchy in Data","text":"far book learned widely used foundational regression techniques inferential modeling. Starting chapter, look situations need adapt combine techniques address certain inference goals data characteristics. chapter look situations data hierarchy wish consider hierarchy modeling efforts.often case data explicit hierarchy. example, observation data may refer different individual individual may member different groups. Similarly, observation might refer event involving individual, may data multiple events individual. particular problem modeling, may wish take consideration effect hierarchical grouping. requires model mixture random effects fixed effects—called mixed model.Separately, can case data given latent hierarchy. input variables data might measures smaller set higher-level latent constructs, may interpretable model hypothesize, confirm model latent constructs outcome interest rather using larger number explicit input variables. Latent variable modeling common technique address situation, chapter review form latent variable modeling called structural equation modeling, effective especially making inferences survey instruments large numbers items.topics quite broad, many different approaches, techniques terms involved mixed modeling latent variable modeling. chapter cover simpler approaches, suffice majority common situations people analytics. deeper treatment topics, see Jiang (2007) mixed models Bartholomew, Knott, Moustaki (2011) Skrondal Rabe-Hesketh (2004) latent variable models.","code":""},{"path":"modeling-explicit-and-latent-hierarchy-in-data.html","id":"mixed","chapter":"8 Modeling Explicit and Latent Hierarchy in Data","heading":"8.1 Mixed models for explicit hierarchy in data","text":"common explicit hierarchies see data group-based time-based. group-based hierarchy occurs taking observations belong different groups. example, first walkthrough example Chapter 4, modeled final examination performance examination performance previous three years. case considered student observation independent identically distributed, ran linear regression model students. receive additional information students actually mix students different degree programs, may wish take account model problem—, want assume student observation independent identically distributed within degree program.Similarly, time-based hierarchy occurs multiple observations subject taken different times. example, conducting weekly survey people course year, modeling answers questions might depend answers others, may wish consider effect person model.situations introduce new grouping variable problem modeling, thus creating hierarchy. hard imagine analyzing group may produce different statistical properties compared analyzing entire population—example, correlations data inside groups less evident looking overall population. Therefore cases model may provide useful inferences grouping taken account.","code":""},{"path":"modeling-explicit-and-latent-hierarchy-in-data.html","id":"fixed-and-random-effects","chapter":"8 Modeling Explicit and Latent Hierarchy in Data","heading":"8.1.1 Fixed and random effects","text":"Let’s imagine set observations consisting continuous outcome variable \\(y\\) input variables \\(x_1, x_2, \\dots, x_p\\). Let’s also assume additional data point observation assign group \\(G\\). asked determine relationship outcome input variables. One option develop linear model \\(y = \\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p + \\epsilon\\), ignoring group data. model, assume coefficients fixed effect input variables—, act every observation way. may fine trust group membership unlikely impact relationship modeled, comfortable making inferences variables observation level ., however, belief group membership may effect relationship modeled, interested interpreting model group observation level, need adjust model mixed model accurate reliable inference. common adjustment random intercept. situation, imagine group membership effect ‘starting point’ relationship: intercept. Therefore, given observation \\(y = \\alpha_G + \\beta_0 + \\beta_1x_1 + \\dots + \\beta_px_p + \\epsilon\\), \\(\\alpha_G\\) random effect mean zero associated group observation member . can restated :\\[\ny = \\beta_G + \\beta_1x_1 + \\dots + \\beta_px_p + \\epsilon\n\\]\\(\\beta_G = \\alpha_G + \\beta_0\\), random intercept mean \\(\\beta_0\\).model similar standard linear regression model, except instead fixed intercept, intercept varies group. Therefore, essentially two ‘levels’ model: one observation level describe \\(y\\) one group level describe \\(\\beta_G\\). reason mixed models sometimes known multilevel models.difficult see approach can extended. example, suppose believe groups also effect coefficient input variable \\(x_1\\) well intercept. \\[\ny = \\beta_{G0} + \\beta_{G1}x_1 + \\beta_2x_2 + \\dots + \\beta_px_p\n\\]\n\\(\\beta_{G0}\\) random intercept mean \\(\\beta_0\\), \\(\\beta_{G1}\\) random slope mean \\(\\beta_1\\). case, mixed model return estimated coefficients observation level statistics random effects \\(\\beta_{G0}\\) \\(\\beta_{G1}\\) group level.Finally, model need linear apply. approach also extends logistic models generalized linear models. example, \\(y\\) binary outcome variable model binomial logistic regression model, last equation translate \\[\n\\mathrm{ln}\\left(\\frac{P(y = 1)}{P(y = 0)}\\right) = \\beta_{G0} + \\beta_{G1}x_1 + \\beta_2x_2 + \\dots + \\beta_px_p\n\\]","code":""},{"path":"modeling-explicit-and-latent-hierarchy-in-data.html","id":"running-a-mixed-model","chapter":"8 Modeling Explicit and Latent Hierarchy in Data","heading":"8.1.2 Running a mixed model","text":"Let’s look fun straightforward example mixed models can useful. speed_dating data set set information captured experiments speed dating students Columbia University New York37. row represents one meeting individual partner opposite sex. data contains following fields:iid id number individual.gender gender individual 0 Female 1 Male.match indicates meeting resulted match.samerace indicates individual partner race.race race individual, race coded follows: Black/African American=1, European/Caucasian-American=2, Latino/Hispanic American=3, Asian/Pacific Islander/Asian-American=4, Native American=5, =6.goal reason individual participating event, coded follows: Seemed like fun night =1, meet new people=2, get date=3, Looking serious relationship=4, say =5, =6.dec binary rating individual whether like see partner (1 Yes 0 ).attr individual’s rating 10 attractiveness partner.intel individual’s rating 10 intelligence level partner.prob individual’s rating 10 whether believe partner want see .agediff absolute difference ages individual partner.data can explored numerous ways, focus modeling options. interested binary outcome dec (decision individual), like understand relates age difference, racial similarity ratings attr, intel prob. First, let’s assume don’t care individual makes mind speed date, interested dynamics speed date decisions. simply run binomial logistic regression data set, ignoring iid grouping variables like race, goal gender.general, see factors significantly influence speed dating decision seem attractiveness partner feeling reciprocation interest partner, age difference, racial similarity intelligence seem play significant role level speed date .Now let’s say interested given individual weighs factors coming decision. Different individuals may different ingoing criteria making speed dating decisions. result, individual may varying base likelihoods positive decision, individual may affected input variables different ways come decision. Therefore need assign random effects individuals based iid. lme4 package R contains functions performing mixed linear regression models mixed generalized linear regression models. functions take formulas additional terms define random effects estimated. function linear model lmer() generalized linear model glmer().simple case, let’s assume individual different ingoing base likelihood making positive decision speed date. therefore model random intercept according iid individual. use formula dec ~ agediff + samerace + attr + intel + prob + (1 | iid), (1 | iid) means ‘random effect iid intercept model’.can see two levels results summary. fixed effects level gives coefficients model observation (speed date) level, random effects tell us intercept (base likelihood) model can vary according individual. see considerable variance intercept individual individual, taking account, now see decision individual given date significantly influenced factors model. stuck simple binomial model, effects age difference, racial similarity intelligence individual level gotten lost, reached erroneous conclusion none really matter speed dating.illustrate graphically, Interactive Figure 8.1 shows speed_dating data subset three individuals IIDs 252, 254 256. curve represents plain binomial logistic regression model fitted attr prob input variables, irrelevant IID individual.\nFigure 8.1: 3D visualization fitted plain binomial model subset speed_dating data three specific iids\nInteractive Figure 8.2 shows three separate curves IID generated mixed binomial logistic regression model random intercept fitted two input variables. , can see different individuals process two inputs decision making different ways, leading different individual formulas determine likelihood positive decision. plain binomial regression model find best single formula data irrelevant individual, mixed model allows us take different individual formulas account determining effects input variables.\nFigure 8.2: 3D visualization individual-level binomial models created iid_intercept_model subset speed_dating data three specific iids\nbelieve different individuals influenced differently one various decision factors consider speed date, can extend random effects slope coefficients model. example use (1 + agediff | iid) model random effect iid intercept agediff coefficient. Similarly, wanted consider two grouping variables—like iid goal—intercept, add (1 | iid) (1 | goal) model formula.","code":"\n# if needed, get data\nurl <- \"http://peopleanalytics-regression-book.org/data/speed_dating.csv\"\nspeed_dating <- read.csv(url)\n# run standard binomial model\nmodel <- glm(dec ~ agediff + samerace + attr + intel + prob, \n             data = speed_dating, \n             family = \"binomial\")\n\nsummary(model)## \n## Call:\n## glm(formula = dec ~ agediff + samerace + attr + intel + prob, \n##     family = \"binomial\", data = speed_dating)\n## \n## Deviance Residuals: \n##     Min       1Q   Median       3Q      Max  \n## -2.6497  -0.8514  -0.3477   0.8809   2.8871  \n## \n## Coefficients:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -5.812900   0.184340 -31.534   <2e-16 ***\n## agediff     -0.010518   0.009029  -1.165   0.2440    \n## samerace    -0.093422   0.055710  -1.677   0.0936 .  \n## attr         0.661139   0.019382  34.111   <2e-16 ***\n## intel       -0.004485   0.020763  -0.216   0.8290    \n## prob         0.270553   0.014565  18.575   <2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## (Dispersion parameter for binomial family taken to be 1)\n## \n##     Null deviance: 10647.3  on 7788  degrees of freedom\n## Residual deviance:  8082.9  on 7783  degrees of freedom\n##   (589 observations deleted due to missingness)\n## AIC: 8094.9\n## \n## Number of Fisher Scoring iterations: 5\n# run binomial mixed effects model\nlibrary(lme4)\n\niid_intercept_model <- lme4:::glmer(\n  dec ~ agediff + samerace + attr + intel + prob + (1 | iid),\n  data = speed_dating,\n  family = \"binomial\"\n)\n# view summary without correlation table of fixed effects\nsummary(iid_intercept_model, \n        correlation = FALSE)## Generalized linear mixed model fit by maximum likelihood (Laplace Approximation) ['glmerMod']\n##  Family: binomial  ( logit )\n## Formula: dec ~ agediff + samerace + attr + intel + prob + (1 | iid)\n##    Data: speed_dating\n## \n##      AIC      BIC   logLik deviance df.resid \n##   6420.3   6469.0  -3203.1   6406.3     7782 \n## \n## Scaled residuals: \n##      Min       1Q   Median       3Q      Max \n## -25.6965  -0.3644  -0.0606   0.3608  25.0368 \n## \n## Random effects:\n##  Groups Name        Variance Std.Dev.\n##  iid    (Intercept) 5.18     2.276   \n## Number of obs: 7789, groups:  iid, 541\n## \n## Fixed effects:\n##              Estimate Std. Error z value Pr(>|z|)    \n## (Intercept) -12.88882    0.42143 -30.583  < 2e-16 ***\n## agediff      -0.03671    0.01401  -2.621  0.00877 ** \n## samerace      0.20187    0.08139   2.480  0.01313 *  \n## attr          1.07894    0.03334  32.363  < 2e-16 ***\n## intel         0.31592    0.03473   9.098  < 2e-16 ***\n## prob          0.61998    0.02873  21.581  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"},{"path":"modeling-explicit-and-latent-hierarchy-in-data.html","id":"struc-eq-model","chapter":"8 Modeling Explicit and Latent Hierarchy in Data","heading":"8.2 Structural equation models for latent hierarchy in data","text":"section focus entirely survey data use cases, common application structural equation modeling people analytics. However noted survey data situation latent variables may modeled, technology substantially broader applications. Indeed, advanced practitioners may see opportunities experiment technology use cases.frequent occurrence surveys conducted large samples people, public survey large company survey, attempts run regression models can problematic due large number survey questions items. Often many items highly correlated, even , high dimensionality makes interpretability challenging. Decision-makers usually interested explanations involve 50 100 variables.Usually, large number survey items independently measuring different construct. Many items can considered addressing similar thematic constructs. example, items ‘believe compensated well’ ‘happy benefits offered employer’ considered related employee rewards. cases, survey instruments can explicitly constructed around themes, cases, surveys grown organically time include disorganized set items grouped themes fact.common request analyst model certain outcome using many items complex survey input variables. cases outcome modeled item survey —usually overall measure sentiment—cases outcome independent survey instrument, example future attrition organization. situation, model using themes input variables likely lot useful interpretable model using items input variables.Structural equation modeling technique allows analyst hypothesize smaller set latent variables factors explain responses survey items (‘measured variables’), regresses outcome interest latent factors. two-part approach, part separate model , follows:Measurement model: focused well hypothesized factors explain responses survey items using technique called factor analysis. common case, subject matter expert pre-organized items several groups corresponding hypothesized latent variables, process called confirmatory factor analysis, objective confirm groupings represent high-quality measurement model, adjusting necessary refine model. simplest case, items fitted separate independent themes overlap.Measurement model: focused well hypothesized factors explain responses survey items using technique called factor analysis. common case, subject matter expert pre-organized items several groups corresponding hypothesized latent variables, process called confirmatory factor analysis, objective confirm groupings represent high-quality measurement model, adjusting necessary refine model. simplest case, items fitted separate independent themes overlap.Structural model: Assuming satisfactory measurement model, structural model effectively regression model explains proposed factors relate outcome interest.Structural model: Assuming satisfactory measurement model, structural model effectively regression model explains proposed factors relate outcome interest.walkthrough example, work politics_survey data set.data set represents results survey conducted political party set approximately 2100 voters. results Likert scale 1 4 1 indicates strong negative sentiment statement 4 indicates strong positive sentiment. Subject matter experts already grouped items proposed latent variables factors, data takes following form:Overall represents overall intention vote party next election.Items beginning Pol considered related policies political party.Items beginning Hab considered related prior voting habits relation political party.Items beginning Loc considered related interest local issues around respondent resided.Items beginning Env considered related interest environmental issues.Items beginning Int considered related interest international issues.Items beginning Pers considered related personalities party representatives/leaders.Items beginning Nat considered related interest national issues.Items beginning Eco considered related interest economic issues.Let’s take quick look data.outcome interest Overall rating. first aim confirm eight factors suggested subject matter experts represent satisfactory measurement model (reasonably explain responses 22 items), adjusting refining needed. Assuming can confirm satisfactory measurement model, second aim run structural model determine factor relates overall intention vote party next election.","code":"\n# if needed, get data\nurl <- \"http://peopleanalytics-regression-book.org/data/politics_survey.csv\"\npolitics_survey <- read.csv(url)\nhead(politics_survey)##   Overall Pol1 Pol2 Pol3 Hab1 Hab2 Hab3 Loc1 Loc2 Loc3 Loc4 Env1 Env2 Int1 Int2 Pers1 Pers2 Pers3 Nat1 Nat2 Nat3 Eco1\n## 1       3    2    2    2    2    2    2    3    3    2    2    2    3    3    3     3     4     4    3    3    4    3\n## 2       4    4    4    4    4    4    4    4    4    4    4    4    4    4    3     4     4     4    4    4    4    3\n## 3       4    4    4    4    3    2    2    4    4    4    4    4    4    4    4     4     4     4    4    4    4    4\n## 4       3    4    4    4    3    2    2    4    3    3    4    4    4    4    3     2     3     3    4    4    2    4\n## 5       3    3    3    4    4    3    3    3    4    3    3    4    4    3    4     4     3     3    4    3    4    3\n## 6       4    3    3    4    3    2    3    3    3    2    2    3    3    4    3     3     4     3    3    4    3    4\n##   Eco2\n## 1    3\n## 2    4\n## 3    4\n## 4    4\n## 5    3\n## 6    4"},{"path":"modeling-explicit-and-latent-hierarchy-in-data.html","id":"running-and-assessing-the-measurement-model","chapter":"8 Modeling Explicit and Latent Hierarchy in Data","heading":"8.2.1 Running and assessing the measurement model","text":"proposed measurement model can seen Figure 8.3. path diagram, see eight latent variables factors (circles) map individual measured items (squares) survey using single headed arrows. making simplifying assumption latent variable influences independent group survey items. diagram also notes latent variables may correlated , indicated double-headed arrows top. Dashed-line paths indicate specific item used scale variance latent factor.\nFigure 8.3: Simple path diagram showing proposed measurement model politics_survey\nlavaan package R specialized package running analysis latent variables. function cfa() can used perform confirmatory factor analysis specified measurement model. measurement model can specified using appropriately commented formatted text string follows. Note =~ notation, note also factor defined new line.measurement model defined, confirmatory factor analysis can run summary viewed. lavaan summary functions used section produce quite large outputs . proceed highlight parts output important interpreting refining model.large set results, can focus important parameters examine. First, note results come attached warning. One particular warning look relates covariance matrix non-positive definite. renders attempted measurement invalid usually caused small sample size complexity measurement model. Since receive warning, can proceed safely.Second, examine fit statistics. Numerous statistics reported38, larger samples data set, following measures examined:CFI TLI, compare proposed model baseline (null random) model determine better. Ideally look measures exceed 0.95. see measurement model comes close meeting criteria.CFI TLI, compare proposed model baseline (null random) model determine better. Ideally look measures exceed 0.95. see measurement model comes close meeting criteria.RMSEA ideally less 0.06, met measurement model.RMSEA ideally less 0.06, met measurement model.SRMR ideally less 0.08, met measurement model.SRMR ideally less 0.08, met measurement model.Finally, parameter estimates latent variables examined. particular Std.column similar standardized regression coefficients. parameters commonly known factor loadings—can interpreted extent item response explained proposed latent variable. general, factor loadings 0.7 considered reasonable. Factor loadings less may introducing unacceptable measurement error. One option occurs drop item completely measurement model, explore alternative measurement model item assigned another latent variable. case analyst need balance considerations need factors measured multiple items wherever possible order minimize aspects measurement error.case consider dropping Pol3, Loc1, Pers1 Nat3 measurement model factor loadings less 0.7 factors contain three items. fit revised measurement model, rather printing entire output , focus CFI, TLI, RMSEA SRMR statistics see improved. advisable, however, factor loadings also checked, especially primary items scale variance latent factors removed.now see measurement model comfortably meets fit requirements. case chose completely drop four items model. Analysts may wish experiment relaxing criteria dropping items, reassigning items factors achieve good balance fit factor measurement reliability.","code":"\n# define measurement model\n\nmeas_mod <- \"\n# measurement model\nPol =~ Pol1 + Pol2 + Pol3\nHab =~ Hab1 + Hab2 + Hab3\nLoc =~ Loc1 + Loc2 + Loc3\nEnv =~ Env1 + Env2\nInt =~ Int1 + Int2\nPers =~ Pers1 + Pers2 + Pers3\nNat =~ Nat1 + Nat2 + Nat3\nEco =~ Eco1 + Eco2\n\"\nlibrary(lavaan)\n\ncfa_meas_mod <- lavaan::cfa(model = meas_mod, data = politics_survey)\nlavaan::summary(cfa_meas_mod, fit.measures = TRUE, standardized = TRUE)## lavaan 0.6-7 ended normally after 108 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of free parameters                         70\n##                                                       \n##   Number of observations                          2108\n##                                                       \n## Model Test User Model:\n##                                                       \n##   Test statistic                               838.914\n##   Degrees of freedom                               161\n##   P-value (Chi-square)                           0.000\n## \n## Model Test Baseline Model:\n## \n##   Test statistic                             17137.996\n##   Degrees of freedom                               210\n##   P-value                                        0.000\n## \n## User Model versus Baseline Model:\n## \n##   Comparative Fit Index (CFI)                    0.960\n##   Tucker-Lewis Index (TLI)                       0.948\n## \n## Loglikelihood and Information Criteria:\n## \n##   Loglikelihood user model (H0)             -37861.518\n##   Loglikelihood unrestricted model (H1)     -37442.061\n##                                                       \n##   Akaike (AIC)                               75863.036\n##   Bayesian (BIC)                             76258.780\n##   Sample-size adjusted Bayesian (BIC)        76036.383\n## \n## Root Mean Square Error of Approximation:\n## \n##   RMSEA                                          0.045\n##   90 Percent confidence interval - lower         0.042\n##   90 Percent confidence interval - upper         0.048\n##   P-value RMSEA <= 0.05                          0.998\n## \n## Standardized Root Mean Square Residual:\n## \n##   SRMR                                           0.035\n## \n## Parameter Estimates:\n## \n##   Standard errors                             Standard\n##   Information                                 Expected\n##   Information saturated (h1) model          Structured\n## \n## Latent Variables:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##   Pol =~                                                                \n##     Pol1              1.000                               0.568    0.772\n##     Pol2              0.883    0.032   27.431    0.000    0.501    0.737\n##     Pol3              0.488    0.024   20.575    0.000    0.277    0.512\n##   Hab =~                                                                \n##     Hab1              1.000                               0.623    0.755\n##     Hab2              1.207    0.032   37.980    0.000    0.752    0.887\n##     Hab3              1.138    0.031   36.603    0.000    0.710    0.815\n##   Loc =~                                                                \n##     Loc1              1.000                               0.345    0.596\n##     Loc2              1.370    0.052   26.438    0.000    0.473    0.827\n##     Loc3              1.515    0.058   26.169    0.000    0.523    0.801\n##   Env =~                                                                \n##     Env1              1.000                               0.408    0.809\n##     Env2              0.605    0.031   19.363    0.000    0.247    0.699\n##   Int =~                                                                \n##     Int1              1.000                               0.603    0.651\n##     Int2              1.264    0.060   20.959    0.000    0.762    0.869\n##   Pers =~                                                               \n##     Pers1             1.000                               0.493    0.635\n##     Pers2             1.048    0.041   25.793    0.000    0.517    0.770\n##     Pers3             0.949    0.039   24.440    0.000    0.468    0.695\n##   Nat =~                                                                \n##     Nat1              1.000                               0.522    0.759\n##     Nat2              0.991    0.032   31.325    0.000    0.518    0.744\n##     Nat3              0.949    0.035   27.075    0.000    0.495    0.638\n##   Eco =~                                                                \n##     Eco1              1.000                               0.525    0.791\n##     Eco2              1.094    0.042   26.243    0.000    0.575    0.743\n## \n## Covariances:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##   Pol ~~                                                                \n##     Hab               0.165    0.011   14.947    0.000    0.466    0.466\n##     Loc               0.106    0.007   15.119    0.000    0.540    0.540\n##     Env               0.089    0.007   12.101    0.000    0.385    0.385\n##     Int               0.146    0.012   12.248    0.000    0.425    0.425\n##     Pers              0.162    0.010   15.699    0.000    0.577    0.577\n##     Nat               0.177    0.010   17.209    0.000    0.596    0.596\n##     Eco               0.150    0.010   15.123    0.000    0.504    0.504\n##   Hab ~~                                                                \n##     Loc               0.069    0.006   11.060    0.000    0.323    0.323\n##     Env               0.051    0.007    7.161    0.000    0.200    0.200\n##     Int               0.134    0.012   11.395    0.000    0.357    0.357\n##     Pers              0.121    0.010   12.619    0.000    0.393    0.393\n##     Nat               0.105    0.009   11.271    0.000    0.324    0.324\n##     Eco               0.089    0.009    9.569    0.000    0.273    0.273\n##   Loc ~~                                                                \n##     Env               0.076    0.005   15.065    0.000    0.541    0.541\n##     Int               0.091    0.007   12.192    0.000    0.438    0.438\n##     Pers              0.098    0.007   14.856    0.000    0.574    0.574\n##     Nat               0.116    0.007   16.780    0.000    0.642    0.642\n##     Eco               0.090    0.006   14.354    0.000    0.496    0.496\n##   Env ~~                                                                \n##     Int               0.075    0.008    9.506    0.000    0.303    0.303\n##     Pers              0.075    0.007   11.482    0.000    0.375    0.375\n##     Nat               0.093    0.007   13.616    0.000    0.439    0.439\n##     Eco               0.078    0.007   11.561    0.000    0.365    0.365\n##   Int ~~                                                                \n##     Pers              0.156    0.012   13.349    0.000    0.525    0.525\n##     Nat               0.186    0.012   14.952    0.000    0.592    0.592\n##     Eco               0.137    0.011   12.374    0.000    0.432    0.432\n##   Pers ~~                                                               \n##     Nat               0.185    0.010   17.898    0.000    0.717    0.717\n##     Eco               0.153    0.010   15.945    0.000    0.590    0.590\n##   Nat ~~                                                                \n##     Eco               0.196    0.010   19.440    0.000    0.715    0.715\n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##    .Pol1              0.219    0.012   19.015    0.000    0.219    0.404\n##    .Pol2              0.211    0.010   21.455    0.000    0.211    0.457\n##    .Pol3              0.216    0.007   29.384    0.000    0.216    0.737\n##    .Hab1              0.293    0.011   25.855    0.000    0.293    0.430\n##    .Hab2              0.153    0.011   14.434    0.000    0.153    0.213\n##    .Hab3              0.254    0.012   21.814    0.000    0.254    0.335\n##    .Loc1              0.217    0.007   29.063    0.000    0.217    0.645\n##    .Loc2              0.103    0.006   18.226    0.000    0.103    0.316\n##    .Loc3              0.153    0.007   20.463    0.000    0.153    0.358\n##    .Env1              0.088    0.008   10.643    0.000    0.088    0.345\n##    .Env2              0.064    0.003   18.407    0.000    0.064    0.511\n##    .Int1              0.495    0.021   23.182    0.000    0.495    0.576\n##    .Int2              0.188    0.025    7.653    0.000    0.188    0.244\n##    .Pers1             0.361    0.013   27.065    0.000    0.361    0.597\n##    .Pers2             0.184    0.009   20.580    0.000    0.184    0.408\n##    .Pers3             0.234    0.009   24.865    0.000    0.234    0.517\n##    .Nat1              0.201    0.009   23.320    0.000    0.201    0.425\n##    .Nat2              0.215    0.009   24.119    0.000    0.215    0.446\n##    .Nat3              0.357    0.013   27.981    0.000    0.357    0.593\n##    .Eco1              0.165    0.010   16.244    0.000    0.165    0.374\n##    .Eco2              0.268    0.013   20.071    0.000    0.268    0.448\n##     Pol               0.323    0.018   18.035    0.000    1.000    1.000\n##     Hab               0.389    0.020   19.276    0.000    1.000    1.000\n##     Loc               0.119    0.009   13.906    0.000    1.000    1.000\n##     Env               0.166    0.011   15.560    0.000    1.000    1.000\n##     Int               0.364    0.026   13.846    0.000    1.000    1.000\n##     Pers              0.243    0.017   14.619    0.000    1.000    1.000\n##     Nat               0.273    0.015   18.788    0.000    1.000    1.000\n##     Eco               0.276    0.015   17.974    0.000    1.000    1.000\nmeas_mod_revised <- \"\n# measurement model\nPol =~ Pol1 + Pol2\nHab =~ Hab1 + Hab2 + Hab3\nLoc =~ Loc2 + Loc3\nEnv =~ Env1 + Env2\nInt =~ Int1 + Int2\nPers =~ Pers2 + Pers3\nNat =~ Nat1 + Nat2\nEco =~ Eco1 + Eco2\n\"\n\ncfa_meas_mod_rev <- lavaan::cfa(model = meas_mod_revised, \n                                data = politics_survey)\n\nfits <- lavaan::fitmeasures(cfa_meas_mod_rev)\n\nfits[c(\"cfi\", \"tli\", \"rmsea\", \"srmr\")]##        cfi        tli      rmsea       srmr \n## 0.97804888 0.96719394 0.03966962 0.02736629"},{"path":"modeling-explicit-and-latent-hierarchy-in-data.html","id":"running-and-interpreting-the-structural-model","chapter":"8 Modeling Explicit and Latent Hierarchy in Data","heading":"8.2.2 Running and interpreting the structural model","text":"satisfactory measurement model, structural model simple regression formula. sem() function lavaan can used perform full structural equation model including measurement model structural model. Like cfa(), extensive output can expected function, assuming measurement model satisfactory, key interest now structural model elements output.Std.column Regressions section output provides fundamentals structural model—standardized estimates can approximately interpreted proportion variance outcome explained factor. can make following interpretations:Policies, habit interest local issues represent three strongest drivers likelihood voting party next election, explain approximately 70% overall variance outcome.Policies, habit interest local issues represent three strongest drivers likelihood voting party next election, explain approximately 70% overall variance outcome.Interest national international issues, interest economy significant relationship likelihood vote party next election.Interest national international issues, interest economy significant relationship likelihood vote party next election.Interest environment significant negative relationship likelihood vote party next election.Interest environment significant negative relationship likelihood vote party next election.\nFigure 8.4: Path diagram full structural equation model politics_survey\nfull structural equation model can seen Figure 8.4. simple example illustrates value structural equation modeling reducing dimensions complex regression problem developing intuitive interpretable results stakeholders. underlying theory latent variable modeling, implementation lavaan package, offer much flexibility parameter control options illustrated exploration highly recommended. Bartholomew, Knott, Moustaki (2011) Skrondal Rabe-Hesketh (2004) excellent resources deeper study theory wider range case examples.","code":"\n# define full SEM using revised measurement model\nfull_sem <- \"\n# measurement model\nPol =~ Pol1 + Pol2\nHab =~ Hab1 + Hab2 + Hab3\nLoc =~ Loc2 + Loc3\nEnv =~ Env1 + Env2\nInt =~ Int1 + Int2\nPers =~ Pers2 + Pers3\nNat =~ Nat1 + Nat2\nEco =~ Eco1 + Eco2\n\n# structural model\nOverall ~ Pol + Hab + Loc + Env + Int + Pers + Nat + Eco\n\"\n\n# run full SEM \nfull_model <- lavaan::sem(model = full_sem, data = politics_survey)\nlavaan::summary(full_model, standardized = TRUE)## lavaan 0.6-7 ended normally after 99 iterations\n## \n##   Estimator                                         ML\n##   Optimization method                           NLMINB\n##   Number of free parameters                         71\n##                                                       \n##   Number of observations                          2108\n##                                                       \n## Model Test User Model:\n##                                                       \n##   Test statistic                               465.318\n##   Degrees of freedom                               100\n##   P-value (Chi-square)                           0.000\n## \n## Parameter Estimates:\n## \n##   Standard errors                             Standard\n##   Information                                 Expected\n##   Information saturated (h1) model          Structured\n## \n## Latent Variables:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##   Pol =~                                                                \n##     Pol1              1.000                               0.626    0.850\n##     Pol2              0.714    0.029   25.038    0.000    0.447    0.657\n##   Hab =~                                                                \n##     Hab1              1.000                               0.630    0.763\n##     Hab2              1.184    0.031   38.592    0.000    0.746    0.879\n##     Hab3              1.127    0.030   37.058    0.000    0.710    0.816\n##   Loc =~                                                                \n##     Loc2              1.000                               0.461    0.806\n##     Loc3              1.179    0.036   32.390    0.000    0.544    0.833\n##   Env =~                                                                \n##     Env1              1.000                               0.411    0.815\n##     Env2              0.596    0.031   19.281    0.000    0.245    0.695\n##   Int =~                                                                \n##     Int1              1.000                               0.605    0.653\n##     Int2              1.256    0.062   20.366    0.000    0.760    0.867\n##   Pers =~                                                               \n##     Pers2             1.000                               0.520    0.774\n##     Pers3             0.939    0.036   25.818    0.000    0.488    0.726\n##   Nat =~                                                                \n##     Nat1              1.000                               0.511    0.742\n##     Nat2              1.033    0.034   29.958    0.000    0.527    0.758\n##   Eco =~                                                                \n##     Eco1              1.000                               0.529    0.797\n##     Eco2              1.078    0.042   25.716    0.000    0.570    0.737\n## \n## Regressions:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##   Overall ~                                                             \n##     Pol               0.330    0.036    9.281    0.000    0.206    0.307\n##     Hab               0.255    0.024   10.614    0.000    0.161    0.240\n##     Loc               0.224    0.047    4.785    0.000    0.103    0.154\n##     Env              -0.114    0.042   -2.738    0.006   -0.047   -0.070\n##     Int               0.046    0.028    1.605    0.108    0.028    0.041\n##     Pers              0.112    0.047    2.383    0.017    0.058    0.087\n##     Nat               0.122    0.071    1.728    0.084    0.063    0.093\n##     Eco               0.002    0.043    0.041    0.967    0.001    0.001\n## \n## Covariances:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##   Pol ~~                                                                \n##     Hab               0.183    0.012   15.476    0.000    0.465    0.465\n##     Loc               0.156    0.009   16.997    0.000    0.540    0.540\n##     Env               0.096    0.008   12.195    0.000    0.374    0.374\n##     Int               0.162    0.013   12.523    0.000    0.427    0.427\n##     Pers              0.171    0.011   15.975    0.000    0.525    0.525\n##     Nat               0.195    0.011   17.798    0.000    0.610    0.610\n##     Eco               0.167    0.011   15.752    0.000    0.506    0.506\n##   Hab ~~                                                                \n##     Loc               0.091    0.008   11.218    0.000    0.315    0.315\n##     Env               0.052    0.007    7.199    0.000    0.200    0.200\n##     Int               0.138    0.012   11.426    0.000    0.361    0.361\n##     Pers              0.112    0.010   11.484    0.000    0.341    0.341\n##     Nat               0.105    0.010   11.045    0.000    0.327    0.327\n##     Eco               0.091    0.009    9.608    0.000    0.273    0.273\n##   Loc ~~                                                                \n##     Env               0.103    0.006   16.413    0.000    0.544    0.544\n##     Int               0.120    0.010   12.529    0.000    0.429    0.429\n##     Pers              0.130    0.008   16.209    0.000    0.542    0.542\n##     Nat               0.153    0.008   18.203    0.000    0.648    0.648\n##     Eco               0.117    0.008   14.985    0.000    0.479    0.479\n##   Env ~~                                                                \n##     Int               0.075    0.008    9.505    0.000    0.303    0.303\n##     Pers              0.075    0.007   11.058    0.000    0.351    0.351\n##     Nat               0.091    0.007   13.181    0.000    0.434    0.434\n##     Eco               0.079    0.007   11.583    0.000    0.364    0.364\n##   Int ~~                                                                \n##     Pers              0.153    0.012   13.118    0.000    0.486    0.486\n##     Nat               0.173    0.012   14.159    0.000    0.560    0.560\n##     Eco               0.138    0.011   12.293    0.000    0.431    0.431\n##   Pers ~~                                                               \n##     Nat               0.192    0.010   18.922    0.000    0.724    0.724\n##     Eco               0.156    0.010   16.369    0.000    0.567    0.567\n##   Nat ~~                                                                \n##     Eco               0.192    0.010   18.968    0.000    0.710    0.710\n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(>|z|)   Std.lv  Std.all\n##    .Pol1              0.150    0.013   11.205    0.000    0.150    0.277\n##    .Pol2              0.263    0.010   25.479    0.000    0.263    0.569\n##    .Hab1              0.285    0.011   25.570    0.000    0.285    0.418\n##    .Hab2              0.163    0.010   15.746    0.000    0.163    0.227\n##    .Hab3              0.253    0.011   22.046    0.000    0.253    0.334\n##    .Loc2              0.114    0.006   18.168    0.000    0.114    0.350\n##    .Loc3              0.130    0.008   15.729    0.000    0.130    0.306\n##    .Env1              0.085    0.008   10.227    0.000    0.085    0.336\n##    .Env2              0.064    0.003   18.701    0.000    0.064    0.518\n##    .Int1              0.492    0.022   22.597    0.000    0.492    0.574\n##    .Int2              0.192    0.025    7.547    0.000    0.192    0.249\n##    .Pers2             0.181    0.010   17.472    0.000    0.181    0.401\n##    .Pers3             0.215    0.010   21.151    0.000    0.215    0.474\n##    .Nat1              0.213    0.009   22.690    0.000    0.213    0.450\n##    .Nat2              0.205    0.010   21.502    0.000    0.205    0.425\n##    .Eco1              0.160    0.010   15.413    0.000    0.160    0.364\n##    .Eco2              0.273    0.014   20.156    0.000    0.273    0.457\n##    .Overall           0.242    0.008   29.506    0.000    0.242    0.537\n##     Pol               0.392    0.020   19.235    0.000    1.000    1.000\n##     Hab               0.397    0.020   19.581    0.000    1.000    1.000\n##     Loc               0.213    0.011   19.724    0.000    1.000    1.000\n##     Env               0.169    0.011   15.606    0.000    1.000    1.000\n##     Int               0.366    0.027   13.699    0.000    1.000    1.000\n##     Pers              0.270    0.015   17.514    0.000    1.000    1.000\n##     Nat               0.261    0.015   17.787    0.000    1.000    1.000\n##     Eco               0.280    0.016   17.944    0.000    1.000    1.000"},{"path":"modeling-explicit-and-latent-hierarchy-in-data.html","id":"learning-exercises-6","chapter":"8 Modeling Explicit and Latent Hierarchy in Data","heading":"8.3 Learning exercises","text":"","code":""},{"path":"modeling-explicit-and-latent-hierarchy-in-data.html","id":"discussion-questions-6","chapter":"8 Modeling Explicit and Latent Hierarchy in Data","heading":"8.3.1 Discussion questions","text":"Describe common forms explicit hierarchies data. Can think data sets worked recently contain explicit hierarchy?Describe meaning ‘fixed effect’ ‘random effect’ mixed regression model.parameter mixed regression model commonly used applying random effect?Describe mixed models sometimes referred multilevel models.two-level mixed model, describe two levels statistics produced interpret statistics.latent variable modeling, difference latent variable measured variable?Describe reasons latent variable modeling can valuable practice.Describe two components structural equation model. purpose component?steps involved confirmatory factor analysis sufficiently large data set? Describe fit criteria ideal standards criteria.Describe process refining factor analysis based fit criteria factor loadings. considerations addressed process?","code":""},{"path":"modeling-explicit-and-latent-hierarchy-in-data.html","id":"data-exercises-6","chapter":"8 Modeling Explicit and Latent Hierarchy in Data","heading":"8.3.2 Data exercises","text":"Exercises 1–4, use speed_dating set used earlier chapter39.Split data two sets according gender participant. Run standard binomial logistic regression models set determine relationship dec decision outcome input variables samerace, agediff, attr, intel prob.Run similar mixed models sets random intercept iid.different conclusions can make comparing mixed models standard models?Experiment random slope effects see reveal anything new input variables.exercises 5–10, load employee_survey data set via peopleanalyticsdata package download internet40. data set contains results engagement survey employees technology company. row represents responses individual survey column represents specific survey question, responses Likert scale 1 4, 1 indicating strongly negative sentiment 4 indicating strongly positive sentiment. Subject matter experts grouped items hypothesized latent factors follows:Happiness overall measure employees current sentiment job.Items beginning Ben relate employment benefits.Items beginning Work relate general work environment.Items beginning Man relate perceptions management.Items beginning Car relate perceptions career prospects.Write proposed measurement model, defining latent factors terms measured items.Run confirmatory factor analysis proposed measurement model. Examine fit factor loadings.Experiment removal measured items measurement model order improve overall fit.satisfied fit measurement model, run full structural equation model data.Interpret results structural model. factors appear related overall employee sentiment? Approximately proportion variance overall sentiment model explain?dropped measured items measurement model, experiment assigning factors see improves fit model. statistics use compare different measurement models?","code":""},{"path":"survival.html","id":"survival","chapter":"9 Survival Analysis for Modeling Singular Events Over Time","heading":"9 Survival Analysis for Modeling Singular Events Over Time","text":"previous chapters, outcomes modeling occurred particular point time following input variables measured. example, Chapter 4 input variables measured first three years education program outcome measured end fourth year. many situations, outcome interested singular event can occur time following input variables measured, can occur different time different individuals, occurred reoccur repeat. medical studies, death can occur onset disease can diagnosed time study period. employment contexts, attrition event can occur various times throughout year.obvious simple way deal simply agree look specific point time measure whether event occurred point, example, ‘many employees left three-year point?’. approach allows us use standard generic regression models like studied previous chapters. approach limitations.Firstly, able infer conclusions likelihood event occurred end period study. make inferences likelihood event throughout period study. able say attrition twice likely certain types individuals time throughout three years powerful merely saying attrition twice likely three-year point.Secondly, sample size constrained state data end period study. Therefore lose track individual two years six months, observation needs dropped data set focused three-year point. Wherever possible, loss data something statistician want avoid affects accuracy statistical power inferences, also means research effort wasted.Survival analysis general term modeling time-associated binary non-repeated outcome, usually involving understanding comparative risk outcome two different groups interest. two common components elementary survival analysis, follows:graphical representation future outcome risk different groups time, using survival curves based Kaplan-Meier estimates survival rate. usually effective way establish prima facie relevance certain input variable survival outcome effective visual way communicating relevance input variable non-statisticians.Cox proportional hazard regression model establish statistical significance input variables estimate effect input variable comparative risk outcome throughout study period.seeking depth treatment survival analysis consult texts use medical/clinical contexts, recommended source Collett (2015). chapter use walkthrough example illustrate typical use survival analysis people analytics context.job_retention data set shows results study around 3,800 individuals employed various fields employment one-year period. beginning study, individuals asked rate sentiment towards job. individuals followed monthly year determine still working job left job substantially different job. individual successfully followed given month, longer followed remainder study period.walkthrough example, particular fields interested :gender: gender individual studiedfield: field employment worked beginning studylevel: level position organization beginning study—Low, Medium Highsentiment: sentiment score reported scale 1 10 beginning study, 1 indicating extremely negative sentiment 10 indicating extremely positive sentimentleft: binary variable indicating whether individual left job last follow-upmonth: month last follow-","code":"\n# if needed, get job_retention data\nurl <- \"http://peopleanalytics-regression-book.org/data/job_retention.csv\"\njob_retention <- read.csv(url)\nhead(job_retention)##   gender                  field  level sentiment intention left month\n## 1      M      Public/Government   High         3         8    1     1\n## 2      F                Finance    Low         8         4    0    12\n## 3      M Education and Training Medium         7         7    1     5\n## 4      M                Finance    Low         8         4    0    12\n## 5      M                Finance   High         7         6    1     1\n## 6      F                 Health Medium         6        10    1     2"},{"path":"survival.html","id":"tracking-and-illustrating-survival-rates-over-the-study-period","chapter":"9 Survival Analysis for Modeling Singular Events Over Time","heading":"9.1 Tracking and illustrating survival rates over the study period","text":"example, defining ‘survival’ ‘remaining substantially job’‍. can regard starting point month 0, following months 1 12. given month \\(\\), can define survival rate \\(S_i\\) follows\\[\nS_i = S_{- 1}(1 - \\frac{l_i}{n_i})\n\\]\n\\(l_i\\) number reported left month \\(\\), \\(n_i\\) number still substantially job month \\(- 1\\), \\(S_0 = 1\\).survival package R allows easy construction survival rates data similar format job_retention data set. survival object created using Surv() function track survival rate time period.can see survival object records month individual left job recorded done data set. , object records last month record individual, appended ‘+’ indicate last record available.survfit() function allows us calculate Kaplan-Meier estimates survival different groups data can compare . can using usual formula notation using survival object outcome. Let’s take look survival gender.can see n.risk, n.event survival columns group correspond \\(n_i\\), \\(l_i\\) \\(S_i\\) formula confidence intervals survival rate given. can useful wish illustrate likely effect given input variable survival likelihood.Let’s imagine wish determine sentiment individual impact survival likelihood. can divide population two () groups based sentiment compare survival rates.can see survival seems consistently trend higher high sentiment towards jobs. ggsurvplot() function survminer package can visualize neatly also provide additional statistical information differences groups, shown Figure 9.1.\nFigure 9.1: Survival curves sentiment category job_retention data\nconfirms survival difference two sentiment groups statistically significant provides highly intuitive visualization effect sentiment retention throughout period study.","code":"\nlibrary(survival)\n\n# create survival object with event as 'left' and time as 'month'\nretention <- Surv(event = job_retention$left, \n                  time = job_retention$month)\n\n# view unique values of retention\nunique(retention)##  [1]  1  12+  5   2   3   6   8   4   8+  4+ 11  10   9   7+  5+  3+  7   9+ 11+ 12  10+  6+  2+  1+\n# kaplan-meier estimates of survival by gender\nkmestimate_gender <- survival::survfit(\n  formula = Surv(event = left, time = month) ~ gender, \n  data = job_retention\n)\n\nsummary(kmestimate_gender)## Call: survfit(formula = Surv(event = left, time = month) ~ gender, \n##     data = job_retention)\n## \n##                 gender=F \n##  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n##     1   1167       7    0.994 0.00226        0.990        0.998\n##     2   1140      24    0.973 0.00477        0.964        0.982\n##     3   1102      45    0.933 0.00739        0.919        0.948\n##     4   1044      45    0.893 0.00919        0.875        0.911\n##     5    987      30    0.866 0.01016        0.846        0.886\n##     6    940      51    0.819 0.01154        0.797        0.842\n##     7    882      43    0.779 0.01248        0.755        0.804\n##     8    830      47    0.735 0.01333        0.709        0.762\n##     9    770      40    0.697 0.01394        0.670        0.725\n##    10    718      21    0.676 0.01422        0.649        0.705\n##    11    687      57    0.620 0.01486        0.592        0.650\n##    12    621      17    0.603 0.01501        0.575        0.633\n## \n##                 gender=M \n##  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n##     1   2603      17    0.993 0.00158        0.990        0.997\n##     2   2559      66    0.968 0.00347        0.961        0.975\n##     3   2473     100    0.929 0.00508        0.919        0.939\n##     4   2360      86    0.895 0.00607        0.883        0.907\n##     5   2253      56    0.873 0.00660        0.860        0.886\n##     6   2171     120    0.824 0.00756        0.810        0.839\n##     7   2029      85    0.790 0.00812        0.774        0.806\n##     8   1916     114    0.743 0.00875        0.726        0.760\n##     9   1782      96    0.703 0.00918        0.685        0.721\n##    10   1661      50    0.682 0.00938        0.664        0.700\n##    11   1590     101    0.638 0.00972        0.620        0.658\n##    12   1460      36    0.623 0.00983        0.604        0.642\n# create a new field to define high sentiment (>= 7)\njob_retention$sentiment_category <- ifelse(\n  job_retention$sentiment >= 7, \n  \"High\", \n  \"Not High\"\n)\n\n# generate survival rates by sentiment category\nkmestimate_sentimentcat <- survival::survfit(\n  formula = Surv(event = left, time = month) ~ sentiment_category,\n  data = job_retention\n)\n\nsummary(kmestimate_sentimentcat)## Call: survfit(formula = Surv(event = left, time = month) ~ sentiment_category, \n##     data = job_retention)\n## \n##                 sentiment_category=High \n##  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n##     1   3225      15    0.995 0.00120        0.993        0.998\n##     2   3167      62    0.976 0.00272        0.971        0.981\n##     3   3075     120    0.938 0.00429        0.929        0.946\n##     4   2932     102    0.905 0.00522        0.895        0.915\n##     5   2802      65    0.884 0.00571        0.873        0.895\n##     6   2700     144    0.837 0.00662        0.824        0.850\n##     7   2532     110    0.801 0.00718        0.787        0.815\n##     8   2389     140    0.754 0.00778        0.739        0.769\n##     9   2222     112    0.716 0.00818        0.700        0.732\n##    10   2077      56    0.696 0.00835        0.680        0.713\n##    11   1994     134    0.650 0.00871        0.633        0.667\n##    12   1827      45    0.634 0.00882        0.617        0.651\n## \n##                 sentiment_category=Not High \n##  time n.risk n.event survival std.err lower 95% CI upper 95% CI\n##     1    545       9    0.983 0.00546        0.973        0.994\n##     2    532      28    0.932 0.01084        0.911        0.953\n##     3    500      25    0.885 0.01373        0.859        0.912\n##     4    472      29    0.831 0.01618        0.800        0.863\n##     5    438      21    0.791 0.01758        0.757        0.826\n##     6    411      27    0.739 0.01906        0.703        0.777\n##     7    379      18    0.704 0.01987        0.666        0.744\n##     8    357      21    0.662 0.02065        0.623        0.704\n##     9    330      24    0.614 0.02136        0.574        0.658\n##    10    302      15    0.584 0.02171        0.543        0.628\n##    11    283      24    0.534 0.02209        0.493        0.579\n##    12    254       8    0.517 0.02218        0.476        0.563\nlibrary(survminer)\n\n# show survival curves with p-value estimate and confidence intervals\nsurvminer::ggsurvplot(\n  kmestimate_sentimentcat,\n  pval = TRUE,\n  conf.int = TRUE,\n  palette = c(\"blue\", \"red\"),\n  linetype = c(\"solid\", \"dashed\"),\n  xlab = \"Month\",\n  ylab = \"Retention Rate\"\n)"},{"path":"survival.html","id":"coxphmodel","chapter":"9 Survival Analysis for Modeling Singular Events Over Time","heading":"9.2 Cox proportional hazard regression models","text":"Let’s imagine survival outcome modeling population time \\(t\\), interested set input variables \\(x_1, x_2, \\dots, x_p\\) influences survival outcome. Given survival outcome binary variable, can model survival time \\(t\\) binary logistic regression. define \\(h(t)\\) proportion survived time \\(t\\), called hazard function, based work Chapter 5:\\[\nh(t) = h_0(t)e^{\\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_px_p}\n\\]\n\\(h_0(t)\\) base intercept hazard time \\(t\\), \\(\\beta_i\\) coefficient associated \\(x_i\\) .Now let’s imagine comparing hazard two different individuals \\(\\) \\(B\\) population. make assumption hazard curves \\(h^(t)\\) individual \\(\\) \\(h^B(t)\\) individual \\(B\\) always proportional never cross—called proportional hazard assumption. assumption, can conclude \\[\n\\begin{aligned}\n\\frac{h^B(t)}{h^(t)} &= \\frac{h_0(t)e^{\\beta_1x_1^B + \\beta_2x_2^B + \\dots + \\beta_px_p^B}}{h_0(t)e^{\\beta_1x_1^+ \\beta_2x_2^+ \\dots + \\beta_px_p^}} \\\\\n&= e^{\\beta_1(x_1^B-x_1^) + \\beta_2(x_2^B-x_2^) + \\dots \\beta_p(x_p^B-x_p^)}\n\\end{aligned}\n\\]Note \\(t\\) final equation. important observation hazard person B relative person constant independent time. allows us take complicating factor model. means can model effect input variables hazard without needing account changes times, making model similar interpretation standard binomial regression model.","code":""},{"path":"survival.html","id":"running-a-cox-proportional-hazard-regression-model","chapter":"9 Survival Analysis for Modeling Singular Events Over Time","heading":"9.2.1 Running a Cox proportional hazard regression model","text":"Cox proportional hazard model can run using coxph() function survival package, outcome survival object. Let’s model survival input variables gender, field, level sentiment.model returns following41Coefficients input variable p-values. can conclude working Finance Health associated significantly greater likelihood leaving period studied, higher sentiment associated significantly lower likelihood leaving.Coefficients input variable p-values. can conclude working Finance Health associated significantly greater likelihood leaving period studied, higher sentiment associated significantly lower likelihood leaving.Relative odds ratios associated input variable. example, single extra point sentiment reduces odds leaving ~11%. single less point increases odds leaving ~12%. Confidence intervals coefficients also provided.Relative odds ratios associated input variable. example, single extra point sentiment reduces odds leaving ~11%. single less point increases odds leaving ~12%. Confidence intervals coefficients also provided.Three statistical tests null hypothesis coefficients zero. null hypothesis rejected three tests can interpreted meaning model significant.Three statistical tests null hypothesis coefficients zero. null hypothesis rejected three tests can interpreted meaning model significant.Importantly, well statistically validating sentiment significant effect retention, Cox model allowed us control possible mediating variables. can now say sentiment significant effect retention even individuals gender, field level.","code":"\n# run cox model against survival outcome\ncox_model <- survival::coxph(\n  formula = Surv(event = left, time = month) ~ gender + \n    field + level + sentiment,\n  data = job_retention\n)\n\nsummary(cox_model)## Call:\n## survival::coxph(formula = Surv(event = left, time = month) ~ \n##     gender + field + level + sentiment, data = job_retention)\n## \n##   n= 3770, number of events= 1354 \n## \n##                            coef exp(coef) se(coef)      z Pr(>|z|)    \n## genderM                -0.04548   0.95553  0.05886 -0.773 0.439647    \n## fieldFinance            0.22334   1.25025  0.06681  3.343 0.000829 ***\n## fieldHealth             0.27830   1.32089  0.12890  2.159 0.030849 *  \n## fieldLaw                0.10532   1.11107  0.14515  0.726 0.468086    \n## fieldPublic/Government  0.11499   1.12186  0.08899  1.292 0.196277    \n## fieldSales/Marketing    0.08776   1.09173  0.10211  0.859 0.390082    \n## levelLow                0.14813   1.15967  0.09000  1.646 0.099799 .  \n## levelMedium             0.17666   1.19323  0.10203  1.732 0.083362 .  \n## sentiment              -0.11756   0.88909  0.01397 -8.415  < 2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n##                        exp(coef) exp(-coef) lower .95 upper .95\n## genderM                   0.9555     1.0465    0.8514    1.0724\n## fieldFinance              1.2502     0.7998    1.0968    1.4252\n## fieldHealth               1.3209     0.7571    1.0260    1.7005\n## fieldLaw                  1.1111     0.9000    0.8360    1.4767\n## fieldPublic/Government    1.1219     0.8914    0.9423    1.3356\n## fieldSales/Marketing      1.0917     0.9160    0.8937    1.3336\n## levelLow                  1.1597     0.8623    0.9721    1.3834\n## levelMedium               1.1932     0.8381    0.9770    1.4574\n## sentiment                 0.8891     1.1248    0.8651    0.9138\n## \n## Concordance= 0.578  (se = 0.008 )\n## Likelihood ratio test= 89.18  on 9 df,   p=2e-15\n## Wald test            = 94.95  on 9 df,   p=<2e-16\n## Score (logrank) test = 95.31  on 9 df,   p=<2e-16"},{"path":"survival.html","id":"checking-the-proportional-hazard-assumption","chapter":"9 Survival Analysis for Modeling Singular Events Over Time","heading":"9.2.2 Checking the proportional hazard assumption","text":"Note mentioned previous section critical assumption Cox proportional hazard model valid, called proportional hazard assumption. always, important check assumption finalizing inferences conclusions model.popular test assumption uses residual known Schoenfeld residual, expected independent time proportional hazard assumption holds. cox.zph() function survival package runs statistical test null hypothesis Schoenfeld residuals independent time. test conducted every input variable model whole, significant result reject proportional hazard assumption.case, can confirm proportional hazard assumption rejected. ggcoxzph() function survminer package takes result cox.zph() check allows graphical check plotting residuals time, seen Figure 9.2.\nFigure 9.2: Schoenfeld test proportional hazard assumption cox_model\n","code":"\n(ph_check <- survival::cox.zph(cox_model))##            chisq df    p\n## gender     0.726  1 0.39\n## field      6.656  5 0.25\n## level      2.135  2 0.34\n## sentiment  1.828  1 0.18\n## GLOBAL    11.156  9 0.27\nsurvminer::ggcoxzph(ph_check, \n                    font.main = 10, \n                    font.x = 10, \n                    font.y = 10)"},{"path":"survival.html","id":"frailty-models","chapter":"9 Survival Analysis for Modeling Singular Events Over Time","heading":"9.3 Frailty models","text":"noticed example previous section certain fields employment appeared significant effect attrition hazard. therefore possible different fields employment different base hazard functions, may wish take account determining input variables significant relationship attrition. analogous mixed model looked Section 8.1.case apply random intercept effect base hazard function \\(h_0(t)\\) according field employment individual, order take account modeling. kind model called frailty model, taken clinical context, different groups patients may different frailties (background risks death).many variants frailty models run clinical context (see Collett (2015) excellent exposition ), main application frailty model people analytics adapt Cox proportional hazard model take account different background risks hazard event occurring among different groups data. called shared frailty model. frailtypack R package allows various frailty models run relative ease. run shared frailty model job_retention data take account different background attrition risk different fields employment.can see frailty parameter significant, indicating sufficient difference background attrition risk justify application random hazard effect. also see level employment now becomes significant addition sentiment, Low Medium level employees likely leave compared High level employees.frailtyPenal() function can also useful way observe different baseline survivals groups data. example, simple stratified Cox proportional hazard model based sentiment category can constructed42.can plotted observe baseline retention differs group, Figure 9.343.\nFigure 9.3: Baseline retention curves two sentiment categories job_retention data set\n","code":"\nlibrary(frailtypack)\n\n(frailty_model <- frailtypack::frailtyPenal(\n  formula = Surv(event = left, time = month) ~ gender + \n    level + sentiment + cluster(field),\n  data = job_retention,\n  n.knots = 12, \n  kappa = 10000\n))## \n## Be patient. The program is computing ... \n## The program took 1.69 seconds## Call:\n## frailtypack::frailtyPenal(formula = Surv(event = left, time = month) ~ \n##     gender + level + sentiment + cluster(field), data = job_retention, \n##     n.knots = 12, kappa = 10000)\n## \n## \n##   Shared Gamma Frailty model parameter estimates  \n##   using a Penalized Likelihood on the hazard function \n## \n##                  coef exp(coef) SE coef (H) SE coef (HIH)         z          p\n## genderM     -0.029531  0.970901   0.0591820     0.0591820 -0.498986 6.1779e-01\n## levelLow     0.198548  1.219630   0.0917396     0.0917396  2.164255 3.0445e-02\n## levelMedium  0.223266  1.250154   0.1035510     0.1035510  2.156101 3.1076e-02\n## sentiment   -0.108262  0.897392   0.0141325     0.0141325 -7.660518 1.8541e-14\n## \n##         chisq df global p\n## level 5.28624  2   0.0711\n## \n##     Frailty parameter, Theta: 48.3209 (SE (H): 25.5895 ) p = 0.029492 \n##  \n##       penalized marginal log-likelihood = -5510.36\n##       Convergence criteria: \n##       parameters = 3.05e-05 likelihood = 4.91e-06 gradient = 1.55e-09 \n## \n##       LCV = the approximate likelihood cross-validation criterion\n##             in the semi parametrical case     = 1.46587 \n## \n##       n= 3770\n##       n events= 1354  n groups= 6\n##       number of iterations:  18 \n## \n##       Exact number of knots used:  12 \n##       Value of the smoothing parameter:  10000, DoF:  6.31\nstratified_base <- frailtypack::frailtyPenal(\n  formula = Surv(event = left, time = month) ~ \n    strata(sentiment_category),\n  data = job_retention,\n  n.knots = 12,\n  kappa = rep(10000, 2)\n)\nplot(stratified_base, type.plot = \"Survival\", \n     pos.legend = \"topright\", Xlab = \"Month\",\n     Ylab = \"Baseline retention rate\",\n     color = 1)"},{"path":"survival.html","id":"learning-exercises-7","chapter":"9 Survival Analysis for Modeling Singular Events Over Time","heading":"9.4 Learning exercises","text":"","code":""},{"path":"survival.html","id":"discussion-questions-7","chapter":"9 Survival Analysis for Modeling Singular Events Over Time","heading":"9.4.1 Discussion questions","text":"Describe reasons survival analysis useful tool analyzing data outcome events happen different times.Describe Kaplan-Meier survival estimate calculated.common uses survival curves practice?important run Cox proportional hazard model addition calculating survival estimates trying understand effect given variable survival?Describe assumption underlies Cox proportional hazard model assumption can checked.frailty model, might useful context survival analysis?","code":""},{"path":"survival.html","id":"data-exercises-7","chapter":"9 Survival Analysis for Modeling Singular Events Over Time","heading":"9.4.2 Data exercises","text":"exercises, use job_retention data set walkthrough example chapter, can loaded via peopleanalyticsdata package downloaded internet44. intention field represents score 1 10 individual’s intention leave job next 12 months, 1 indicates extremely low intention 10 indicates extremely high intention. response recorded beginning study period.Create three categories intention follows: High (score 7 higher), Moderate (score 4–6), Low (score 3 less)Calculate Kaplan-Meier survival estimates three categories visualize using survival curves.Determine effect intention retention using Cox proportional hazard model, controlling gender, field level.Perform appropriate check proportional hazard assumption holds model.Run similar model, time include sentiment input variable. interpret results?Experiment running frailty model take account different background attrition risk field employment.","code":""},{"path":"alt-approaches.html","id":"alt-approaches","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10 Alternative Technical Approaches in R, Python and Julia","text":"outlined earlier book, technical implementations modeling techniques previous chapters relied wherever possible base R code specialist packages specific methodologies—allowed focus basics understanding, running interpreting models key aim book. interested wider range technical options running inferential statistical models, chapter illustrates alternative options considered starting point interested rather -depth exposition.First look options generating models predictable formats R. seen prior chapters output many models R can inconsistent. many cases given information need, cases less need. Formats can vary, sometimes need look different parts output see specific statistics seek. tidymodels set packages tries bring principles tidy data realm statistical modeling illustrate briefly.Second, whose preference use Python, provide examples inferential regression models can run Python. Python particularly well-tooled running predictive models, full range statistical inference tools available R. particular, using predictive modeling machine learning packages like scikit-learn conduct regression modeling can often leave analyst lacking seeking information certain model statistics statistics typically sought predictive modeling workflow. briefly illustrate Python packages perform modeling greater emphasis inference versus prediction.Finally, show examples regression models Julia, new increasingly popular open source programming language. Julia considerably younger language R Python, uptake rapid. Already many common models can easily implemented Julia, advanced models still awaiting development packages implement .","code":""},{"path":"alt-approaches.html","id":"tidier-modeling-approaches-in-r","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.1 ‘Tidier’ modeling approaches in R","text":"tidymodels meta-package collection packages collectively apply principles tidy data construction statistical models. information learning resources tidymodels can found . Within tidymodels two packages particularly useful controlling output models R: broom parsnip packages.","code":""},{"path":"alt-approaches.html","id":"the-broom-package","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.1.1 The broom package","text":"Consistent named, broom aims tidy output models predictable format. works 100 different types models R. order illustrate use, let’s run model previous chapter—specifically salesperson promotion model Chapter 5.Chapter 5, convert promoted column factor run binomial logistic regression model promoted outcome.now model sitting memory. can use three key functions broom package view variety model statistics. First, tidy() function allows us see coefficient statistics model.glance() function allows us see row overall model statistics:augment() function augments observations data set range observation-level model statistics residuals:functions model-agnostic wide range common models R. example, can use proportional odds model soccer discipline Chapter 7, generate relevant statistics tidy tables.broom functions integrate well tidyverse methods, allow easy running models nested subsets data. example, want run soccer discipline model across different countries data set see model statistics neat table, can use typical tidyverse grammar using dplyr.similar way, putting model formulas dataframe column, numerous models can run single command results viewed tidy dataframe.","code":"\n# obtain salespeople data\nurl <- \"http://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople <- read.csv(url)\n# convert promoted to factor\nsalespeople$promoted <- as.factor(salespeople$promoted)\n\n# build model to predict promotion based on sales and customer_rate\npromotion_model <- glm(formula = promoted ~ sales + customer_rate,\n                       family = \"binomial\",\n                       data = salespeople)\n# load tidymodels metapackage\nlibrary(tidymodels)\n\n# view coefficient statistics\nbroom::tidy(promotion_model)## # A tibble: 3 x 5\n##   term          estimate std.error statistic  p.value\n##   <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   -19.5      3.35        -5.83 5.48e- 9\n## 2 sales           0.0404   0.00653      6.19 6.03e-10\n## 3 customer_rate  -1.12     0.467       -2.40 1.63e- 2\n# view model statistics\nbroom::glance(promotion_model)## # A tibble: 1 x 8\n##   null.deviance df.null logLik   AIC   BIC deviance df.residual  nobs\n##           <dbl>   <int>  <dbl> <dbl> <dbl>    <dbl>       <int> <int>\n## 1          440.     349  -32.6  71.1  82.7     65.1         347   350\n# view augmented data\nhead(broom::augment(promotion_model))## # A tibble: 6 x 9\n##   promoted sales customer_rate .fitted   .resid .std.resid      .hat .sigma  .cooksd\n##   <fct>    <int>         <dbl>   <dbl>    <dbl>      <dbl>     <dbl>  <dbl>    <dbl>\n## 1 0          594          3.94  0.0522 -1.20      -1.22    0.0289     0.429 1.08e- 2\n## 2 0          446          4.06 -6.06   -0.0683    -0.0684  0.00212    0.434 1.66e- 6\n## 3 1          674          3.83  3.41    0.255      0.257   0.0161     0.434 1.84e- 4\n## 4 0          525          3.62 -2.38   -0.422     -0.425   0.0153     0.433 4.90e- 4\n## 5 1          657          4.4   2.08    0.485      0.493   0.0315     0.433 1.40e- 3\n## 6 1          918          4.54 12.5     0.00278    0.00278 0.0000174  0.434 2.24e-11\n# get soccer data\nurl <- \"http://peopleanalytics-regression-book.org/data/soccer.csv\"\nsoccer <- read.csv(url)\n\n# convert discipline to ordered factor\nsoccer$discipline <- ordered(soccer$discipline, \n                             levels = c(\"None\", \"Yellow\", \"Red\"))\n\n# run proportional odds model\nlibrary(MASS)\nsoccer_model <- polr(\n  formula = discipline ~ n_yellow_25 + n_red_25 + position + \n    country + level + result, \n  data = soccer\n)\n\n# view model statistics\nbroom::glance(soccer_model)## # A tibble: 1 x 7\n##     edf logLik   AIC   BIC deviance df.residual  nobs\n##   <int>  <dbl> <dbl> <dbl>    <dbl>       <dbl> <dbl>\n## 1    10 -1722. 3465. 3522.    3445.        2281  2291\n# load the tidyverse metapackage (includes dplyr)\nlibrary(tidyverse)\n\n# define function to run soccer model and glance at results\nsoccer_model_glance <- function(form, df) {\n  model <- polr(formula = form, data = df)\n  broom::glance(model)\n}\n\n# run it nested by country\nsoccer %>% \n  dplyr::nest_by(country) %>% \n  dplyr::summarise(\n    soccer_model_glance(\"discipline ~ n_yellow_25 + n_red_25\", data)\n  )## # A tibble: 2 x 8\n## # Groups:   country [2]\n##   country   edf logLik   AIC   BIC deviance df.residual  nobs\n##   <chr>   <int>  <dbl> <dbl> <dbl>    <dbl>       <dbl> <dbl>\n## 1 England     4  -883. 1773. 1794.    1765.        1128  1132\n## 2 Germany     4  -926. 1861. 1881.    1853.        1155  1159\n# create model formula column\nformula <- c(\n  \"discipline ~ n_yellow_25\", \n  \"discipline ~ n_yellow_25 + n_red_25\",\n  \"discipline ~ n_yellow_25 + n_red_25 + position\"\n)\n\n# create dataframe\nmodels <- data.frame(formula)\n\n# run models and glance at results\nmodels %>% \n  dplyr::group_by(formula) %>% \n  dplyr::summarise(soccer_model_glance(formula, soccer))## # A tibble: 3 x 8\n##   formula                                          edf logLik   AIC   BIC deviance df.residual  nobs\n##   <chr>                                          <int>  <dbl> <dbl> <dbl>    <dbl>       <dbl> <dbl>\n## 1 discipline ~ n_yellow_25                           3 -1861. 3728. 3745.    3722.        2288  2291\n## 2 discipline ~ n_yellow_25 + n_red_25                4 -1809. 3627. 3650.    3619.        2287  2291\n## 3 discipline ~ n_yellow_25 + n_red_25 + position     6 -1783. 3579. 3613.    3567.        2285  2291"},{"path":"alt-approaches.html","id":"the-parsnip-package","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.1.2 The parsnip package","text":"parsnip package aims create unified interface running models, avoid users needing understand different model terminology minutiae. also takes hierarchical approach defining models similar nature object-oriented approaches Python users familiar .let’s use salesperson promotion model example illustrate. start defining model family wish use, case logistic regression, define specific engine mode.can use translate() function see kind model created:Now model defined, can fit using formula data use broom view coefficients:parsnip functions particularly motivated around tooling machine learning model workflows similar way scikit-learn Python, can offer attractive approach coding inferential models, particularly common families models used.","code":"\nmodel <- parsnip::logistic_reg() %>% \n  parsnip::set_engine(\"glm\") %>% \n  parsnip::set_mode(\"classification\")\nmodel %>% \n  parsnip::translate()## Logistic Regression Model Specification (classification)\n## \n## Computational engine: glm \n## \n## Model fit template:\n## stats::glm(formula = missing_arg(), data = missing_arg(), weights = missing_arg(), \n##     family = stats::binomial)\nmodel %>% \n  parsnip::fit(formula = promoted ~ sales + customer_rate,\n               data = salespeople) %>% \n  broom::tidy()## # A tibble: 3 x 5\n##   term          estimate std.error statistic  p.value\n##   <chr>            <dbl>     <dbl>     <dbl>    <dbl>\n## 1 (Intercept)   -19.5      3.35        -5.83 5.48e- 9\n## 2 sales           0.0404   0.00653      6.19 6.03e-10\n## 3 customer_rate  -1.12     0.467       -2.40 1.63e- 2"},{"path":"alt-approaches.html","id":"inferential-statistical-modeling-in-python","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.2 Inferential statistical modeling in Python","text":"general, modeling functions contained scikit-learn—tends go-modeling package Python users—oriented towards predictive modeling can challenging navigate primarily interested inferential modeling. section briefly review approaches running models contained book Python. statsmodels package highly recommended offers wide range models report similar statistics reviewed book. Full statsmodels documentation can found .","code":""},{"path":"alt-approaches.html","id":"ordinary-least-squares-ols-linear-regression","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.2.1 Ordinary Least Squares (OLS) linear regression","text":"OLS linear regression model reviewed Chapter 4 can generated using statsmodels package, can report reasonably thorough set model statistics. using statsmodels formula API, model formulas similar used R can used.","code":"import pandas as pd\nimport statsmodels.formula.api as smf\n\n# get data\nurl = \"http://peopleanalytics-regression-book.org/data/ugtests.csv\"\nugtests = pd.read_csv(url)\n\n# define model\nmodel = smf.ols(formula = \"Final ~ Yr3 + Yr2 + Yr1\", data = ugtests)\n\n# fit model\nugtests_model = model.fit()\n\n# see results summary\nprint(ugtests_model.summary())##                             OLS Regression Results                            \n## ==============================================================================\n## Dep. Variable:                  Final   R-squared:                       0.530\n## Model:                            OLS   Adj. R-squared:                  0.529\n## Method:                 Least Squares   F-statistic:                     365.5\n## Date:                Fri, 19 Aug 2022   Prob (F-statistic):          8.22e-159\n## Time:                        12:47:59   Log-Likelihood:                -4711.6\n## No. Observations:                 975   AIC:                             9431.\n## Df Residuals:                     971   BIC:                             9451.\n## Df Model:                           3                                         \n## Covariance Type:            nonrobust                                         \n## ==============================================================================\n##                  coef    std err          t      P>|t|      [0.025      0.975]\n## ------------------------------------------------------------------------------\n## Intercept     14.1460      5.480      2.581      0.010       3.392      24.900\n## Yr3            0.8657      0.029     29.710      0.000       0.809       0.923\n## Yr2            0.4313      0.033     13.267      0.000       0.367       0.495\n## Yr1            0.0760      0.065      1.163      0.245      -0.052       0.204\n## ==============================================================================\n## Omnibus:                        0.762   Durbin-Watson:                   2.006\n## Prob(Omnibus):                  0.683   Jarque-Bera (JB):                0.795\n## Skew:                           0.067   Prob(JB):                        0.672\n## Kurtosis:                       2.961   Cond. No.                         858.\n## ==============================================================================\n## \n## Notes:\n## [1] Standard Errors assume that the covariance matrix of the errors is correctly specified."},{"path":"alt-approaches.html","id":"binomial-logistic-regression","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.2.2 Binomial logistic regression","text":"Binomial logistic regression models can generated similar way OLS linear regression models using statsmodels formula API, calling binomial family general statsmodels API.","code":"import pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# obtain salespeople data\nurl = \"http://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople = pd.read_csv(url)\n\n# define model\nmodel = smf.glm(formula = \"promoted ~ sales + customer_rate\", \n                data = salespeople, \n                family = sm.families.Binomial())\n\n\n# fit model\npromotion_model = model.fit()\n\n\n# see results summary\nprint(promotion_model.summary())##                  Generalized Linear Model Regression Results                  \n## ==============================================================================\n## Dep. Variable:               promoted   No. Observations:                  350\n## Model:                            GLM   Df Residuals:                      347\n## Model Family:                Binomial   Df Model:                            2\n## Link Function:                  logit   Scale:                          1.0000\n## Method:                          IRLS   Log-Likelihood:                -32.566\n## Date:                Fri, 19 Aug 2022   Deviance:                       65.131\n## Time:                        12:48:01   Pearson chi2:                     198.\n## No. Iterations:                     9                                         \n## Covariance Type:            nonrobust                                         \n## =================================================================================\n##                     coef    std err          z      P>|z|      [0.025      0.975]\n## ---------------------------------------------------------------------------------\n## Intercept       -19.5177      3.347     -5.831      0.000     -26.078     -12.958\n## sales             0.0404      0.007      6.189      0.000       0.028       0.053\n## customer_rate    -1.1221      0.467     -2.403      0.016      -2.037      -0.207\n## ================================================================================="},{"path":"alt-approaches.html","id":"multinomial-logistic-regression","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.2.3 Multinomial logistic regression","text":"Multinomial logistic regression similarly available using statsmodels formula API. usual, care must taken ensure reference category appropriately defined, dummy input variables need explicitly constructed, constant term must added ensure intercept calculated.","code":"import pandas as pd\nimport statsmodels.api as sm\n\n# load health insurance data\nurl = \"http://peopleanalytics-regression-book.org/data/health_insurance.csv\"\nhealth_insurance = pd.read_csv(url)\n\n# convert product to categorical as an outcome variable\ny = pd.Categorical(health_insurance['product'])\n\n# create dummies for gender\nX1 = pd.get_dummies(health_insurance['gender'], drop_first = True)\n\n# replace back into input variables \nX2 = health_insurance.drop(['product', 'gender'], axis = 1)\nX = pd.concat([X1, X2], axis = 1)\n\n# add a constant term to ensure intercept is calculated\nXc = sm.add_constant(X)\n\n# define model\nmodel = sm.MNLogit(y, Xc)\n\n# fit model\ninsurance_model = model.fit()\n\n# see results summary\nprint(insurance_model.summary())##                           MNLogit Regression Results                          \n## ==============================================================================\n## Dep. Variable:                      y   No. Observations:                 1453\n## Model:                        MNLogit   Df Residuals:                     1439\n## Method:                           MLE   Df Model:                           12\n## Date:                Fri, 19 Aug 2022   Pseudo R-squ.:                  0.5332\n## Time:                        12:48:02   Log-Likelihood:                -744.68\n## converged:                       True   LL-Null:                       -1595.3\n## Covariance Type:            nonrobust   LLR p-value:                     0.000\n## ==================================================================================\n##            y=B       coef    std err          z      P>|z|      [0.025      0.975]\n## ----------------------------------------------------------------------------------\n## const             -4.6010      0.511     -9.012      0.000      -5.602      -3.600\n## Male              -2.3826      0.232    -10.251      0.000      -2.838      -1.927\n## Non-binary         0.2528      1.226      0.206      0.837      -2.151       2.656\n## age                0.2437      0.015     15.790      0.000       0.213       0.274\n## household         -0.9677      0.069    -13.938      0.000      -1.104      -0.832\n## position_level    -0.4153      0.089     -4.658      0.000      -0.590      -0.241\n## absent             0.0117      0.013      0.900      0.368      -0.014       0.037\n## ----------------------------------------------------------------------------------\n##            y=C       coef    std err          z      P>|z|      [0.025      0.975]\n## ----------------------------------------------------------------------------------\n## const            -10.2261      0.620    -16.501      0.000     -11.441      -9.011\n## Male               0.0967      0.195      0.495      0.621      -0.286       0.480\n## Non-binary        -1.2698      2.036     -0.624      0.533      -5.261       2.721\n## age                0.2698      0.016     17.218      0.000       0.239       0.301\n## household          0.2043      0.050      4.119      0.000       0.107       0.302\n## position_level    -0.2136      0.082     -2.597      0.009      -0.375      -0.052\n## absent             0.0033      0.012      0.263      0.793      -0.021       0.028\n## =================================================================================="},{"path":"alt-approaches.html","id":"structural-equation-models","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.2.4 Structural equation models","text":"semopy package specialized package implementation Structural Equation Models Python, implementation similar lavaan package R. However, reporting intuitive compared lavaan. full tutorial available . example run model studied Section 8.2 using semopy.inspect results:","code":"import pandas as pd\nfrom semopy import Model\n\n\n# get data\nurl = \"http://peopleanalytics-regression-book.org/data/politics_survey.csv\"\npolitics_survey = pd.read_csv(url)\n\n\n# define full measurement and structural model\nmeasurement_model = \"\"\"\n# measurement model\nPol =~ Pol1 + Pol2\nHab =~ Hab1 + Hab2 + Hab3\nLoc =~ Loc2 + Loc3\nEnv =~ Env1 + Env2\nInt =~ Int1 + Int2\nPers =~ Pers2 + Pers3\nNat =~ Nat1 + Nat2\nEco =~ Eco1 + Eco2\n\n# structural model\nOverall ~ Pol + Hab + Loc + Env + Int + Pers + Nat + Eco\n\"\"\"\n\nfull_model = Model(measurement_model)\n\n\n# fit model to data and inspect\nfull_model.fit(politics_survey)# inspect the results of SEM (first few rows)\nfull_model.inspect().head()##    lval op rval  Estimate   Std. Err  z-value p-value\n## 0  Pol1  ~  Pol  1.000000          -        -       -\n## 1  Pol2  ~  Pol  0.713719  0.0285052  25.0382       0\n## 2  Hab1  ~  Hab  1.000000          -        -       -\n## 3  Hab2  ~  Hab  1.183981  0.0306792  38.5923       0\n## 4  Hab3  ~  Hab  1.127639  0.0304292  37.0578       0"},{"path":"alt-approaches.html","id":"survival-analysis","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.2.5 Survival analysis","text":"lifelines package Python designed support survival analysis, functions calculate survival estimates, plot survival curves, perform Cox proportional hazard regression check proportional hazard assumptions. full tutorial available .example plot Kaplan-Meier survival curves Python using Chapter 9 walkthrough example. survival curves displayed Figure 10.1.\nFigure 10.1: Survival curves sentiment category job retention data\nexample fit Cox Proportional Hazard model similarly Section 9.245.Proportional Hazard assumptions can checked using check_assumptions() method46.","code":"import pandas as pd\nfrom lifelines import KaplanMeierFitter\nfrom matplotlib import pyplot as plt\n\n# get data\nurl = \"http://peopleanalytics-regression-book.org/data/job_retention.csv\"\njob_retention = pd.read_csv(url)\n\n# fit our data to Kaplan-Meier estimates\nT = job_retention[\"month\"]\nE = job_retention[\"left\"]\nkmf = KaplanMeierFitter()\nkmf.fit(T, event_observed = E)\n\n# split into high and not high sentiment\nhighsent = (job_retention[\"sentiment\"] >= 7)\n\n\n# set up plot\nsurvplot = plt.subplot()\n\n# plot high sentiment survival function\nkmf.fit(T[highsent], event_observed = E[highsent], \nlabel = \"High Sentiment\")\n\nkmf.plot_survival_function(ax = survplot)\n\n# plot not high sentiment survival function\nkmf.fit(T[~highsent], event_observed = E[~highsent], \nlabel = \"Not High Sentiment\")\n\nkmf.plot_survival_function(ax = survplot)\n\n# show survival curves by sentiment category\nplt.show()from lifelines import CoxPHFitter\n\n# fit Cox PH model to job_retention data\ncph = CoxPHFitter()\ncph.fit(job_retention, duration_col = 'month', event_col = 'left', \n        formula = \"gender + field + level + sentiment\")# view results\ncph.print_summary()## <lifelines.CoxPHFitter: fitted with 3770 total observations, 2416 right-censored observations>\n##              duration col = 'month'\n##                 event col = 'left'\n##       baseline estimation = breslow\n##    number of observations = 3770\n## number of events observed = 1354\n##    partial log-likelihood = -10724.52\n##          time fit was run = 2022-08-19 12:48:11 UTC\n## \n## ---\n##                              coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%\n## covariate                                                                                                                          \n## gender[T.M]                 -0.05       0.96       0.06            -0.16             0.07                 0.85                 1.07\n## field[T.Finance]             0.22       1.25       0.07             0.09             0.35                 1.10                 1.43\n## field[T.Health]              0.28       1.32       0.13             0.03             0.53                 1.03                 1.70\n## field[T.Law]                 0.11       1.11       0.15            -0.18             0.39                 0.84                 1.48\n## field[T.Public/Government]   0.11       1.12       0.09            -0.06             0.29                 0.94                 1.34\n## field[T.Sales/Marketing]     0.09       1.09       0.10            -0.11             0.29                 0.89                 1.33\n## level[T.Low]                 0.15       1.16       0.09            -0.03             0.32                 0.97                 1.38\n## level[T.Medium]              0.18       1.19       0.10            -0.02             0.38                 0.98                 1.46\n## sentiment                   -0.12       0.89       0.01            -0.14            -0.09                 0.87                 0.91\n## \n##                                z      p   -log2(p)\n## covariate                                         \n## gender[T.M]                -0.77   0.44       1.19\n## field[T.Finance]            3.34 <0.005      10.24\n## field[T.Health]             2.16   0.03       5.02\n## field[T.Law]                0.73   0.47       1.10\n## field[T.Public/Government]  1.29   0.20       2.35\n## field[T.Sales/Marketing]    0.86   0.39       1.36\n## level[T.Low]                1.65   0.10       3.32\n## level[T.Medium]             1.73   0.08       3.58\n## sentiment                  -8.41 <0.005      54.49\n## ---\n## Concordance = 0.58\n## Partial AIC = 21467.04\n## log-likelihood ratio test = 89.18 on 9 df\n## -log2(p) of ll-ratio test = 48.58cph.check_assumptions(job_retention, p_value_threshold = 0.05)## The ``p_value_threshold`` is set at 0.05. Even under the null hypothesis of no violations, some\n## covariates will be below the threshold by chance. This is compounded when there are many covariates.\n## Similarly, when there are lots of observations, even minor deviances from the proportional hazard\n## assumption will be flagged.\n## \n## With that in mind, it's best to use a combination of statistical tests and visual tests to determine\n## the most serious violations. Produce visual plots using ``check_assumptions(..., show_plots=True)``\n## and looking for non-constant lines. See link [A] below for a full example.\n## \n## <lifelines.StatisticalResult: proportional_hazard_test>\n##  null_distribution = chi squared\n## degrees_of_freedom = 1\n##              model = <lifelines.CoxPHFitter: fitted with 3770 total observations, 2416 right-censored observations>\n##          test_name = proportional_hazard_test\n## \n## ---\n##                                  test_statistic    p  -log2(p)\n## field[T.Finance]           km              1.20 0.27      1.88\n##                            rank            1.09 0.30      1.76\n## field[T.Health]            km              4.27 0.04      4.69\n##                            rank            4.10 0.04      4.54\n## field[T.Law]               km              1.14 0.29      1.81\n##                            rank            0.85 0.36      1.49\n## field[T.Public/Government] km              1.92 0.17      2.59\n##                            rank            1.87 0.17      2.54\n## field[T.Sales/Marketing]   km              2.00 0.16      2.67\n##                            rank            2.22 0.14      2.88\n## gender[T.M]                km              0.41 0.52      0.94\n##                            rank            0.39 0.53      0.91\n## level[T.Low]               km              1.53 0.22      2.21\n##                            rank            1.52 0.22      2.20\n## level[T.Medium]            km              0.09 0.77      0.38\n##                            rank            0.13 0.72      0.47\n## sentiment                  km              2.78 0.10      3.39\n##                            rank            2.32 0.13      2.97\n## \n## \n## 1. Variable 'field[T.Health]' failed the non-proportional test: p-value is 0.0387.\n## \n##    Advice: with so few unique values (only 2), you can include `strata=['field[T.Health]', ...]` in\n## the call in `.fit`. See documentation in link [E] below.\n## \n## ---\n## [A]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html\n## [B]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Bin-variable-and-stratify-on-it\n## [C]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Introduce-time-varying-covariates\n## [D]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Modify-the-functional-form\n## [E]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Stratification\n## \n## []"},{"path":"alt-approaches.html","id":"other-model-variants","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.2.6 Other model variants","text":"Implementation model variants featured earlier chapters becomes thinner Python. However, note following:Ordinal regression currently available release version statsmodels package available development version. mord package offers implementation ordinal regression predictive analytics purposes, inferential modeling users need wait release statsmodels contains ordinal regression methods immediate use need install development version source.Ordinal regression currently available release version statsmodels package available development version. mord package offers implementation ordinal regression predictive analytics purposes, inferential modeling users need wait release statsmodels contains ordinal regression methods immediate use need install development version source.Mixed models currently implementation linear mixed modeling statsmodels. Generalized linear mixed models equivalent found lme4 R package yet available Python.Mixed models currently implementation linear mixed modeling statsmodels. Generalized linear mixed models equivalent found lme4 R package yet available Python.","code":""},{"path":"alt-approaches.html","id":"inferential-statistical-modeling-in-julia","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.3 Inferential statistical modeling in Julia","text":"GLM package Julia offers functions variety elementary regression models. package contains implementation linear regression well binomial logistic regression.","code":""},{"path":"alt-approaches.html","id":"ordinary-least-squares-ols-linear-regression-1","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.3.1 Ordinary Least Squares (OLS) linear regression","text":"run OLS linear model ugtests data set Chapter 4:Summary statistics \\(R^2\\) adjusted \\(R^2\\) can obtained using appropriate functions:","code":"using DataFrames, CSV, GLM;\n\n# get data\nurl = \"http://peopleanalytics-regression-book.org/data/ugtests.csv\";\nugtests = CSV.read(download(url), DataFrame);\n\n# run linear model\nols_model = lm(@formula(Final ~ Yr1 + Yr2 + Yr3), ugtests)## StatsModels.TableRegressionModel{LinearModel{GLM.LmResp{Vector{Float64}}, GLM.DensePredChol{Float64, CholeskyPivoted{Float64, Matrix{Float64}}}}, Matrix{Float64}}\n## \n## Final ~ 1 + Yr1 + Yr2 + Yr3\n## \n## Coefficients:\n## ───────────────────────────────────────────────────────────────────────────\n##                   Coef.  Std. Error      t  Pr(>|t|)   Lower 95%  Upper 95%\n## ───────────────────────────────────────────────────────────────────────────\n## (Intercept)  14.146       5.48006     2.58    0.0100   3.39187    24.9001\n## Yr1           0.0760262   0.0653816   1.16    0.2452  -0.0522794   0.204332\n## Yr2           0.431285    0.0325078  13.27    <1e-36   0.367492    0.495079\n## Yr3           0.865681    0.0291375  29.71    <1e-99   0.808501    0.922861\n## ───────────────────────────────────────────────────────────────────────────# r-squared\nr2(ols_model)## 0.530327461953525# adjusted r-squared\nadjr2(ols_model)## 0.5288763624538964"},{"path":"alt-approaches.html","id":"binomial-logistic-regression-1","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.3.2 Binomial logistic regression","text":"run binomial logistic regression salespeople data set Chapter 5. Note categorical inputs need explicitly converted running model.AIC, log likelihood summary statistics model can obtained using appropriate functions. example:","code":"using DataFrames, CSV, GLM, Missings, CategoricalArrays;\n\n# get salespeople data\nurl = \"http://peopleanalytics-regression-book.org/data/salespeople.csv\";\nsalespeople = CSV.read(download(url), DataFrame, missingstrings=[\"NA\"]);\n\n# remove missing value rows from dataset\nsalespeople = salespeople[completecases(salespeople), :];\n\n# ensure no missing data structures\nsalespeople = mapcols(col -> disallowmissing(col), salespeople);\n\n# map categorical input columns\nsalespeople.performance = CategoricalArray(salespeople.performance);\n\n# run binomial logistic regression model\npromotion_model = glm(@formula(promoted ~ sales + customer_rate + performance), salespeople, Binomial(), LogitLink())## StatsModels.TableRegressionModel{GeneralizedLinearModel{GLM.GlmResp{Vector{Float64}, Binomial{Float64}, LogitLink}, GLM.DensePredChol{Float64, Cholesky{Float64, Matrix{Float64}}}}, Matrix{Float64}}\n## \n## promoted ~ 1 + sales + customer_rate + performance\n## \n## Coefficients:\n## ──────────────────────────────────────────────────────────────────────────────────\n##                       Coef.  Std. Error      z  Pr(>|z|)    Lower 95%    Upper 95%\n## ──────────────────────────────────────────────────────────────────────────────────\n## (Intercept)     -19.8589     3.44408     -5.77    <1e-08  -26.6092     -13.1087\n## sales             0.0401242  0.00657643   6.10    <1e-08    0.0272347    0.0530138\n## customer_rate    -1.11213    0.482682    -2.30    0.0212   -2.05817     -0.166093\n## performance: 2    0.263      1.02198      0.26    0.7969   -1.74004      2.26604\n## performance: 3    0.684955   0.982167     0.70    0.4856   -1.24006      2.60997\n## performance: 4    0.734493   1.07196      0.69    0.4932   -1.36652      2.8355\n## ──────────────────────────────────────────────────────────────────────────────────# AIC\naic(promotion_model)## 76.3743282275113# log likelihood\nloglikelihood(promotion_model)## -32.18716411375565"},{"path":"alt-approaches.html","id":"multinomial-logistic-regression-1","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.3.3 Multinomial logistic regression","text":"Econometrics package provides convenient function multinomial logistic regression model, levels outcome variable can set argument.","code":"using DataFrames, CSV, Econometrics, CategoricalArrays;\n\n# get health_insurance data\nurl = \"http://peopleanalytics-regression-book.org/data/health_insurance.csv\";\nhealth_insurance = CSV.read(download(url), DataFrame, missingstrings=[\"NA\"]);\n\n# map gender and product to categorical\nhealth_insurance.gender = CategoricalArray(health_insurance.gender);\nhealth_insurance.product = CategoricalArray(health_insurance.product);\n\n# fit model defining reference level\nmodel = fit(EconometricModel, \n            @formula(product ~ age + household + position_level + gender + absent),\n            health_insurance,\n            contrasts = Dict(:product => DummyCoding(base = \"A\")))## Probability Model for Nominal Response\n## Categories: A, B, C\n## Number of observations: 1453\n## Null Loglikelihood: -1595.27\n## Loglikelihood: -744.68\n## R-squared: 0.5332\n## LR Test: 1701.18 ∼ χ²(12) ⟹ Pr > χ² = 0.0000\n## Formula: product ~ 1 + age + household + position_level + gender + absent\n## ───────────────────────────────────────────────────────────────────────────────────────────────────────\n##                                         PE         SE        t-value  Pr > |t|        2.50%      97.50%\n## ───────────────────────────────────────────────────────────────────────────────────────────────────────\n## product: B ~ (Intercept)          -4.60097     0.510551    -9.01179     <1e-18   -5.60248    -3.59947\n## product: B ~ age                   0.243662    0.0154313   15.7902      <1e-51    0.213392    0.273933\n## product: B ~ household            -0.967711    0.06943    -13.9379      <1e-40   -1.10391    -0.831516\n## product: B ~ position_level       -0.415304    0.0891671   -4.65759     <1e-05   -0.590215   -0.240392\n## product: B ~ gender: Male         -2.38258     0.232425   -10.251       <1e-23   -2.8385     -1.92665\n## product: B ~ gender: Non-binary    0.252837    1.22625      0.206186    0.8367   -2.1526      2.65827\n## product: B ~ absent                0.0116769   0.0129814    0.899512    0.3685   -0.0137875   0.0371413\n## product: C ~ (Intercept)         -10.2261      0.619736   -16.5007      <1e-55  -11.4418     -9.01041\n## product: C ~ age                   0.269812    0.0156702   17.2182      <1e-59    0.239073    0.300551\n## product: C ~ household             0.204347    0.0496062    4.11938     <1e-04    0.107039    0.301655\n## product: C ~ position_level       -0.213592    0.0822607   -2.59652     0.0095   -0.374955   -0.052228\n## product: C ~ gender: Male          0.0967029   0.195435     0.494809    0.6208   -0.286664    0.48007\n## product: C ~ gender: Non-binary   -1.26982     2.03625     -0.623608    0.5330   -5.26416     2.72452\n## product: C ~ absent                0.00326628  0.0124181    0.263026    0.7926   -0.0210932   0.0276258\n## ───────────────────────────────────────────────────────────────────────────────────────────────────────"},{"path":"alt-approaches.html","id":"proportional-odds-logistic-regression","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.3.4 Proportional odds logistic regression","text":"Econometrics package also provides convenient function proportional odds logistic regression.Note \\(R^2\\) results presented correspond McFadden variant pseudo-\\(R^2\\), although variants can obtained specifying appropriate function.","code":"using DataFrames, CSV, Econometrics, CategoricalArrays;\n\n# get soccer data\nurl = \"http://peopleanalytics-regression-book.org/data/soccer.csv\";\nsoccer = CSV.read(download(url), DataFrame, missingstrings=[\"NA\"]);\n\n# map columns not starting with \"n\" to categorical\ntransform!(soccer, names(soccer, r\"^(?!n)\") .=> CategoricalArray, renamecols = false);\n\n# map discipline column to ordinal\nsoccer.discipline = levels!(categorical(soccer.discipline, ordered = true), [\"None\", \"Yellow\", \"Red\"]);\n\n# run proportional odds model\nsoccer_model = fit(EconometricModel,\n                   @formula(discipline ~ n_yellow_25 + n_red_25 + position + result + country + level),\n                   soccer)## Probability Model for Ordinal Response\n## Categories: None < Yellow < Red\n## Number of observations: 2291\n## Null Loglikelihood: -1915.63\n## Loglikelihood: -1722.27\n## R-squared: 0.1009\n## LR Test: 386.73 ∼ χ²(8) ⟹ Pr > χ² = 0.0000\n## Formula: discipline ~ n_yellow_25 + n_red_25 + position + result + country + level\n## ─────────────────────────────────────────────────────────────────────────────────────────────\n##                                  PE         SE       t-value  Pr > |t|       2.50%     97.50%\n## ─────────────────────────────────────────────────────────────────────────────────────────────\n## n_yellow_25                  0.32236    0.0330776   9.74557     <1e-21   0.257495    0.387226\n## n_red_25                     0.383243   0.0405051   9.4616      <1e-20   0.303813    0.462674\n## position: M                  0.196847   0.116487    1.68986     0.0912  -0.0315846   0.425278\n## position: S                 -0.685337   0.150112   -4.56551     <1e-05  -0.979707   -0.390967\n## result: L                    0.483032   0.111951    4.31466     <1e-04   0.263495    0.702569\n## result: W                   -0.739473   0.121293   -6.09658     <1e-08  -0.977329   -0.501617\n## country: Germany             0.132972   0.0935995   1.42065     0.1556  -0.0505771   0.316521\n## level: 2                     0.0909663  0.0935472   0.972411    0.3309  -0.09248     0.274413\n## (Intercept): None | Yellow   2.50851    0.191826   13.077       <1e-37   2.13234     2.88468\n## (Intercept): Yellow | Red    3.92572    0.205714   19.0834      <1e-74   3.52232     4.32913\n## ─────────────────────────────────────────────────────────────────────────────────────────────r2(soccer_model, :Nagelkerke)## 0.19124446229501849"},{"path":"alt-approaches.html","id":"mixed-models","chapter":"10 Alternative Technical Approaches in R, Python and Julia","heading":"10.3.5 Mixed models","text":"MixedModels package allows construction linear mixed models mixed GLMs. approach mixed model speed dating Section 8.1:","code":"using DataFrames, CSV, MixedModels, CategoricalArrays;\n\n# get speed_dating data\nurl = \"http://peopleanalytics-regression-book.org/data/speed_dating.csv\";\nspeed_dating = CSV.read(download(url), DataFrame, missingstrings=[\"NA\"]);\n\n# make iid categorical\nspeed_dating.iid = CategoricalArray(speed_dating.iid);\n\n# run mixed GLM\ndating_model = fit(MixedModel, @formula(dec ~ agediff + samerace + attr + intel + prob + (1|iid)),\n                   speed_dating, Binomial(), LogitLink())## Generalized Linear Mixed Model fit by maximum likelihood (nAGQ = 1)\n##   dec ~ 1 + agediff + samerace + attr + intel + prob + (1 | iid)\n##   Distribution: Bernoulli{Float64}\n##   Link: LogitLink()\n## \n## \n##    logLik    deviance     AIC       AICc        BIC    \n##  -3203.2519  6406.5038  6420.5038  6420.5182  6469.2270\n## Variance components:\n##        Column   VarianceStd.Dev.\n## iid (Intercept)  5.13935 2.26701\n## \n##  Number of obs: 7789; levels of grouping factors: 541\n## \n## Fixed-effects parameters:\n## ──────────────────────────────────────────────────────\n##                    Coef.  Std. Error       z  Pr(>|z|)\n## ──────────────────────────────────────────────────────\n## (Intercept)  -12.7706      0.369743   -34.54    <1e-99\n## agediff       -0.0355554   0.0137893   -2.58    0.0099\n## samerace       0.20165     0.0800526    2.52    0.0118\n## attr           1.0782      0.030779    35.03    <1e-99\n## intel          0.299976    0.0333542    8.99    <1e-18\n## prob           0.620459    0.0267208   23.22    <1e-99\n## ──────────────────────────────────────────────────────"},{"path":"power-tests.html","id":"power-tests","chapter":"11 Power Analysis to Estimate Required Sample Sizes for Modeling","heading":"11 Power Analysis to Estimate Required Sample Sizes for Modeling","text":"vast majority situations people analytics, researchers analysts limited control size samples. common situation , course, analyses run whatever data can gleaned cleaned time available. time, seen previous work, even certain difference might exist real life populations studied, means certain specific analysis samples populations elucidate difference. Whether difference visible depends statistical properties samples used. Therefore, researchers analysts living reality conduct inferential analysis, usefulness work depends large degree samples available.suggests conscientious analyst well advised -front work determine samples chance yielding results inferential value. practical context, however, partly true (important reason chapter left towards end book). Estimating required sample sizes imprecise science. Although mathematics suggest theory precise, reality guessing inputs mathematics. many cases clueless inputs move realms pure speculation produce ranges required sample sizes wide fairly meaningless practice.said, situations conducting power analysis—, analysis required statistical properties samples order certain minimum probability observing true difference—makes sense. Power analysis important element experimental design. Experiments people analytics usually take one two forms:Prospective experiments involve running sort test pilot populations determine certain measure hypothesized effect. example, introducing certain new employee benefit specific subset company limited period time, determining difference impact employee satisfaction compared receive benefit.Prospective experiments involve running sort test pilot populations determine certain measure hypothesized effect. example, introducing certain new employee benefit specific subset company limited period time, determining difference impact employee satisfaction compared receive benefit.Retrospective experiments involve use historical data test certain measure hypothesized effect. usually occurs opportunistically apparent certain measure occurred past limited time, data can drawn test whether measure resulted hypothesized effect.Retrospective experiments involve use historical data test certain measure hypothesized effect. usually occurs opportunistically apparent certain measure occurred past limited time, data can drawn test whether measure resulted hypothesized effect.prospective retrospective experiments can involve lot work—either setting experiments extracting data history. natural question whether chances success justify required resources effort. proceeding cases, sensible get point view likely power experiment level sample size might needed order establish meaningful inference. reason, power analysis common component research proposals medical social sciences.Power analysis relatively blunt instrument whose primary value make sure substantial effort wasted foolhardy research. analyst already reasonably available data wants test effect certain phenomenon, direct approach just go run appropriate model assuming relatively straightforward . Power analysis considered clearly substantial labor involved proposed modeling work.","code":""},{"path":"power-tests.html","id":"errors-effect-sizes-and-statistical-power","chapter":"11 Power Analysis to Estimate Required Sample Sizes for Modeling","heading":"11.1 Errors, effect sizes and statistical power","text":"looking practical ways conduct power tests proposed experiments, let’s review example logical mathematical principles behind power testing, understand results power tests mean. Recall Section 3.3 logical mechanisms testing hypotheses statistical difference. Given data samples two groups population, null hypothesis \\(H_0\\) hypothesis difference exist groups overall population. null hypothesis rejected, accept alternative hypothesis \\(H_1\\) difference exist groups population.Recall also use statistical properties samples make inferences null alternative hypotheses based statistical likelihood. means four possible situations can occur run hypothesis tests:fail reject \\(H_0\\), fact \\(H_1\\) false. good outcome.reject \\(H_0\\), fact \\(H_1\\) false. known Type error.fail reject \\(H_0\\), fact \\(H_1\\) true. known Type II error.reject \\(H_0\\), fact \\(H_1\\) true. good outcome one often motivation hypothesis test first place.Statistical power refers fourth situation probability \\(H_0\\) rejected \\(H_1\\) true. Statistical power depends minimum three criteria:significance level \\(\\alpha\\) analysis wishes reject \\(H_0\\) (see Section 3.3). Usually \\(\\alpha = 0.05\\).size \\(n\\) sample used.size difference observed sample, known effect size. numerous definitions effect size depend specific type power test conducted.example illustrate mathematical relationship criteria, let’s assume run experiment group employees size \\(n\\) introduce new benefit test satisfaction levels introduction. statistic random variable, can expect mean difference satisfaction normal distribution. Let \\(\\mu_0\\) mean population null hypothesis let \\(\\mu_1\\) mean population alternative hypothesis. Now let’s assume sample observe mean satisfaction \\(\\mu^*\\) experiment. Recall Chapter 3 meet statistical significance standard \\(\\alpha\\), need \\(\\mu^*\\) greater certain multiple standard error \\(\\frac{\\sigma}{\\sqrt{n}}\\) \\(\\mu_0\\) based normal distribution. Let’s call multiple \\(z_{\\alpha}\\). Therefore, can say statistical power hypothesis test :\\[\n\\begin{aligned}\n\\mathrm{Power} &= P(\\mu^* > \\mu_0 + z_{\\alpha}\\frac{\\sigma}{\\sqrt{n}}\\vert{\\mu = \\mu_1}) \\\\\n&= P(\\frac{\\mu^* - \\mu_1}{\\frac{\\sigma}{\\sqrt{n}}} > -\\frac{\\mu_1 - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} + z_{\\alpha}\\vert{\\mu = \\mu_1}) \\\\\n&= 1 - \\Phi(-\\frac{\\mu_1 - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}} + z_{\\alpha}) \\\\\n&= 1 - \\Phi(-\\frac{\\mu_1 - \\mu_0}{\\sigma}\\sqrt{n} + z_{\\alpha}) \\\\\n&=  1 - \\Phi(-d\\sqrt{n} + z_{\\alpha})\n\\end{aligned}\n\\]\\(\\Phi\\) cumulative normal probability distribution function, \\(d = \\frac{\\mu_1 - \\mu_0}{\\sigma}\\) known Cohen’s effect size. Therefore, can see power depends measure observed effect size two samples (defined Cohen’s \\(d\\)) significance level \\(\\alpha\\) sample size \\(n\\)47.reader may immediately observe many measures known typical point wish power analysis. can assert minimum level statistical power wish —usually somewhere 0.8 0.9. can also assert \\(\\alpha\\). point experimental design, usually know sample size know difference observed sample (effect size). implies dealing single equation one unknown, means unique solution48. Practically speaking, looking ranges values common power analysis.","code":""},{"path":"power-tests.html","id":"simple-stats","chapter":"11 Power Analysis to Estimate Required Sample Sizes for Modeling","heading":"11.2 Power analysis for simple hypothesis tests","text":"Usually run power analyses get sense required sample sizes. Given observations unknowns previous section, assert certain possible statistical results order estimate required sample sizes. often, need suggest observed effect size order obtain minimum sample size effect size return statistically significant result desired level statistical power.Using example previous section, let’s assume see ‘medium’ effect size samples. Cohen’s Rule Thumb \\(d\\) states \\(d = 0.2\\) small effect size, \\(d = 0.5\\) medium effect size \\(d = 0.8\\) large effect size. can use wp.t() function WebPower package R power analysis paired two-sample \\(t\\)-test return minimum required sample size. can assume \\(d = 0.5\\) require power 0.8—, want 80% probability test return accurate rejection null hypothesis.tells us need absolute minimum 34 individuals sample effect size 0.5 return significant difference alpha 0.05 80% probability. Alternatively can test power specific proposed sample size.tells us minimum sample size 40 result power 0.87. similar process can used plot dependence power sample size various conditions Figure 11.1. known power curve.\nFigure 11.1: Plot power sample size paired t-test\ncan see ‘sweet spot’ approximately 40–60 minimum required participants, diminishing return statistical power . Similarly can plot proposed minimum sample size range effect sizes Figure 11.2.\nFigure 11.2: Plot power effect size paired t-test\nSimilar power test variants exist common simple hypothesis tests. Let’s assume want institute screening test recruiting process, want validate test running random set employees aim proving test score significant non-zero correlation job performance. assume see moderate correlation \\(r = 0.3\\) sample49, can use wp.correlation() function WebPower power analysis, resulting Figure 11.3.\nFigure 11.3: Plot power sample size correlation test\nFigure 11.3 informs us likely want hitting least 100 employees study reasonable chance establishing possible validity screening test.","code":"\nlibrary(WebPower)\n\n# get minimum n for power of 0.8\n(n_test <- WebPower::wp.t(d = 0.5, p = 0.8, type = \"paired\"))## Paired t-test\n## \n##            n   d alpha power\n##     33.36713 0.5  0.05   0.8\n## \n## NOTE: n is number of *pairs*\n## URL: http://psychstat.org/ttest\n# get power for n of 40\n(p_test <- WebPower::wp.t(n1 = 40, d = 0.5, type = \"paired\"))## Paired t-test\n## \n##      n   d alpha     power\n##     40 0.5  0.05 0.8693981\n## \n## NOTE: n is number of *pairs*\n## URL: http://psychstat.org/ttest\n# test a range of sample sizes\nsample_sizes <- 20:100\npower <- WebPower::wp.t(n1 = sample_sizes, d = 0.5, type = \"paired\")\n\nplot(power)\n# test a range of effect sizes\neffect_sizes <- 2:8/10\nsamples <- WebPower::wp.t(n1 = rep(40, 7), \n                          d = effect_sizes, \n                          type = \"paired\")\nplot(samples$d, samples$power, type = \"b\",\n     xlab = \"Effect size\", ylab = \"Power\")\nsample_sizes <- 50:150\ncorrel_powers <- WebPower::wp.correlation(n = sample_sizes, r = 0.3)\nplot(correl_powers)"},{"path":"power-tests.html","id":"power-analysis-for-linear-regression-models","chapter":"11 Power Analysis to Estimate Required Sample Sizes for Modeling","heading":"11.3 Power analysis for linear regression models","text":"power tests linear regression models, effect size statistic difference model fit two models compared. commonly comparison ‘full’ fitted model involving specific input variables compared ‘reduced’ model fewer input variables (often random variance model input variables).\\(f^2\\) statistic defined follows:\\[\nf^2 = \\frac{R_{\\mathrm{full}}^2 - R_{\\mathrm{reduced}}^2}{1 - R_{\\mathrm{full}}^2}\n\\]\nformula refers \\(R^2\\) fit statistics two models compared. example, imagine already know GPA college significant relationship job performance, wish determine proposed screening test incremental validity top knowing college GPA. might run two linear regression models, one relating job performance GPA, another relating job performance GPA screening test score. Assuming observe relatively small effect size screening test, assume \\(f^2 = 0.05\\)50, can plot sample size power determining whether two models significantly different. also need define number predictors full model (p1 = 2) reduced model (p2 = 1). plot shown Figure 11.4.\nFigure 11.4: Plot power sample size small effect second input variable linear regression model\n","code":"\nsample_sizes <- 100:300\nf_sq_power <- WebPower::wp.regression(n = sample_sizes, \n                                      p1 = 2, p2 = 1, f2 = 0.05)\n\nplot(f_sq_power)"},{"path":"power-tests.html","id":"power-analysis-for-log-likelihood-regression-models","chapter":"11 Power Analysis to Estimate Required Sample Sizes for Modeling","heading":"11.4 Power analysis for log-likelihood regression models","text":"Chapter 5, reviewed measures fit log-likelihood models still subject debate. Given , unsurprising measures effect size log-likelihood models well established. well-developed current method appeared Demidenko (2007), works want power test single input variable \\(x\\) using Wald test significance model coefficients (see Section 7.3.2 reminder Wald test).method, statistical power significance test input variable \\(x\\) determined using multiple inputs follows:likelihood positive outcome \\(x = 0\\) used determine intercept (p0 code ).likelihood positive outcome \\(x = 1\\) used determine regression coefficient \\(x\\) (p1 code ).distribution \\(x\\) inputted (family ) parameters distribution also entered (parameter ). example, distribution assumed normal mean standard deviation entered parameters.information fed Wald test, power specific sample sizes calculated.example, let’s assume wanted determine new screening test significant effect promotion likelihood running experiment employees considered promotion. assume screening test scored percentile scale mean 53 standard deviation 21. know approximately 50% considered promotion promoted, believe screening test may small effect whereby score zero still 40% chance promotion every additional point scored increase chance 0.2 percentage points. run wp.logistic() function WebPower plot power curve various sample sizes Figure 11.5.\nFigure 11.5: Plot power sample size single input variable logistic regression\ntest suggests need 1000 individuals experiment order least 80% chance establishing statistical significance true relationship screening test score promotion likelihood.","code":"\nsample_sizes <- 50:2000\nlogistic_power <- WebPower::wp.logistic(n = sample_sizes, \n                                        p0 = 0.4, p1 = 0.402,\n                                        family = \"normal\", \n                                        parameter = c(53, 21))\n\nplot(logistic_power)"},{"path":"power-tests.html","id":"power-analysis-for-hierarchical-regression-models","chapter":"11 Power Analysis to Estimate Required Sample Sizes for Modeling","heading":"11.5 Power analysis for hierarchical regression models","text":"Power tests explicit hierarchical models usually originate context design clinical trials, concern entire sample size study also need determine split sample treatment control. rare power analysis need conducted hierarchical models people analytics technology available WebPower package explore .Cluster randomized trials trials possible allocate individuals randomly treatment control groups entire clusters allocated random instead. creates substantial additional complexity understanding statistical power required sample sizes. wp.crt2arm() function WebPower supports power analysis 2-arm trials (treatment control), wp.crt3arm() function supports power analysis 3-arm trials (Two different treatments control).Multisite randomized trials trials individuals assigned treatment control groups random, individuals also belong different clusters important modeling—example, may members clinical groups based pre-existing conditions, may treated different hospitals outpatient facilities. , makes substantially complex calculation statistical power. wp.mrt2arm() wp.mrt3arm() functions offer support .Power tests also available structural equation models. involves comparing ‘complete’ structural model ‘subset’ model coefficients ‘complete’ model set zero. power tests can valuable structural models applied previously responses survey instruments intention test alternative models future. can provide information required future survey participation response rates order establish whether improved fit can established alternative models.two approaches power tests structural equation models, using chi square test root mean squared error (RMSEA) approach. methods take substantial number input parameters, consistent complexity structural equation model parameters various alternatives measuring fit models. chi square test approach implemented wp.sem.chisq() function, RMSEA approach implemented wp.sem.rmsea() function WebPower.","code":""},{"path":"power-tests.html","id":"power-analysis-using-python","chapter":"11 Power Analysis to Estimate Required Sample Sizes for Modeling","heading":"11.6 Power analysis using Python","text":"limited set resources power analysis available stats.power module statsmodels package. example, conduct power analysis paired \\(t\\)-test Section 11.2 .power curve can constructed Figure 11.6.\nFigure 11.6: Plot power sample size paired t-test\n","code":"import math\nfrom statsmodels.stats.power import TTestPower\n\npower = TTestPower()\nn_test = power.solve_power(effect_size = 0.5,\n                           power = 0.8,\n                           alpha = 0.05)\nprint(math.ceil(n_test))## 34import matplotlib.pyplot as plt\nimport numpy as np\n\nfig = plt.figure()\nfig = TTestPower().plot_power(dep_var = 'nobs',\n                              nobs = np.arange(20, 100),\n                              effect_size = np.array([0.5]),\n                              alpha = 0.05)\nplt.show()"},{"path":"solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html","id":"solutions-to-exercises-slide-presentations-videos-and-other-learning-resources","chapter":"Solutions to Exercises, Slide Presentations, Videos and Other Learning Resources","heading":"Solutions to Exercises, Slide Presentations, Videos and Other Learning Resources","text":"section progressively post helpful learning resources created consequence book.","code":""},{"path":"solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html","id":"solutions-to-exercises","chapter":"Solutions to Exercises, Slide Presentations, Videos and Other Learning Resources","heading":"Solutions to exercises","text":"intention publish comprehensive set solutions exercises book. Many exercises can approached different ways think important readers apply learning without constrained example solution.However, made place available readers can contribute solutions exercises benefit readers. interested seeing submitted solutions, requesting submitting solution , please visit https://solutions.peopleanalytics-regression-book.org.encourage contribute learning expertise considering submitting solution discussion question data exercise.","code":""},{"path":"solutions-to-exercises-slide-presentations-videos-and-other-learning-resources.html","id":"learning-resources","chapter":"Solutions to Exercises, Slide Presentations, Videos and Other Learning Resources","heading":"Learning resources","text":"set links slide presentations, videos interactive tutorials developed various teaching sessions related book:Statistical Inference Regression Analysis using R: Xaringan slide presentation covering hypothesis testing, linear binomial logistic regression.Linear Regression Analysis Prediction: Xaringan slide presentation explaining linear regression using popular data set advertising.Introduction Binomial Logistic Regression Inference Binary Outcomes: Xaringan slide presentation explaining binomial logistic regression using speed dating example.Explaining Performance Ratings Using Proportional Odds Regression: Xaringan slide presentation using proportional odds logistic regression explain performance ratings. Video session .Multinomial Logistic Regression Batch Modeling Tidyverse: Xaringan slide presentation focused multinomial logistic regression covering ‘tidy’ approaches modeling.Regression Analysis: ‘Swiss Army Knife’ Advanced People Analytics: Xaringan slide presentation various regression methods deep dive proportional odds logistic regression.Regression Modeling People Analytics: Survival Analysis: Xaringan slide presentation various regression methods deep dive survival analysis. Video session .Introduction Hypothesis Testing: Interactive tutorial beginners.","code":""},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
